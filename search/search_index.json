{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Scale-Out Computing on AWS is a solution that helps customers more easily deploy and operate a multiuser environment for computationally intensive workflows. The solution features a large selection of compute resources; fast network backbone; unlimited storage; and budget and cost management directly integrated within AWS. The solution also deploys a user interface (UI) and automation tools that allows you to create your own queues, scheduler resources, Amazon Machine Images (AMIs), software, and libraries. This solution is designed to provide a production ready reference implementation to be a starting point for deploying an AWS environment to run scale-out workloads, allowing you to focus on running simulations designed to solve complex computational problems. Easy installation \u00b6 Installation of your Scale-Out Computing on AWS cluster is fully automated and managed by CloudFormation Did you know? You can have multiple Scale-Out Computing on AWS clusters on the same AWS account Scale-Out Computing on AWS comes with a list of unique tags, making resource tracking easy for AWS Administrators Access your cluster in 1 click \u00b6 You can access your Scale-Out Computing on AWS cluster either using DCV (Desktop Cloud Visualization) 1 or through SSH. Simple Job Submission \u00b6 Scale-Out Computing on AWS supports a list of parameters designed to simplify your job submission on AWS . Advanced users can either manually choose compute/storage/network configuration for their job or simply ignore these parameters and let Scale-Out Computing on AWS picks the most optimal hardware (defined by the HPC administrator) # Advanced Configuration user@host$ qsub -l instance_type = c5n.18xlarge \\ -l instance_ami = ami-123abcde -l nodes = 2 -l scratch_size = 300 -l efa_support = true -l spot_price = 1 .55 myscript.sh # Basic Configuration user@host$ qsub myscript.sh Info Check our Web-Based utility to generate you submission command Refer to this page for tutorial and examples Refer to this page to list all supported parameters Jobs can also be submitted via HTTP API or via web interface OS agnostic and support for custom AMI \u00b6 Customers can integrate their Centos7/Rhel7/AmazonLinux2 AMI automatically by simply using -l instance_ami=<ami_id> at job submission. There is no limitation in term of AMI numbers (you can have 10 jobs running simultaneously using 10 different AMIs). SOCA supports heterogeneous environment, so you can have concurrent jobs running different operating system on the same cluster. AMI using OS different than the scheduler In case your AMI is different than your scheduler host, you can specify the OS manually to ensure packages will be installed based on the node distribution. In this example, we assume your Scale-Out Computing on AWS deployment was done using AmazonLinux2, but you want to submit a job on your personal RHEL7 AMI user@host$ qsub -l instance_ami = <ami_id> -l base_os = rhel7 myscript.sh Scale-Out Computing on AWS AMI requirements When you use a custom AMI, just make sure that your AMI does not use /apps, /scratch or /data partitions as Scale-Out Computing on AWS will need to use these locations during the deployment. Read this page for AMI creation best practices Web User Interface \u00b6 Scale-Out Computing on AWS includes a simple web ui designed to simplify user interactions such as: Start/Stop DCV sessions in 1 click Download private key in both PEM or PPK format Check the queue and job status in real-time Add/Remove LDAP users Access the analytic dashboard Access your filesystem Understand why your jobs are stuck in the queue Create Application profiles and let your users submit job directly via the web interface HTTP Rest API \u00b6 Users can submit/retrieve/delete jobs remotely via an HTTP REST API Budgets and Cost Management \u00b6 You can review your HPC costs filtered by user/team/project/queue very easily using AWS Cost Explorer. Scale-Out Computing on AWS also supports AWS Budget and let you create budgets assigned to user/team/project or queue. To prevent over-spend, Scale-Out Computing on AWS includes hooks to restrict job submission when customer-defined budget has expired. Lastly, Scale-Out Computing on AWS let you create queue ACLs or instance restriction at a queue level. Refer to this link for all best practices in order to control your HPC cost on AWS and prevent overspend . Detailed Cluster Analytics \u00b6 Scale-Out Computing on AWS includes OpenSearch (formerly Elasticsearch) and automatically ingest job and hosts data in real-time for accurate visualization of your cluster activity. Don't know where to start? Scale-Out Computing on AWS includes dashboard examples if you are not familiar with OpenSearch (formerly Elasticsearch) or Kibana. 100% Customizable \u00b6 Scale-Out Computing on AWS is built entirely on top of AWS and can be customized by users as needed. Most of the logic is based of CloudFormation templates, shell scripts and python code. More importantly, the entire Scale-Out Computing on AWS codebase is open-source and available on Github . Persistent and Unlimited Storage \u00b6 Scale-Out Computing on AWS includes two unlimited EFS storage (/apps and /data). Customers also have the ability to deploy high-speed SSD EBS disks or FSx for Lustre as scratch location on their compute nodes. Refer to this page to learn more about the various storage options offered by Scale-Out Computing on AWS Centralized user-management \u00b6 Customers can create unlimited LDAP users and groups . By default Scale-Out Computing on AWS includes a default LDAP account provisioned during installation as well as a \"Sudoers\" LDAP group which manage SUDO permission on the cluster. Automatic backup \u00b6 Scale-Out Computing on AWS automatically backup your data with no additional effort required on your side. Support for network licenses \u00b6 Scale-Out Computing on AWS includes a FlexLM-enabled script which calculate the number of licenses for a given features and only start the job/provision the capacity when enough licenses are available. Automatic Errors Handling \u00b6 Scale-Out Computing on AWS performs various dry run checks before provisioning the capacity. However, it may happen than AWS can't fullfill all requests (eg: need 5 instances but only 3 can be provisioned due to capacity shortage within a placement group). In this case, Scale-Out Computing on AWS will try to provision the capacity for 30 minutes. After 30 minutes, and if the capacity is still not available, Scale-Out Computing on AWS will automatically reset the request and try to provision capacity in a different availability zone. To simplify troubleshooting, all these errors are reported on the web interface Custom fair-share \u00b6 Each user is given a score which vary based on: Number of job in the queue Time each job is queued Priority of each job Type of instance Job that belong to the user with the highest score will start next. Fair Share is is configured at the queue level (so you can have one queue using FIFO and another one Fair Share) And more ... \u00b6 Refer to the various sections (tutorial/security/analytics ...) to learn more about this solution DCV is a remote visualization technology that enables users to easily and securely connect to graphic-intensive 3D applications hosted on a remote high-performance server.* \u21a9","title":"What is Scale-Out Computing on AWS ?"},{"location":"#easy-installation","text":"Installation of your Scale-Out Computing on AWS cluster is fully automated and managed by CloudFormation Did you know? You can have multiple Scale-Out Computing on AWS clusters on the same AWS account Scale-Out Computing on AWS comes with a list of unique tags, making resource tracking easy for AWS Administrators","title":"Easy installation"},{"location":"#access-your-cluster-in-1-click","text":"You can access your Scale-Out Computing on AWS cluster either using DCV (Desktop Cloud Visualization) 1 or through SSH.","title":"Access your cluster in 1 click"},{"location":"#simple-job-submission","text":"Scale-Out Computing on AWS supports a list of parameters designed to simplify your job submission on AWS . Advanced users can either manually choose compute/storage/network configuration for their job or simply ignore these parameters and let Scale-Out Computing on AWS picks the most optimal hardware (defined by the HPC administrator) # Advanced Configuration user@host$ qsub -l instance_type = c5n.18xlarge \\ -l instance_ami = ami-123abcde -l nodes = 2 -l scratch_size = 300 -l efa_support = true -l spot_price = 1 .55 myscript.sh # Basic Configuration user@host$ qsub myscript.sh Info Check our Web-Based utility to generate you submission command Refer to this page for tutorial and examples Refer to this page to list all supported parameters Jobs can also be submitted via HTTP API or via web interface","title":"Simple Job Submission"},{"location":"#os-agnostic-and-support-for-custom-ami","text":"Customers can integrate their Centos7/Rhel7/AmazonLinux2 AMI automatically by simply using -l instance_ami=<ami_id> at job submission. There is no limitation in term of AMI numbers (you can have 10 jobs running simultaneously using 10 different AMIs). SOCA supports heterogeneous environment, so you can have concurrent jobs running different operating system on the same cluster. AMI using OS different than the scheduler In case your AMI is different than your scheduler host, you can specify the OS manually to ensure packages will be installed based on the node distribution. In this example, we assume your Scale-Out Computing on AWS deployment was done using AmazonLinux2, but you want to submit a job on your personal RHEL7 AMI user@host$ qsub -l instance_ami = <ami_id> -l base_os = rhel7 myscript.sh Scale-Out Computing on AWS AMI requirements When you use a custom AMI, just make sure that your AMI does not use /apps, /scratch or /data partitions as Scale-Out Computing on AWS will need to use these locations during the deployment. Read this page for AMI creation best practices","title":"OS agnostic and support for custom AMI"},{"location":"#web-user-interface","text":"Scale-Out Computing on AWS includes a simple web ui designed to simplify user interactions such as: Start/Stop DCV sessions in 1 click Download private key in both PEM or PPK format Check the queue and job status in real-time Add/Remove LDAP users Access the analytic dashboard Access your filesystem Understand why your jobs are stuck in the queue Create Application profiles and let your users submit job directly via the web interface","title":"Web User Interface"},{"location":"#http-rest-api","text":"Users can submit/retrieve/delete jobs remotely via an HTTP REST API","title":"HTTP Rest API"},{"location":"#budgets-and-cost-management","text":"You can review your HPC costs filtered by user/team/project/queue very easily using AWS Cost Explorer. Scale-Out Computing on AWS also supports AWS Budget and let you create budgets assigned to user/team/project or queue. To prevent over-spend, Scale-Out Computing on AWS includes hooks to restrict job submission when customer-defined budget has expired. Lastly, Scale-Out Computing on AWS let you create queue ACLs or instance restriction at a queue level. Refer to this link for all best practices in order to control your HPC cost on AWS and prevent overspend .","title":"Budgets and Cost Management"},{"location":"#detailed-cluster-analytics","text":"Scale-Out Computing on AWS includes OpenSearch (formerly Elasticsearch) and automatically ingest job and hosts data in real-time for accurate visualization of your cluster activity. Don't know where to start? Scale-Out Computing on AWS includes dashboard examples if you are not familiar with OpenSearch (formerly Elasticsearch) or Kibana.","title":"Detailed Cluster Analytics"},{"location":"#100-customizable","text":"Scale-Out Computing on AWS is built entirely on top of AWS and can be customized by users as needed. Most of the logic is based of CloudFormation templates, shell scripts and python code. More importantly, the entire Scale-Out Computing on AWS codebase is open-source and available on Github .","title":"100% Customizable"},{"location":"#persistent-and-unlimited-storage","text":"Scale-Out Computing on AWS includes two unlimited EFS storage (/apps and /data). Customers also have the ability to deploy high-speed SSD EBS disks or FSx for Lustre as scratch location on their compute nodes. Refer to this page to learn more about the various storage options offered by Scale-Out Computing on AWS","title":"Persistent and Unlimited Storage"},{"location":"#centralized-user-management","text":"Customers can create unlimited LDAP users and groups . By default Scale-Out Computing on AWS includes a default LDAP account provisioned during installation as well as a \"Sudoers\" LDAP group which manage SUDO permission on the cluster.","title":"Centralized user-management"},{"location":"#automatic-backup","text":"Scale-Out Computing on AWS automatically backup your data with no additional effort required on your side.","title":"Automatic backup"},{"location":"#support-for-network-licenses","text":"Scale-Out Computing on AWS includes a FlexLM-enabled script which calculate the number of licenses for a given features and only start the job/provision the capacity when enough licenses are available.","title":"Support for network licenses"},{"location":"#automatic-errors-handling","text":"Scale-Out Computing on AWS performs various dry run checks before provisioning the capacity. However, it may happen than AWS can't fullfill all requests (eg: need 5 instances but only 3 can be provisioned due to capacity shortage within a placement group). In this case, Scale-Out Computing on AWS will try to provision the capacity for 30 minutes. After 30 minutes, and if the capacity is still not available, Scale-Out Computing on AWS will automatically reset the request and try to provision capacity in a different availability zone. To simplify troubleshooting, all these errors are reported on the web interface","title":"Automatic Errors Handling"},{"location":"#custom-fair-share","text":"Each user is given a score which vary based on: Number of job in the queue Time each job is queued Priority of each job Type of instance Job that belong to the user with the highest score will start next. Fair Share is is configured at the queue level (so you can have one queue using FIFO and another one Fair Share)","title":"Custom fair-share"},{"location":"#and-more","text":"Refer to the various sections (tutorial/security/analytics ...) to learn more about this solution DCV is a remote visualization technology that enables users to easily and securely connect to graphic-intensive 3D applications hosted on a remote high-performance server.* \u21a9","title":"And more ..."},{"location":"analytics/","text":"About \u00b6 Scale-Out Computing on AWS includes OpenSearch (formerly Elasticsearch) and automatically ingest job and hosts data in real-time for accurate visualization of your cluster activity. Don't know where to start? Scale-Out Computing on AWS includes dashboard examples if you are not familiar with OpenSearch (formerly Elasticsearch) or Kibana. Refer to the left sidebar for more detailed resources","title":"About"},{"location":"analytics/#about","text":"Scale-Out Computing on AWS includes OpenSearch (formerly Elasticsearch) and automatically ingest job and hosts data in real-time for accurate visualization of your cluster activity. Don't know where to start? Scale-Out Computing on AWS includes dashboard examples if you are not familiar with OpenSearch (formerly Elasticsearch) or Kibana. Refer to the left sidebar for more detailed resources","title":"About"},{"location":"analytics/build-kibana-dashboards/","text":"Pre-Requisites You must have configure your Kibana index first On your Kibana cluster , click \"Visualize\" to create a new visualization. Below are some example to help you get started Note: For each dashboard, you can get detailed data at user, queue, job or project level by simply using the \"Filters\" section Money spent by instance type \u00b6 Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: Sum Field: estimated_price_ondemand X Axis (Buckets): Aggregation: Terms Field: instance_type_used Order By: metric: Sum of estimated_price_ondemand Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Sum of estimated_price_ondemand Jobs per user \u00b6 Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: user.keyword Order By: metric: Count Jobs per user split by instance type \u00b6 Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Count Most active projects \u00b6 Configuration Select \"Pie\" and \"jobs\" index Slice Size (Metrics): Aggregation: Count Split Slices (Buckets): Aggregation: Terms Field: project.keyword Order By: metric: Count If needed, you can filter by project name (note: this type of filtering can be applied to all type of dashboard) Instance type launched by user \u00b6 Configuration Select \"Heat Map\" and \"jobs\" index Value (Metrics): Aggregation: Count Y Axis (Buckets): Aggregation: Term Field: instance_type_used Order By: metric: Count X Axis (Buckets): Aggregation: Terms Field: user Order By: metric: Count Number of nodes in the cluster \u00b6 Configuration Select \"Lines\" and \"pbsnodes\" index Y Axis (Metrics): Aggregation: Unique Count Field: Mom.keyword X Axis (Buckets): Aggregation: Date Histogram, Field: timestamp Interval: Minute Total number of CPUs by instance type \u00b6 Configuration Select \"Area\" and \"pbsnodes\" index Y Axis (Metrics): Aggregation: Sum Field: resources_available.ncpus X Axis (Buckets): Aggregation: Date Histogram, Field: timestamp Interval: minute Split Series (Buckets): Sub Aggregation: Terms Field: resources_available.instance_type.keyword Order By: metric: Sum Detailed information per user \u00b6 Configuration Select \"Datatables\" and \"jobs\" index Metric (Metrics): Aggregation: Count Split Rows (Buckets): Aggregation: Term Field: user.keyword Order By: metric: Count Split Rows (Buckets): Aggregation: Term Field: instance_type_used.keyword Order By: metric: Count Split Rows (Buckets): Aggregation: Term Field: price_ondemand.keyword Order By: metric: Count Split Rows (Buckets): Aggregation: Term Field: job_name.keyword Order By: metric: Count Find the price for a given simulation \u00b6 Each job comes with price_ondemand and price_reserved attributes which are calculated based on: number of nodes * ( simulation_hours * instance_hourly_rate )","title":"Create your own analytics dashboard"},{"location":"analytics/build-kibana-dashboards/#money-spent-by-instance-type","text":"Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: Sum Field: estimated_price_ondemand X Axis (Buckets): Aggregation: Terms Field: instance_type_used Order By: metric: Sum of estimated_price_ondemand Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Sum of estimated_price_ondemand","title":"Money spent by instance type"},{"location":"analytics/build-kibana-dashboards/#jobs-per-user","text":"Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: user.keyword Order By: metric: Count","title":"Jobs per user"},{"location":"analytics/build-kibana-dashboards/#jobs-per-user-split-by-instance-type","text":"Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Count","title":"Jobs per user split by instance type"},{"location":"analytics/build-kibana-dashboards/#most-active-projects","text":"Configuration Select \"Pie\" and \"jobs\" index Slice Size (Metrics): Aggregation: Count Split Slices (Buckets): Aggregation: Terms Field: project.keyword Order By: metric: Count If needed, you can filter by project name (note: this type of filtering can be applied to all type of dashboard)","title":"Most active projects"},{"location":"analytics/build-kibana-dashboards/#instance-type-launched-by-user","text":"Configuration Select \"Heat Map\" and \"jobs\" index Value (Metrics): Aggregation: Count Y Axis (Buckets): Aggregation: Term Field: instance_type_used Order By: metric: Count X Axis (Buckets): Aggregation: Terms Field: user Order By: metric: Count","title":"Instance type launched by user"},{"location":"analytics/build-kibana-dashboards/#number-of-nodes-in-the-cluster","text":"Configuration Select \"Lines\" and \"pbsnodes\" index Y Axis (Metrics): Aggregation: Unique Count Field: Mom.keyword X Axis (Buckets): Aggregation: Date Histogram, Field: timestamp Interval: Minute","title":"Number of nodes in the cluster"},{"location":"analytics/build-kibana-dashboards/#total-number-of-cpus-by-instance-type","text":"Configuration Select \"Area\" and \"pbsnodes\" index Y Axis (Metrics): Aggregation: Sum Field: resources_available.ncpus X Axis (Buckets): Aggregation: Date Histogram, Field: timestamp Interval: minute Split Series (Buckets): Sub Aggregation: Terms Field: resources_available.instance_type.keyword Order By: metric: Sum","title":"Total number of CPUs by instance type"},{"location":"analytics/build-kibana-dashboards/#detailed-information-per-user","text":"Configuration Select \"Datatables\" and \"jobs\" index Metric (Metrics): Aggregation: Count Split Rows (Buckets): Aggregation: Term Field: user.keyword Order By: metric: Count Split Rows (Buckets): Aggregation: Term Field: instance_type_used.keyword Order By: metric: Count Split Rows (Buckets): Aggregation: Term Field: price_ondemand.keyword Order By: metric: Count Split Rows (Buckets): Aggregation: Term Field: job_name.keyword Order By: metric: Count","title":"Detailed information per user"},{"location":"analytics/build-kibana-dashboards/#find-the-price-for-a-given-simulation","text":"Each job comes with price_ondemand and price_reserved attributes which are calculated based on: number of nodes * ( simulation_hours * instance_hourly_rate )","title":"Find the price for a given simulation"},{"location":"analytics/monitor-cluster-activity/","text":"Info Indexes have been renamed on SOCA 2.7.0 and newer: jobs -> soca_jobs pbsnodes -> soca_nodes soca_desktops (new) Dashboard URL \u00b6 Open your AWS console and navigate to CloudFormation. Select your parent Stack, click Output, and retrieve \"WebUserInterface\" Index Information \u00b6 Cluster Nodes Data Job Data DCV Desktop Data Kibana Index Name soca_nodes soca_jobs soca_desktops Script /apps/soca/$SOCA_CONFIGURATION/cluster_analytics/cluster_nodes_tracking.py /apps/soca/$SOCA_CONFIGURATION/cluster_analytics/job_tracking.py /apps/soca/$SOCA_CONFIGURATION/cluster_analytics/desktops_tracking.py Recurrence 1 minute 1 hour (note: job must be terminated to be shown on OpenSearch (formerly Elasticsearch)) 10 minutes Data uploaded Host Info (status of provisioned host, lifecycle, memory, cpu etc ..) Job Info (allocated hardware, licenses, simulation cost, job owner, instance type ...) Desktop Instance information Timestamp Key Use \"timestamp\" when you create the index for the first time use \"start_iso\" when you create the index for the first time Use \"timestamp\" when you create the index for the first time Note Analytics scripts are cron jobs running on the scheduler node. You can change the recurrence to match your own requirements. Create Indexes \u00b6 Since it's the first time you access this endpoint, you will need to configure your indexes. First, access Kibana URL and click \"Explore on my Own\" Go under Management and Click Index Patterns Create your first index by typing pbsnodes* . Click next, and then specify the Time Filter key ( timestamp ). Once done, click Create Index Pattern. Repeat the same operation for jobs* index This time, select start_iso as time filter key. Once your indexes are configured, go to Kibana, select \"Discover\" tab to start visualizing the data Examples \u00b6 Cluster Node \u00b6 Job Metadata \u00b6 Troubleshooting access permission \u00b6 Access to OpenSearch (formerly Elasticsearch) is restricted to the IP you have specified during the installation. If your IP change for any reason, you won't be able to access the analytics dashboard and will get the following error message: { \"Message\" : \"User: anonymous is not authorized to perform: es:ESHttpGet\" } To solve this issue, log in to AWS Console and go to OpenSearch (formerly Elasticsearch) Service dashboard. Select your OpenSearch (formerly Elasticsearch) cluster and click \"Modify Access Policy\" Finally, simply add your new IP under the \"Condition\" block, then click Submit Please note it may take up to 5 minutes for your IP to be validated Create your own dashboard \u00b6","title":"Monitor your cluster and job activity"},{"location":"analytics/monitor-cluster-activity/#dashboard-url","text":"Open your AWS console and navigate to CloudFormation. Select your parent Stack, click Output, and retrieve \"WebUserInterface\"","title":"Dashboard URL"},{"location":"analytics/monitor-cluster-activity/#index-information","text":"Cluster Nodes Data Job Data DCV Desktop Data Kibana Index Name soca_nodes soca_jobs soca_desktops Script /apps/soca/$SOCA_CONFIGURATION/cluster_analytics/cluster_nodes_tracking.py /apps/soca/$SOCA_CONFIGURATION/cluster_analytics/job_tracking.py /apps/soca/$SOCA_CONFIGURATION/cluster_analytics/desktops_tracking.py Recurrence 1 minute 1 hour (note: job must be terminated to be shown on OpenSearch (formerly Elasticsearch)) 10 minutes Data uploaded Host Info (status of provisioned host, lifecycle, memory, cpu etc ..) Job Info (allocated hardware, licenses, simulation cost, job owner, instance type ...) Desktop Instance information Timestamp Key Use \"timestamp\" when you create the index for the first time use \"start_iso\" when you create the index for the first time Use \"timestamp\" when you create the index for the first time Note Analytics scripts are cron jobs running on the scheduler node. You can change the recurrence to match your own requirements.","title":"Index Information"},{"location":"analytics/monitor-cluster-activity/#create-indexes","text":"Since it's the first time you access this endpoint, you will need to configure your indexes. First, access Kibana URL and click \"Explore on my Own\" Go under Management and Click Index Patterns Create your first index by typing pbsnodes* . Click next, and then specify the Time Filter key ( timestamp ). Once done, click Create Index Pattern. Repeat the same operation for jobs* index This time, select start_iso as time filter key. Once your indexes are configured, go to Kibana, select \"Discover\" tab to start visualizing the data","title":"Create Indexes"},{"location":"analytics/monitor-cluster-activity/#examples","text":"","title":"Examples"},{"location":"analytics/monitor-cluster-activity/#cluster-node","text":"","title":"Cluster Node"},{"location":"analytics/monitor-cluster-activity/#job-metadata","text":"","title":"Job Metadata"},{"location":"analytics/monitor-cluster-activity/#troubleshooting-access-permission","text":"Access to OpenSearch (formerly Elasticsearch) is restricted to the IP you have specified during the installation. If your IP change for any reason, you won't be able to access the analytics dashboard and will get the following error message: { \"Message\" : \"User: anonymous is not authorized to perform: es:ESHttpGet\" } To solve this issue, log in to AWS Console and go to OpenSearch (formerly Elasticsearch) Service dashboard. Select your OpenSearch (formerly Elasticsearch) cluster and click \"Modify Access Policy\" Finally, simply add your new IP under the \"Condition\" block, then click Submit Please note it may take up to 5 minutes for your IP to be validated","title":"Troubleshooting access permission"},{"location":"analytics/monitor-cluster-activity/#create-your-own-dashboard","text":"","title":"Create your own dashboard"},{"location":"analytics/monitor-system-with-metricbeat/","text":"What is MetricBeat \u00b6 Metricbeat is a lightweight shipper that you can install on your servers to periodically collect metrics from the operating system and from services running on the server. How to enable MetricBeat \u00b6 MetricBeat is disabled by default. To enable it either submit a job with system_metrics=True or enable this parameter at the queue level. When this feature is active, SOCA will automatically install and configure MetricBeat on all compute nodes provisioned for your jobs. Edit ComputeNodeConfigureMetrics.sh as needed (eg: change the check period, number of process to track etc ...) Note The very first job using MetricBeat will take an extra 45 secs as SOCA will perform the initial dashboard setup on Kibana. This is a one time operation, and can be deactivated if you do not want to install MetricBeat dashboards by default Access MetricBeat data \u00b6 MetricBeat is automatically integrated with your OpenSearch (formerly Elasticsearch) cluster. Click 'Dashboard' and search for \"MetricBeat System\" \"Host Overview ECS\" dashboard will give you system information related to nodes provisioned per job, users or queue. By default ELK reports \"Last 15 minutes\" data, so make sure you update the time selection accordingly You can filter the results by job id, job owner, queue, process name or host IP. See the example below which return metrics information for job 19544","title":"Monitor Nodes with MetricBeat"},{"location":"analytics/monitor-system-with-metricbeat/#what-is-metricbeat","text":"Metricbeat is a lightweight shipper that you can install on your servers to periodically collect metrics from the operating system and from services running on the server.","title":"What is MetricBeat"},{"location":"analytics/monitor-system-with-metricbeat/#how-to-enable-metricbeat","text":"MetricBeat is disabled by default. To enable it either submit a job with system_metrics=True or enable this parameter at the queue level. When this feature is active, SOCA will automatically install and configure MetricBeat on all compute nodes provisioned for your jobs. Edit ComputeNodeConfigureMetrics.sh as needed (eg: change the check period, number of process to track etc ...) Note The very first job using MetricBeat will take an extra 45 secs as SOCA will perform the initial dashboard setup on Kibana. This is a one time operation, and can be deactivated if you do not want to install MetricBeat dashboards by default","title":"How to enable MetricBeat"},{"location":"analytics/monitor-system-with-metricbeat/#access-metricbeat-data","text":"MetricBeat is automatically integrated with your OpenSearch (formerly Elasticsearch) cluster. Click 'Dashboard' and search for \"MetricBeat System\" \"Host Overview ECS\" dashboard will give you system information related to nodes provisioned per job, users or queue. By default ELK reports \"Last 15 minutes\" data, so make sure you update the time selection accordingly You can filter the results by job id, job owner, queue, process name or host IP. See the example below which return metrics information for job 19544","title":"Access MetricBeat data"},{"location":"budget/","text":"About \u00b6 You can review your HPC costs filtered by user/team/project/queue very easily using AWS Cost Explorer. Scale-Out Computing on AWS also supports AWS Budget and let you create budgets assigned to user/team/project or queue. To prevent over-spend, Scale-Out Computing on AWS includes hooks to restrict job submission when customer-defined budget has expired. Lastly, Scale-Out Computing on AWS let you create queue ACLs or instance restriction at a queue level. Refer to this link for all best practices in order to control your HPC cost on AWS and prevent overspend . Refer to the left sidebar for more detailed resources","title":"About"},{"location":"budget/#about","text":"You can review your HPC costs filtered by user/team/project/queue very easily using AWS Cost Explorer. Scale-Out Computing on AWS also supports AWS Budget and let you create budgets assigned to user/team/project or queue. To prevent over-spend, Scale-Out Computing on AWS includes hooks to restrict job submission when customer-defined budget has expired. Lastly, Scale-Out Computing on AWS let you create queue ACLs or instance restriction at a queue level. Refer to this link for all best practices in order to control your HPC cost on AWS and prevent overspend . Refer to the left sidebar for more detailed resources","title":"About"},{"location":"budget/prevent-overspend-hpc-cost-on-aws-soca/","text":"Scale-Out Computing on AWS offers multiple ways to make sure you will stay within budget while running your HPC workloads on AWS Limit who can submit jobs \u00b6 Only allow specific individual users or/and LDAP groups to submit jobs. Refer to this page for examples and documentation Limit what type of EC2 instance can be provisioned \u00b6 Control what type of EC2 instances can be provisioned for any given queue. Refer to this page for examples and documentation Accelerated Computing Instances Unless required for your workloads, it's recommended to exclude \"p2\", \"p3\", \"g2\", \"g3\", \"p3dn\" or other GPU instances type. Force jobs to run only on Reserved Instances \u00b6 You can limit a job to run only on Reserved Instances if you specify force_ri=True ( Documentation ) flag at job submission or for the entire queue. Your job will stay in the queue if you do not have any Reserved Instance available. Limit the number of concurrent jobs or provisioned instances \u00b6 You can limit the number of concurrent running jobs or provisioned instances at the queue level. Edit queue_mapping.yml , specify either max_running_jobs or max_provisioned_instances to the limit you do not want to exceed. queue_type: compute: queues: [ \"myqueue\" ] max_running_jobs: 5 max_provisioned_instances: 10 In this example, the maximum number of running job for \"myqueue\" will be 5. Similarly, jobs cannot request more than 10 instances (note: you can also limit the type/family of instances you want your user to provision) These settings are independent so you can choose to either limit by # jobs, # instances, both or none. Create a budget \u00b6 Creating an AWS Budget will ensure jobs can't be submitted if the budget allocated to the team/queue/project has exceeded the authorized amount. Refer to this page for examples and documentation Review your HPC cost in a central dashboard \u00b6 Stay on top of your AWS costs in real time. Quickly visualize your overall usage and find answers to your most common questions: Who are my top users? How much money did we spend for Project A? How much storage did we use for Queue B? Where my money is going (storage, compute ...) Etc ... Refer to this page for examples and documentation Best practices \u00b6 Assuming you are on-boarding a new team, here are our recommend best practices: 1 - Create LDAP account for all users 2 - Create LDAP group for the team. Add all users to the group 3 - Create a new queue 4 - Limit the queue to the LDAP group you just created 5 - Limit the type of EC2 instances your users can provision 6 - If needed, configure restricted parameters 7 - Create a Budget to make sure the new team won't spend more than what's authorized 8 - Limit your job to only run on your Reserved Instances or limit the number of provisioned instances for your queue","title":"Keep control of your HPC cost on AWS"},{"location":"budget/prevent-overspend-hpc-cost-on-aws-soca/#limit-who-can-submit-jobs","text":"Only allow specific individual users or/and LDAP groups to submit jobs. Refer to this page for examples and documentation","title":"Limit who can submit jobs"},{"location":"budget/prevent-overspend-hpc-cost-on-aws-soca/#limit-what-type-of-ec2-instance-can-be-provisioned","text":"Control what type of EC2 instances can be provisioned for any given queue. Refer to this page for examples and documentation Accelerated Computing Instances Unless required for your workloads, it's recommended to exclude \"p2\", \"p3\", \"g2\", \"g3\", \"p3dn\" or other GPU instances type.","title":"Limit what type of EC2 instance can be provisioned"},{"location":"budget/prevent-overspend-hpc-cost-on-aws-soca/#force-jobs-to-run-only-on-reserved-instances","text":"You can limit a job to run only on Reserved Instances if you specify force_ri=True ( Documentation ) flag at job submission or for the entire queue. Your job will stay in the queue if you do not have any Reserved Instance available.","title":"Force jobs to run only on Reserved Instances"},{"location":"budget/prevent-overspend-hpc-cost-on-aws-soca/#limit-the-number-of-concurrent-jobs-or-provisioned-instances","text":"You can limit the number of concurrent running jobs or provisioned instances at the queue level. Edit queue_mapping.yml , specify either max_running_jobs or max_provisioned_instances to the limit you do not want to exceed. queue_type: compute: queues: [ \"myqueue\" ] max_running_jobs: 5 max_provisioned_instances: 10 In this example, the maximum number of running job for \"myqueue\" will be 5. Similarly, jobs cannot request more than 10 instances (note: you can also limit the type/family of instances you want your user to provision) These settings are independent so you can choose to either limit by # jobs, # instances, both or none.","title":"Limit the number of concurrent jobs or provisioned instances"},{"location":"budget/prevent-overspend-hpc-cost-on-aws-soca/#create-a-budget","text":"Creating an AWS Budget will ensure jobs can't be submitted if the budget allocated to the team/queue/project has exceeded the authorized amount. Refer to this page for examples and documentation","title":"Create a budget"},{"location":"budget/prevent-overspend-hpc-cost-on-aws-soca/#review-your-hpc-cost-in-a-central-dashboard","text":"Stay on top of your AWS costs in real time. Quickly visualize your overall usage and find answers to your most common questions: Who are my top users? How much money did we spend for Project A? How much storage did we use for Queue B? Where my money is going (storage, compute ...) Etc ... Refer to this page for examples and documentation","title":"Review your HPC cost in a central dashboard"},{"location":"budget/prevent-overspend-hpc-cost-on-aws-soca/#best-practices","text":"Assuming you are on-boarding a new team, here are our recommend best practices: 1 - Create LDAP account for all users 2 - Create LDAP group for the team. Add all users to the group 3 - Create a new queue 4 - Limit the queue to the LDAP group you just created 5 - Limit the type of EC2 instances your users can provision 6 - If needed, configure restricted parameters 7 - Create a Budget to make sure the new team won't spend more than what's authorized 8 - Limit your job to only run on your Reserved Instances or limit the number of provisioned instances for your queue","title":"Best practices"},{"location":"budget/review-hpc-costs/","text":"AWS Cost Explorer \u00b6 Any EC2 resource launched by Scale-Out Computing on AWS comes with an extensive list of EC2 tags that can be used to get detailed information about your cluster usage. List includes (but not limited to): Project Name Job Owner Job Name Job Queue Job Id These are the default tags and you can add your own tags if needed. Step1: Enable Cost Allocation Tags \u00b6 Be patient It could take up to 24 hours for the tags to be active Click on your account name (1) then select \"My Billing Dashboard\" (2) Then click Cost Allocation tag Finally, search all \"Scale-Out Computing on AWS\" tags then click \"Activate\" Step 2: Enable Cost Explorer \u00b6 In your billing dashboard, select \"Cost Explorer\" (1) and click \"Enable Cost Explorer\" (2). Step 3: Query Cost Explorer \u00b6 Open your Cost Explorer tab and specify your filters. In this example I want to get the EC2 cost (1), group by day for my queue named \"cpus\" (2). To get more detailed information, select 'Group By' and apply additional filters. Here is an example if I want user level information for \"cpus\" queue Click \"Tag\" section under \"Group By\" horizontal label (1) and select \"soca:JobOwner\" tag. Your graph will automatically be updated with a cost breakdown by users for \"cpus\" queue","title":"Review your HPC costs"},{"location":"budget/review-hpc-costs/#aws-cost-explorer","text":"Any EC2 resource launched by Scale-Out Computing on AWS comes with an extensive list of EC2 tags that can be used to get detailed information about your cluster usage. List includes (but not limited to): Project Name Job Owner Job Name Job Queue Job Id These are the default tags and you can add your own tags if needed.","title":"AWS Cost Explorer"},{"location":"budget/review-hpc-costs/#step1-enable-cost-allocation-tags","text":"Be patient It could take up to 24 hours for the tags to be active Click on your account name (1) then select \"My Billing Dashboard\" (2) Then click Cost Allocation tag Finally, search all \"Scale-Out Computing on AWS\" tags then click \"Activate\"","title":"Step1: Enable Cost Allocation Tags"},{"location":"budget/review-hpc-costs/#step-2-enable-cost-explorer","text":"In your billing dashboard, select \"Cost Explorer\" (1) and click \"Enable Cost Explorer\" (2).","title":"Step 2: Enable Cost Explorer"},{"location":"budget/review-hpc-costs/#step-3-query-cost-explorer","text":"Open your Cost Explorer tab and specify your filters. In this example I want to get the EC2 cost (1), group by day for my queue named \"cpus\" (2). To get more detailed information, select 'Group By' and apply additional filters. Here is an example if I want user level information for \"cpus\" queue Click \"Tag\" section under \"Group By\" horizontal label (1) and select \"soca:JobOwner\" tag. Your graph will automatically be updated with a cost breakdown by users for \"cpus\" queue","title":"Step 3: Query Cost Explorer"},{"location":"budget/set-up-budget-project/","text":"On this page, I will demonstrate how to configure a budget for a given project and reject job if you exceed the allocated budget For this example, I will create a budget named \"Project 1\" and prevent user to submit job if (1) they do not belong to the project and (2) if the budget has expired. First, read this link to understand how to monitor your cluster cost and budgets on AWS. Configure the scheduler hook \u00b6 To enable this feature, you will first need to verify the project assigned to each job during submission time. The script managing this can be found on your Scale-Out Computing on AWS cluster at /apps/soca/<YOUR_SOCA_CLUSTER_ID>/cluster_hooks/queuejob/check_project_budget.py First, edit this file and manually enter your AWS account id: # User Variables aws_account_id = '<ENTER_YOUR_AWS_ACCOUNT_ID>' budget_config_file = '/apps/soca/<YOUR_SOCA_CLUSTER_ID>/cluster_manager/settings/project_cost_manager.txt' user_must_belong_to_project = True # Change if you don't want to restrict project to a list of users allow_job_no_project = False # Change if you do not want to enforce project at job submission allow_user_multiple_projects = True # Change if you want to restrict a user to one project Then enable the hook by running the following commands as root (on the scheduler host): user@host: qmgr -c \"create hook check_project_budget event=queuejob\" user@host: qmgr -c \"import hook check_project_budget application/x-python default /apps/soca/<YOUR_SOCA_CLUSTER_ID>/cluster_hooks/queuejob/check_project_budget.py\" Test it \u00b6 Submit a job when budget is valid \u00b6 Go to AWS Billing , click Budget on the left sidebar and create a new budget Select \"Cost Budget\". Name your budget \"Project 1\" and configure the Period/Budget based on your requirements. For my example I will allocate a $100 per month recurring budget for my project called \"Project 1\" (use Tag: soca:JobProject ) Set up a email notification when your budget exceed 80% then click \"Confirm Budget\" As you can see, I have still money available for this project (budgeted $100 but only used $15). Let's try to submit a job user@host$ qsub -- /bin/echo Hello qsub: Error. You tried to submit job without project. Specify project using -P parameter This does not work because the job was submitted without project defined. If you still want to let your users do that, edit allow_job_no_project = False on the hook file. Let's try the same request but specify -P \"Project 1\" during submission: user@host$ qsub -P \"Project 1\" -- /bin/echo Hello qsub: User mickael is not assigned to any project. See /apps/soca/cluster_manager/settings/project_cost_manager.txt This time, the hook complains thant my user \"mickael\" is not mapped the the project. This is because (1) the budget does not exist on the HPC cluster or (2) my user has not been approved to use this project. Edit /apps/soca/cluster_manager/settings/project_cost_manager.txt and configure your budget for this user: # This file is used to prevent job submission when budget allocated to a project exceed your threshold # This file is not used by default and must be configured manually using /apps/soca/cluster_hooks/queuejob/check_project_budget.py # Help & Documentation: https://soca.dev/tutorials/set-up-budget-project/ # # # Syntax: # [project 1] # user1 # user2 # [project 2] # user1 # user3 # [project blabla] # user4 # user5 [Project 1] mickael Important The config section (\"Project 1\") MUST match the name of the budget your created on AWS Budget (it's case sensitive) Save this file and try to submit a job, this time the job should go to the queue user@host$ qsub -P \"Project 1\" -- /bin/echo Hello 5 .ip-10-0-1-223 Submit a job when budget is invalid \u00b6 Now let's go back to your AWS budget, and let's simulate we are over-budget Now try to submit a job for \"Project 1\", your request should be rejected user@host$ qsub -P \"Project 1\" -- /bin/echo Hello qsub: Error. Budget for Project 1 exceed allocated threshold. Update it on AWS Budget bash The hook query the AWS Budget in real-time. So if your users are blocked because of budget restriction, you can at any time edit the value on AWS Budget and unblock them (assuming you still have some money left in your pocket :P ) As mentioned above, the project name on /apps/soca/cluster_manager/settings/project_cost_manager.txt and the name of your AWS Budget must match (case sensitive). If a user tries to launch a job associated to a project which does not exist on AWS Budget or with an invalid name, you will see the following error: bash-4.2$ qsub -P \"Project 2\" -- /bin/echo Hello qsub: Error. Unable to query AWS Budget API. ERROR: An error occurred ( NotFoundException ) when calling the DescribeBudget operation: [ Exception = NotFoundException ] Failed to call DescribeBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Failed to call GetBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Unable to get budget: Project 2 - the budget doesn ' t exist.","title":"Set up budget per project"},{"location":"budget/set-up-budget-project/#configure-the-scheduler-hook","text":"To enable this feature, you will first need to verify the project assigned to each job during submission time. The script managing this can be found on your Scale-Out Computing on AWS cluster at /apps/soca/<YOUR_SOCA_CLUSTER_ID>/cluster_hooks/queuejob/check_project_budget.py First, edit this file and manually enter your AWS account id: # User Variables aws_account_id = '<ENTER_YOUR_AWS_ACCOUNT_ID>' budget_config_file = '/apps/soca/<YOUR_SOCA_CLUSTER_ID>/cluster_manager/settings/project_cost_manager.txt' user_must_belong_to_project = True # Change if you don't want to restrict project to a list of users allow_job_no_project = False # Change if you do not want to enforce project at job submission allow_user_multiple_projects = True # Change if you want to restrict a user to one project Then enable the hook by running the following commands as root (on the scheduler host): user@host: qmgr -c \"create hook check_project_budget event=queuejob\" user@host: qmgr -c \"import hook check_project_budget application/x-python default /apps/soca/<YOUR_SOCA_CLUSTER_ID>/cluster_hooks/queuejob/check_project_budget.py\"","title":"Configure the scheduler hook"},{"location":"budget/set-up-budget-project/#test-it","text":"","title":"Test it"},{"location":"budget/set-up-budget-project/#submit-a-job-when-budget-is-valid","text":"Go to AWS Billing , click Budget on the left sidebar and create a new budget Select \"Cost Budget\". Name your budget \"Project 1\" and configure the Period/Budget based on your requirements. For my example I will allocate a $100 per month recurring budget for my project called \"Project 1\" (use Tag: soca:JobProject ) Set up a email notification when your budget exceed 80% then click \"Confirm Budget\" As you can see, I have still money available for this project (budgeted $100 but only used $15). Let's try to submit a job user@host$ qsub -- /bin/echo Hello qsub: Error. You tried to submit job without project. Specify project using -P parameter This does not work because the job was submitted without project defined. If you still want to let your users do that, edit allow_job_no_project = False on the hook file. Let's try the same request but specify -P \"Project 1\" during submission: user@host$ qsub -P \"Project 1\" -- /bin/echo Hello qsub: User mickael is not assigned to any project. See /apps/soca/cluster_manager/settings/project_cost_manager.txt This time, the hook complains thant my user \"mickael\" is not mapped the the project. This is because (1) the budget does not exist on the HPC cluster or (2) my user has not been approved to use this project. Edit /apps/soca/cluster_manager/settings/project_cost_manager.txt and configure your budget for this user: # This file is used to prevent job submission when budget allocated to a project exceed your threshold # This file is not used by default and must be configured manually using /apps/soca/cluster_hooks/queuejob/check_project_budget.py # Help & Documentation: https://soca.dev/tutorials/set-up-budget-project/ # # # Syntax: # [project 1] # user1 # user2 # [project 2] # user1 # user3 # [project blabla] # user4 # user5 [Project 1] mickael Important The config section (\"Project 1\") MUST match the name of the budget your created on AWS Budget (it's case sensitive) Save this file and try to submit a job, this time the job should go to the queue user@host$ qsub -P \"Project 1\" -- /bin/echo Hello 5 .ip-10-0-1-223","title":"Submit a job when budget is valid"},{"location":"budget/set-up-budget-project/#submit-a-job-when-budget-is-invalid","text":"Now let's go back to your AWS budget, and let's simulate we are over-budget Now try to submit a job for \"Project 1\", your request should be rejected user@host$ qsub -P \"Project 1\" -- /bin/echo Hello qsub: Error. Budget for Project 1 exceed allocated threshold. Update it on AWS Budget bash The hook query the AWS Budget in real-time. So if your users are blocked because of budget restriction, you can at any time edit the value on AWS Budget and unblock them (assuming you still have some money left in your pocket :P ) As mentioned above, the project name on /apps/soca/cluster_manager/settings/project_cost_manager.txt and the name of your AWS Budget must match (case sensitive). If a user tries to launch a job associated to a project which does not exist on AWS Budget or with an invalid name, you will see the following error: bash-4.2$ qsub -P \"Project 2\" -- /bin/echo Hello qsub: Error. Unable to query AWS Budget API. ERROR: An error occurred ( NotFoundException ) when calling the DescribeBudget operation: [ Exception = NotFoundException ] Failed to call DescribeBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Failed to call GetBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Unable to get budget: Project 2 - the budget doesn ' t exist.","title":"Submit a job when budget is invalid"},{"location":"security/","text":"About \u00b6 This section share all security best practices you should be aware of when using SOCA. Refer to the left sidebar for more detailed resources","title":"About"},{"location":"security/#about","text":"This section share all security best practices you should be aware of when using SOCA. Refer to the left sidebar for more detailed resources","title":"About"},{"location":"security/backup-restore-your-cluster/","text":"By default, Scale-Out Computing on AWS automatically backup your EC2 scheduler and EFS filesystems every day and keep the backups for 30 days using AWS Backup. During the installation, Scale-Out Computing on AWS creates a new backup vault and one backup plan. You can edit them if needed. What is AWS Backups? \u00b6 AWS Backup is a fully managed backup service that makes it easy to centralize and automate the back up of data across AWS services in the cloud. Using AWS Backup, you can centrally configure backup policies and monitor backup activity for AWS resources, such as Amazon EBS volumes, Amazon RDS databases, Amazon DynamoDB tables, Amazon EFS file systems. Backup Vault \u00b6 A \"Backup Vault\" is where all your backups are stored. Your vault is automatically encrypted using your Key Management Service (KMS) key and reference to your SOCA cluster ID ( soca-mycluster in this example) Backup Plan \u00b6 A \"Backup Plan\" is where you define all your backup strategy such as backup frequency, data retention or resource assignments. Backup rules (red section) \u00b6 By default, Scale-Out Computing on AWS creates one backup rule with the following parameters: Backup will start every day between 5AM and 6AM UTC (blue section) Backup will expire after 1 month (orange section) Backup is stored on the encrypted vault created by SOCA (purple section) If needed, you can edit this rule (or create a new one) to match your company backup strategy. Resources Assignments (green section) \u00b6 By default, Scale-Out Computing on AWS backup all data using the soca:BackupPlan tag. Value of this tag must match your cluster ID ( soca-mycluster in this example). Supported Resources AWS Backup only support the following resources: EC2 instances EBS disks EFS filesystems RDS (not used by SOCA) DynamoDB (not used by SOCA) How to add/remove resources to the backup plan \u00b6 Backup resources are managed by soca:BackupPlan tag. The value of this tag must match the value of your Backup Plan (by default it should match the name of your cluster). Apply this tag to any EC2 instance, EBS volumes or EFS filesystem you want to backup. How to restore a backup? \u00b6 On the left sidebar, click \"Protected Resources\" then choose the resource you want to restore This will open a new window with additional information about this resource (either EFS or EC2). Select the latest entry you want to restore from the \"Backups\" section then click \"Restore\" This will open a regular EC2 launch instance or EFS wizard. Specify the parameters (VPC, Subnet, Security Group, IAM role ...) you want to use and click \"Restore Backup\" Restore Role By default, you can only apply SchedulerIAMRole or ComputeNodeIAMRole to the EC2 resource you are restoring. If you want to restore one EC2 instance with a different role, you must edit iam:PassRole policy of your SOCA-Backup role. Make sure to use the SOCA-Backup IAM role created by SOCA during initial installation. If you want to use the default role created by AWS Backup, make sure to add iam:PassRole permission. How to delete a backup ? \u00b6 Select your vault, choose which recovery point you want to remove under the \"Backups\" section then click \"Delete\". Check the status of the backup jobs \u00b6 On the left sidebar, check \"Jobs\" to verify if your backup jobs are running correctly What happen if you delete your Cloudformation stack? \u00b6 Your backup vault won't be deleted if you have active backups in it. In case of accidental termination of your primary cloudformation template, you will still be able to recover your data by restoring the EFS and/or EC2. To delete your AWS Backup entry, you first need to manually remove all backups present in your vault.","title":"Backup your entire cluster automatically"},{"location":"security/backup-restore-your-cluster/#what-is-aws-backups","text":"AWS Backup is a fully managed backup service that makes it easy to centralize and automate the back up of data across AWS services in the cloud. Using AWS Backup, you can centrally configure backup policies and monitor backup activity for AWS resources, such as Amazon EBS volumes, Amazon RDS databases, Amazon DynamoDB tables, Amazon EFS file systems.","title":"What is AWS Backups?"},{"location":"security/backup-restore-your-cluster/#backup-vault","text":"A \"Backup Vault\" is where all your backups are stored. Your vault is automatically encrypted using your Key Management Service (KMS) key and reference to your SOCA cluster ID ( soca-mycluster in this example)","title":"Backup Vault"},{"location":"security/backup-restore-your-cluster/#backup-plan","text":"A \"Backup Plan\" is where you define all your backup strategy such as backup frequency, data retention or resource assignments.","title":"Backup Plan"},{"location":"security/backup-restore-your-cluster/#backup-rules-red-section","text":"By default, Scale-Out Computing on AWS creates one backup rule with the following parameters: Backup will start every day between 5AM and 6AM UTC (blue section) Backup will expire after 1 month (orange section) Backup is stored on the encrypted vault created by SOCA (purple section) If needed, you can edit this rule (or create a new one) to match your company backup strategy.","title":"Backup rules (red section)"},{"location":"security/backup-restore-your-cluster/#resources-assignments-green-section","text":"By default, Scale-Out Computing on AWS backup all data using the soca:BackupPlan tag. Value of this tag must match your cluster ID ( soca-mycluster in this example). Supported Resources AWS Backup only support the following resources: EC2 instances EBS disks EFS filesystems RDS (not used by SOCA) DynamoDB (not used by SOCA)","title":"Resources Assignments (green section)"},{"location":"security/backup-restore-your-cluster/#how-to-addremove-resources-to-the-backup-plan","text":"Backup resources are managed by soca:BackupPlan tag. The value of this tag must match the value of your Backup Plan (by default it should match the name of your cluster). Apply this tag to any EC2 instance, EBS volumes or EFS filesystem you want to backup.","title":"How to add/remove resources to the backup plan"},{"location":"security/backup-restore-your-cluster/#how-to-restore-a-backup","text":"On the left sidebar, click \"Protected Resources\" then choose the resource you want to restore This will open a new window with additional information about this resource (either EFS or EC2). Select the latest entry you want to restore from the \"Backups\" section then click \"Restore\" This will open a regular EC2 launch instance or EFS wizard. Specify the parameters (VPC, Subnet, Security Group, IAM role ...) you want to use and click \"Restore Backup\" Restore Role By default, you can only apply SchedulerIAMRole or ComputeNodeIAMRole to the EC2 resource you are restoring. If you want to restore one EC2 instance with a different role, you must edit iam:PassRole policy of your SOCA-Backup role. Make sure to use the SOCA-Backup IAM role created by SOCA during initial installation. If you want to use the default role created by AWS Backup, make sure to add iam:PassRole permission.","title":"How to restore a backup?"},{"location":"security/backup-restore-your-cluster/#how-to-delete-a-backup","text":"Select your vault, choose which recovery point you want to remove under the \"Backups\" section then click \"Delete\".","title":"How to delete a backup ?"},{"location":"security/backup-restore-your-cluster/#check-the-status-of-the-backup-jobs","text":"On the left sidebar, check \"Jobs\" to verify if your backup jobs are running correctly","title":"Check the status of the backup jobs"},{"location":"security/backup-restore-your-cluster/#what-happen-if-you-delete-your-cloudformation-stack","text":"Your backup vault won't be deleted if you have active backups in it. In case of accidental termination of your primary cloudformation template, you will still be able to recover your data by restoring the EFS and/or EC2. To delete your AWS Backup entry, you first need to manually remove all backups present in your vault.","title":"What happen if you delete your Cloudformation stack?"},{"location":"security/integrate-cognito-sso/","text":"On this page, we will see how you can automatically authenticate your users to Scale-Out Computing on AWS using without having them to enter their password. What is Cognito / Oauth2 \u00b6 With Amazon Cognito , your users can sign-in through social identity providers such as Google, Facebook, and Amazon, and through enterprise identity providers such as Microsoft Active Directory using SAML. Amazon Cognito User Pools provide a secure user directory that scales to hundreds of millions of users. As a fully managed service, User Pools are easy to set up without any worries about server infrastructure. User Pools provide user profiles and authentication tokens for users who sign up directly and for federated users who sign in with social and enterprise identity providers. Additionally, read this link if you are not already familiar with Oauth2 workflow. How it works ? \u00b6 1: Mary has an account on her corporate LDAP or Active Directory. This account has an username (e.g mary), an email (e.g mary@company.com ) and other parameters (cost center, location ... ). She uses her account to log in to her corporate network. 2: Mary wants to access the web UI of SOCA (we assume she already has an active account on SOCA. If not, refer to this page to learn how to manage user account on SOCA ) 2.1: She can access the application by entering her SOCA LDAP username/password 2.2: She can be automatically logged in using Amazon Cognito 3: Assuming SSO is enabled, SOCA will forward the access request Cognito which will use Mary's Corporate LDAP as a Federated identity to determine if she is a valid user. This is the authentication part. 4: Mary's Corporate LDAP will check her account (e.g based on Kerberos ticket) and return a SAML token. This is the authorization part. 5: Based on the authorization results, SOCA will automatically logs Mary in or reject her request What if Mary does not have an account on SOCA? Assuming a user has a valid credential on corporate LDAP/AD but not on SOCA, Cognito will redirect the user to the default SOCA login portal. How do you determine if a user has an account on SOCA? To verify if a corporate user is active and can log in, SOCA checks whether or not this user has an account. By default, we determine the user account name is the first part of an email address. For example, if the email returned by your corporate LDAP for a given user is myuser@company.com , we will assume this user the SOCA account is myuser . If this mapping does not apply to your company, you can change it by editing /apps/soca/$SOCA_CONFIGURATION/cluster_web_ui/generic/auth.py . Create your Cognito User Pool \u00b6 Info This setup is different for each customer, refer fo the official AWS Documentaton for Cognito if needed. First, you need to configure your Cognito User Pool with your existing SAML provider . You can leave most settings by default as we won't be using any user password, custom login UI etc.. This step is optional if you already have a User Pool configured. Integrate your AD/LDAP with Cognito \u00b6 Once your User Pool is created, go to Federation > Identity Provider and choose whatever IDP you want to use (most likely SAML if you are looking to integrate your corporate LDAP) SAML integration vary based on your own corporate settings, reach out to your local IT if needed. Here are some extra links if you need more documentation related to SAML: SAML Authentication Flow Adding SAML Identity Providers to a User Pool Create a SAML Provider on Cognito Warning The only required parameters for Scale-Out Computing on AWS is the email attribute. Create a Cognito Client \u00b6 On Cognito interface, click User Pools > Federated Identities then General Settings > App Clients and finally click Add Another App Client . Note your client name, client id and client secret and leave all other parameters by default. Configure your Cognito Client \u00b6 Now visit App Integration > App client setting and configure your application with the Identity Provider you just created ( Enabled Identity Provider ). You also want to specify the callback url(s) for all domains you are planning to use. The default callback URL you must specify is https://<your_soca_elb_name>/oauth but you can add any other URL if you have multiple environments (e.g: dev/test) Important Info about Callback Update the callback URLs with your custom domain if you are using one. If you don't do that, you will get a Cognito error with \"Redirect URI mismatch\" Configure SOCA \u00b6 Edit /apps/soca/$SOCA_CONFIGURATION/cluster_web_ui/generic/parameters.cfg and update the [cognito] section as shown below: [ cognito ] ## Cognito enable_sso = \"true\" # Set this flag to \"true\" cognito_oauth_authorize_endpoint = \"https://<YOUR_COGNITO_POOL>.auth.<YOUR_REGION>.amazoncognito.com/oauth2/authorize\" cognito_oauth_token_endpoint = \"https://<YOUR_COGNITO_POOL>.auth.<YOUR_REGION>.amazoncognito.com/oauth2/token\" cognito_jws_keys_endpoint = \"https://cognito-idp.<YOUR_REGION>.amazonaws.com/<YOUR_REGION>_<YOUR_POOL_ID>/.well-known/jwks.json\" cognito_app_secret = \"<YOUR_APP_SECRET>\" cognito_app_id = \"<YOUR_APP_ID>\" cognito_root_url = \"<YOUR_WEB_URL>\" cognito_callback_url = \"<YOUR_CALLBACK_URL>\" Important Make sure to use double quotes for all variables (eg. enable_sso=\"true\" and not enable_sso='true') Restart the Web UI \u00b6 Simply restart the Web UI by running: /apps/soca/ $SOCA_CONFIGURATION /cluster_web_ui/socawebui.sh stop /apps/soca/ $SOCA_CONFIGURATION /cluster_web_ui/socawebui.sh start Now try to access https://<YOUR_SOCA_DNS>/ , you should be automatically logged in. Note that we keep /login as fallback authentication mechanism to LDAP, so make sure your users access https://<YOUR_SOCA_DNS>/ and not https://<YOUR_SOCA_DNS>/login if they want to be automatically logged in with SSO","title":"Enable Oauth2 authentication with Cognito"},{"location":"security/integrate-cognito-sso/#what-is-cognito-oauth2","text":"With Amazon Cognito , your users can sign-in through social identity providers such as Google, Facebook, and Amazon, and through enterprise identity providers such as Microsoft Active Directory using SAML. Amazon Cognito User Pools provide a secure user directory that scales to hundreds of millions of users. As a fully managed service, User Pools are easy to set up without any worries about server infrastructure. User Pools provide user profiles and authentication tokens for users who sign up directly and for federated users who sign in with social and enterprise identity providers. Additionally, read this link if you are not already familiar with Oauth2 workflow.","title":"What is Cognito / Oauth2"},{"location":"security/integrate-cognito-sso/#how-it-works","text":"1: Mary has an account on her corporate LDAP or Active Directory. This account has an username (e.g mary), an email (e.g mary@company.com ) and other parameters (cost center, location ... ). She uses her account to log in to her corporate network. 2: Mary wants to access the web UI of SOCA (we assume she already has an active account on SOCA. If not, refer to this page to learn how to manage user account on SOCA ) 2.1: She can access the application by entering her SOCA LDAP username/password 2.2: She can be automatically logged in using Amazon Cognito 3: Assuming SSO is enabled, SOCA will forward the access request Cognito which will use Mary's Corporate LDAP as a Federated identity to determine if she is a valid user. This is the authentication part. 4: Mary's Corporate LDAP will check her account (e.g based on Kerberos ticket) and return a SAML token. This is the authorization part. 5: Based on the authorization results, SOCA will automatically logs Mary in or reject her request What if Mary does not have an account on SOCA? Assuming a user has a valid credential on corporate LDAP/AD but not on SOCA, Cognito will redirect the user to the default SOCA login portal. How do you determine if a user has an account on SOCA? To verify if a corporate user is active and can log in, SOCA checks whether or not this user has an account. By default, we determine the user account name is the first part of an email address. For example, if the email returned by your corporate LDAP for a given user is myuser@company.com , we will assume this user the SOCA account is myuser . If this mapping does not apply to your company, you can change it by editing /apps/soca/$SOCA_CONFIGURATION/cluster_web_ui/generic/auth.py .","title":"How it works ?"},{"location":"security/integrate-cognito-sso/#create-your-cognito-user-pool","text":"Info This setup is different for each customer, refer fo the official AWS Documentaton for Cognito if needed. First, you need to configure your Cognito User Pool with your existing SAML provider . You can leave most settings by default as we won't be using any user password, custom login UI etc.. This step is optional if you already have a User Pool configured.","title":"Create your Cognito User Pool"},{"location":"security/integrate-cognito-sso/#integrate-your-adldap-with-cognito","text":"Once your User Pool is created, go to Federation > Identity Provider and choose whatever IDP you want to use (most likely SAML if you are looking to integrate your corporate LDAP) SAML integration vary based on your own corporate settings, reach out to your local IT if needed. Here are some extra links if you need more documentation related to SAML: SAML Authentication Flow Adding SAML Identity Providers to a User Pool Create a SAML Provider on Cognito Warning The only required parameters for Scale-Out Computing on AWS is the email attribute.","title":"Integrate your AD/LDAP with Cognito"},{"location":"security/integrate-cognito-sso/#create-a-cognito-client","text":"On Cognito interface, click User Pools > Federated Identities then General Settings > App Clients and finally click Add Another App Client . Note your client name, client id and client secret and leave all other parameters by default.","title":"Create a Cognito Client"},{"location":"security/integrate-cognito-sso/#configure-your-cognito-client","text":"Now visit App Integration > App client setting and configure your application with the Identity Provider you just created ( Enabled Identity Provider ). You also want to specify the callback url(s) for all domains you are planning to use. The default callback URL you must specify is https://<your_soca_elb_name>/oauth but you can add any other URL if you have multiple environments (e.g: dev/test) Important Info about Callback Update the callback URLs with your custom domain if you are using one. If you don't do that, you will get a Cognito error with \"Redirect URI mismatch\"","title":"Configure your Cognito Client"},{"location":"security/integrate-cognito-sso/#configure-soca","text":"Edit /apps/soca/$SOCA_CONFIGURATION/cluster_web_ui/generic/parameters.cfg and update the [cognito] section as shown below: [ cognito ] ## Cognito enable_sso = \"true\" # Set this flag to \"true\" cognito_oauth_authorize_endpoint = \"https://<YOUR_COGNITO_POOL>.auth.<YOUR_REGION>.amazoncognito.com/oauth2/authorize\" cognito_oauth_token_endpoint = \"https://<YOUR_COGNITO_POOL>.auth.<YOUR_REGION>.amazoncognito.com/oauth2/token\" cognito_jws_keys_endpoint = \"https://cognito-idp.<YOUR_REGION>.amazonaws.com/<YOUR_REGION>_<YOUR_POOL_ID>/.well-known/jwks.json\" cognito_app_secret = \"<YOUR_APP_SECRET>\" cognito_app_id = \"<YOUR_APP_ID>\" cognito_root_url = \"<YOUR_WEB_URL>\" cognito_callback_url = \"<YOUR_CALLBACK_URL>\" Important Make sure to use double quotes for all variables (eg. enable_sso=\"true\" and not enable_sso='true')","title":"Configure SOCA"},{"location":"security/integrate-cognito-sso/#restart-the-web-ui","text":"Simply restart the Web UI by running: /apps/soca/ $SOCA_CONFIGURATION /cluster_web_ui/socawebui.sh stop /apps/soca/ $SOCA_CONFIGURATION /cluster_web_ui/socawebui.sh start Now try to access https://<YOUR_SOCA_DNS>/ , you should be automatically logged in. Note that we keep /login as fallback authentication mechanism to LDAP, so make sure your users access https://<YOUR_SOCA_DNS>/ and not https://<YOUR_SOCA_DNS>/login if they want to be automatically logged in with SSO","title":"Restart the Web UI"},{"location":"security/limit-concurrent-job-and-instances/","text":"Restrict number of concurrent running jobs \u00b6 Configure max_running_jobs to limit the number of jobs running in parallel for a given queue Considering /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml queue_type: compute: queues: [\"normal\"] max_running_jobs: 5 ... test: queues: [\"test\"] max_running_jobs: 30 In this example, only 5 jobs can be running at the same time in the \"normal\" queue, and 30 in the \"test\" queue. If a job cannot start because of this parameter, an error_message will be visible when running \"qstat -f\" or via the web interface Restrict number of provisioned instances \u00b6 Configure max_provisioned_instances to limit the number of instances that can be provisioned for a given queue queue_type: compute: queues: [\"normal\"] max_provisioned_instances: 10 ... test: queues: [\"test\"] max_provisioned_instances: 20 In this example, you cannot have more than 10 instances provisioned for \"normal\" queue, and 20 for the \"test\" queue. If a job cannot start because of this parameter, an error_message will be visible when running \"qstat -f\" or via the web interface Info Limit apply to all type of instance. Configure allowed_instance_types / excluded_instance_types if you also want to limit the type of EC2 instance than can be provisioned ).","title":"Restrict number of concurrent jobs and/or instances"},{"location":"security/limit-concurrent-job-and-instances/#restrict-number-of-concurrent-running-jobs","text":"Configure max_running_jobs to limit the number of jobs running in parallel for a given queue Considering /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml queue_type: compute: queues: [\"normal\"] max_running_jobs: 5 ... test: queues: [\"test\"] max_running_jobs: 30 In this example, only 5 jobs can be running at the same time in the \"normal\" queue, and 30 in the \"test\" queue. If a job cannot start because of this parameter, an error_message will be visible when running \"qstat -f\" or via the web interface","title":"Restrict number of concurrent running jobs"},{"location":"security/limit-concurrent-job-and-instances/#restrict-number-of-provisioned-instances","text":"Configure max_provisioned_instances to limit the number of instances that can be provisioned for a given queue queue_type: compute: queues: [\"normal\"] max_provisioned_instances: 10 ... test: queues: [\"test\"] max_provisioned_instances: 20 In this example, you cannot have more than 10 instances provisioned for \"normal\" queue, and 20 for the \"test\" queue. If a job cannot start because of this parameter, an error_message will be visible when running \"qstat -f\" or via the web interface Info Limit apply to all type of instance. Configure allowed_instance_types / excluded_instance_types if you also want to limit the type of EC2 instance than can be provisioned ).","title":"Restrict number of provisioned instances"},{"location":"security/manage-queue-acls/","text":"You can manage ACLs for each queue by configuring both allowed_users or excluded_users . These parameters can be configured as: List of allowed/excluded users: allowed_users: [\"user1\", \"user2\"] List of LDAP groups: allowed_users: [\"cn=mynewgroup,ou=Group,dc=soca,dc=local\"] List of username and LDAP groups: allowed_users: [\"user1\", \"cn=mynewgroup,ou=Group,dc=soca,dc=local\", \"user2\"] Restrict queue for some users \u00b6 Considering /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml queue_type: compute: queues: [\"normal\"] allowed_users: [] # empty list = all users can submit job excluded_users: [] # empty list = no restriction, [\"*\"] = only allowed_users can submit job ... test: queues: [\"high\", \"low\"] allowed_users: [] excluded_users: [\"user1\"] In this example, user1 can submit a job to \"normal\" queue but not on \"high\" or \"low\" queues. # Job submission does not work on \"high\" queue because user1 is on the excluded_users list pattern qsub -q high -- /bin/sleep 60 qsub: user1 is not authorized to use submit this job on the queue high. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml # Job submission is ok on \"normal\" queue qsub -q normal -- /bin/sleep 60 19.ip-30-0-2-29 allowed_users overrides excluded_users Job will go through if a user is present is both allowed_users and excluded_users lists. Restrict the queue for everyone except allowed_users \u00b6 excluded_users: [\"*\"] will prevent anyone to use the queue except for the list of allowed_users . In the example below, user1 is the only user authorized to submit job. queue_type: compute: queues: [\"normal\"] allowed_users: [\"user1\"] excluded_users: [\"*\"] Manage ACLs using LDAP groups \u00b6 SOCA will consider allowed_users or excluded_users as LDAP group if you did not specify them as list. Create a text file mynewgroup.ldif and add the following content (note we are adding our user1 as a group member) dn: cn=mynewgroup,ou=Group,dc=soca,dc=local objectClass: top objectClass: posixGroup cn: mynewgroup gidNumber: 6000 memberUid: user1 Create the group using ldapadd command ~ ldapadd -x -D cn=admin,dc=soca,dc=local -y /root/OpenLdapAdminPassword.txt -f mynewgroup.ldif adding new entry \"cn=mynewgroup,ou=Group,dc=soca,dc=local\" Run ldapsearch command to confirm your group has been created correctly and your user1 is part of it ~ ldapsearch -x -b cn=mynewgroup,ou=Group,dc=soca,dc=local -LLL dn: cn=mynewgroup,ou=Group,dc=soca,dc=local objectClass: top objectClass: posixGroup cn: mynewgroup gidNumber: 6000 memberUid: user1 Let's configure our queue to reject all users: allowed_users: [] excluded_users: [\"*\"] Confirm user1 can't submit any job: qsub -q high -- /bin/sleep 60 qsub: user1 is not authorized to use submit this job on the queue high. Contact your HPC admin and update /apps/soca/cluster_manager/settings/queue_mapping.yml Edit your allowed_users and specify your LDAP group: allowed_users: [\"cn=mynewgroup,ou=Group,dc=soca,dc=local\"] excluded_users: [\"*\"] Verify user1 can submit job: qsub -q high -- /bin/sleep 60 22.ip-30-0-2-29 Let's now assume you have a user2 . Confirm this user can't submit job qsub -q high -- /bin/sleep 60 qsub: user2 is not authorized to use submit this job on the queue high. Contact your HPC admin and update /apps/soca/cluster_manager/settings/queue_mapping.yml Create a new ldif file (add_new_user.ldif) and add user2 to your group dn: cn=mynewgroup,ou=Group,dc=soca,dc=local changetype: modify add: memberUid memberUid: user2 Execute the command ~ ldapadd -x -D cn=admin,dc=soca,dc=local -y /root/OpenLdapAdminPassword.txt -f add_new_user.ldif modifying entry \"cn=mynewgroup,ou=Group,dc=soca,dc=local Confirm both users are part of the group: ldapsearch -x -b cn=mynewgroup,ou=Group,dc=soca,dc=local -LLL dn: cn=mynewgroup,ou=Group,dc=soca,dc=local objectClass: top objectClass: posixGroup cn: mynewgroup gidNumber: 6000 memberUid: user1 memberUid: user2 Finally, confirm user2 is now authorized to submit job: qsub -q high -- /bin/sleep 60 23.ip-30-0-2-29 On the other side, you can also prevent users from a LDAP group to use the queue by specifying the ldap group as \"excluded_users\" allowed_users: [] excluded_users: [\"cn=mynewgroup,ou=Group,dc=soca,dc=local\"] Check the logs \u00b6 Scheduler hooks are located on /var/spool/pbs/server_logs/ Code \u00b6 The hook file can be found under /apps/soca/cluster_hooks/$SOCA_CONFIGURATION/queuejob/check_queue_acls.py on your Scale-Out Computing on AWS cluster) Disable the hook \u00b6 You can disable the hook by running the following command on the scheduler host (as root): user@host: qmgr -c \"delete hook check_queue_acls event=queuejob\"","title":"Manage Access Lists at queue level"},{"location":"security/manage-queue-acls/#restrict-queue-for-some-users","text":"Considering /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml queue_type: compute: queues: [\"normal\"] allowed_users: [] # empty list = all users can submit job excluded_users: [] # empty list = no restriction, [\"*\"] = only allowed_users can submit job ... test: queues: [\"high\", \"low\"] allowed_users: [] excluded_users: [\"user1\"] In this example, user1 can submit a job to \"normal\" queue but not on \"high\" or \"low\" queues. # Job submission does not work on \"high\" queue because user1 is on the excluded_users list pattern qsub -q high -- /bin/sleep 60 qsub: user1 is not authorized to use submit this job on the queue high. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml # Job submission is ok on \"normal\" queue qsub -q normal -- /bin/sleep 60 19.ip-30-0-2-29 allowed_users overrides excluded_users Job will go through if a user is present is both allowed_users and excluded_users lists.","title":"Restrict queue for some users"},{"location":"security/manage-queue-acls/#restrict-the-queue-for-everyone-except-allowed_users","text":"excluded_users: [\"*\"] will prevent anyone to use the queue except for the list of allowed_users . In the example below, user1 is the only user authorized to submit job. queue_type: compute: queues: [\"normal\"] allowed_users: [\"user1\"] excluded_users: [\"*\"]","title":"Restrict the queue for everyone except allowed_users"},{"location":"security/manage-queue-acls/#manage-acls-using-ldap-groups","text":"SOCA will consider allowed_users or excluded_users as LDAP group if you did not specify them as list. Create a text file mynewgroup.ldif and add the following content (note we are adding our user1 as a group member) dn: cn=mynewgroup,ou=Group,dc=soca,dc=local objectClass: top objectClass: posixGroup cn: mynewgroup gidNumber: 6000 memberUid: user1 Create the group using ldapadd command ~ ldapadd -x -D cn=admin,dc=soca,dc=local -y /root/OpenLdapAdminPassword.txt -f mynewgroup.ldif adding new entry \"cn=mynewgroup,ou=Group,dc=soca,dc=local\" Run ldapsearch command to confirm your group has been created correctly and your user1 is part of it ~ ldapsearch -x -b cn=mynewgroup,ou=Group,dc=soca,dc=local -LLL dn: cn=mynewgroup,ou=Group,dc=soca,dc=local objectClass: top objectClass: posixGroup cn: mynewgroup gidNumber: 6000 memberUid: user1 Let's configure our queue to reject all users: allowed_users: [] excluded_users: [\"*\"] Confirm user1 can't submit any job: qsub -q high -- /bin/sleep 60 qsub: user1 is not authorized to use submit this job on the queue high. Contact your HPC admin and update /apps/soca/cluster_manager/settings/queue_mapping.yml Edit your allowed_users and specify your LDAP group: allowed_users: [\"cn=mynewgroup,ou=Group,dc=soca,dc=local\"] excluded_users: [\"*\"] Verify user1 can submit job: qsub -q high -- /bin/sleep 60 22.ip-30-0-2-29 Let's now assume you have a user2 . Confirm this user can't submit job qsub -q high -- /bin/sleep 60 qsub: user2 is not authorized to use submit this job on the queue high. Contact your HPC admin and update /apps/soca/cluster_manager/settings/queue_mapping.yml Create a new ldif file (add_new_user.ldif) and add user2 to your group dn: cn=mynewgroup,ou=Group,dc=soca,dc=local changetype: modify add: memberUid memberUid: user2 Execute the command ~ ldapadd -x -D cn=admin,dc=soca,dc=local -y /root/OpenLdapAdminPassword.txt -f add_new_user.ldif modifying entry \"cn=mynewgroup,ou=Group,dc=soca,dc=local Confirm both users are part of the group: ldapsearch -x -b cn=mynewgroup,ou=Group,dc=soca,dc=local -LLL dn: cn=mynewgroup,ou=Group,dc=soca,dc=local objectClass: top objectClass: posixGroup cn: mynewgroup gidNumber: 6000 memberUid: user1 memberUid: user2 Finally, confirm user2 is now authorized to submit job: qsub -q high -- /bin/sleep 60 23.ip-30-0-2-29 On the other side, you can also prevent users from a LDAP group to use the queue by specifying the ldap group as \"excluded_users\" allowed_users: [] excluded_users: [\"cn=mynewgroup,ou=Group,dc=soca,dc=local\"]","title":"Manage ACLs using LDAP groups"},{"location":"security/manage-queue-acls/#check-the-logs","text":"Scheduler hooks are located on /var/spool/pbs/server_logs/","title":"Check the logs"},{"location":"security/manage-queue-acls/#code","text":"The hook file can be found under /apps/soca/cluster_hooks/$SOCA_CONFIGURATION/queuejob/check_queue_acls.py on your Scale-Out Computing on AWS cluster)","title":"Code"},{"location":"security/manage-queue-acls/#disable-the-hook","text":"You can disable the hook by running the following command on the scheduler host (as root): user@host: qmgr -c \"delete hook check_queue_acls event=queuejob\"","title":"Disable the hook"},{"location":"security/manage-queue-instance-types/","text":"You can manage the EC2 instance types allowed for each queue by configuring both allowed_instance_types and excluded_instance_types . This allows you to restrict which instance types or family of instances are allowed to be used for jobs on a per queue basis by either white listing instance types or blocking them. Default settings By default, users can provision any type of instance. There are no restrictions configured out of the box. These parameters can be configured as: List of allowed EC2 instance types for a queue: allowed_instance_types: [\"c5.4xlarge\", \"r5.2xlarge\"] List of excluded EC2 instance types for a queue: excluded_instance_types: [\"f1.16xlarge\", \"i3.2xlarge\"] Allow EC2 instance types by specific type or by instance family: allowed_instance_types: [\"c5\", \"r5.2xlarge\"] Exclude EC2 instance types by specific type or by instance family: excluded_instance_types: [\"f1.16xlarge\", \"i3\"] Instance family specification uses the exact name of the instance family. If you add c5 to the allowed instance list c5 instances will be allowed. c5n instances will be blocked unless c5n is added to the allowed_instance_types to allow c5n instances to run. Allow only compute optimized EC2 instances \u00b6 Considering /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml queue_type: compute: queues: [\"normal\"] allowed_instance_types: [\"c5\", \"c5n\"] excluded_instance_types: [] ... test: queues: [\"test\"] allowed_instance_types: [\"c5.4xlarge\"] excluded_instance_types: [] In this example, only EC2 instance types in the c5 and c5n families can be used for jobs submitted to the normal queue. For the test queue only c5.4xlarge instance type will be allowed. # Job submission to queue \"normal\" using instance type i3.2xlarge is blocked qsub -q normal -l instance_type=i3.2xlarge -- /bin/echo test qsub: i3.2xlarge is not a valid instance type for the job queue normal. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml # Job submission to queue \"normal\" using instance from c5 family is allowed. qsub -q normal -l instance_type=c5.2xlarge -- /bin/echo test 15.ip-110-0-12-28 # Job submission to \"test\" queue only allowed if using c5.4xlarge instance type qsub -q test -l instance_type=c5.2xlarge -- /bin/echo test qsub: c5.2xlarge is not a valid instance type for the job queue test. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml qsub -q test -l instance_type=c5.4xlarge -- /bin/echo test 16.ip-110-0-12-28 Instance types in excluded_instance_types will be blocked even if they appear in allowed_instance_types If an instance type or family appears in both the excluded_instance_types list as well as allowed_instance_types for a queue, the excluded_instance_types setting takes priority. Block users from using specific EC2 instance types \u00b6 Considering /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml queue_type: compute: queues: [\"normal\"] allowed_instance_types: [] excluded_instance_types: [\"f1\",\"g4.16xlarge\",\"g3\"] ... test: queues: [\"test\"] allowed_instance_types: [] excluded_instance_types: [\"f1\",\"g4dn\"] In this example the normal queue will not allow instances in the f1 and g3 family as well as g4.16xlarge instanct types. The test queue will not allow instances in the f1 and g4 family. # Job submission to queue \"normal\" using instance type in f1 family is blocked qsub -q normal -l instance_type=f1.2xlarge -- /bin/echo test qsub: f1.2xlarge is not a valid instance type for the job queue normal. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml # Job submission to queue \"test\" using instance type in g4dn is blocked qsub -q normal -l instance_type=g4dn.xlarge -- /bin/echo test qsub: g4dn.xlarge is not a valid instance type for the job queue test. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml Check the logs \u00b6 Scheduler hooks are located on /var/spool/pbs/server_logs/ Code \u00b6 The hook file can be found under /apps/soca/cluster_hooks/$SOCA_CONFIGURATION/queuejob/check_queue_instance_types.py on your Scale-Out Computing on AWS cluster) Disable the hook \u00b6 You can disable the hook by running the following command on the scheduler host (as root): user@host: qmgr -c \"delete hook check_queue_instance_types event=queuejob\"","title":"Restrict provisioning of specific instance type"},{"location":"security/manage-queue-instance-types/#allow-only-compute-optimized-ec2-instances","text":"Considering /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml queue_type: compute: queues: [\"normal\"] allowed_instance_types: [\"c5\", \"c5n\"] excluded_instance_types: [] ... test: queues: [\"test\"] allowed_instance_types: [\"c5.4xlarge\"] excluded_instance_types: [] In this example, only EC2 instance types in the c5 and c5n families can be used for jobs submitted to the normal queue. For the test queue only c5.4xlarge instance type will be allowed. # Job submission to queue \"normal\" using instance type i3.2xlarge is blocked qsub -q normal -l instance_type=i3.2xlarge -- /bin/echo test qsub: i3.2xlarge is not a valid instance type for the job queue normal. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml # Job submission to queue \"normal\" using instance from c5 family is allowed. qsub -q normal -l instance_type=c5.2xlarge -- /bin/echo test 15.ip-110-0-12-28 # Job submission to \"test\" queue only allowed if using c5.4xlarge instance type qsub -q test -l instance_type=c5.2xlarge -- /bin/echo test qsub: c5.2xlarge is not a valid instance type for the job queue test. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml qsub -q test -l instance_type=c5.4xlarge -- /bin/echo test 16.ip-110-0-12-28 Instance types in excluded_instance_types will be blocked even if they appear in allowed_instance_types If an instance type or family appears in both the excluded_instance_types list as well as allowed_instance_types for a queue, the excluded_instance_types setting takes priority.","title":"Allow only compute optimized EC2 instances"},{"location":"security/manage-queue-instance-types/#block-users-from-using-specific-ec2-instance-types","text":"Considering /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml queue_type: compute: queues: [\"normal\"] allowed_instance_types: [] excluded_instance_types: [\"f1\",\"g4.16xlarge\",\"g3\"] ... test: queues: [\"test\"] allowed_instance_types: [] excluded_instance_types: [\"f1\",\"g4dn\"] In this example the normal queue will not allow instances in the f1 and g3 family as well as g4.16xlarge instanct types. The test queue will not allow instances in the f1 and g4 family. # Job submission to queue \"normal\" using instance type in f1 family is blocked qsub -q normal -l instance_type=f1.2xlarge -- /bin/echo test qsub: f1.2xlarge is not a valid instance type for the job queue normal. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml # Job submission to queue \"test\" using instance type in g4dn is blocked qsub -q normal -l instance_type=g4dn.xlarge -- /bin/echo test qsub: g4dn.xlarge is not a valid instance type for the job queue test. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml","title":"Block users from using specific EC2 instance types"},{"location":"security/manage-queue-instance-types/#check-the-logs","text":"Scheduler hooks are located on /var/spool/pbs/server_logs/","title":"Check the logs"},{"location":"security/manage-queue-instance-types/#code","text":"The hook file can be found under /apps/soca/cluster_hooks/$SOCA_CONFIGURATION/queuejob/check_queue_instance_types.py on your Scale-Out Computing on AWS cluster)","title":"Code"},{"location":"security/manage-queue-instance-types/#disable-the-hook","text":"You can disable the hook by running the following command on the scheduler host (as root): user@host: qmgr -c \"delete hook check_queue_instance_types event=queuejob\"","title":"Disable the hook"},{"location":"security/manage-queue-restricted-parameters/","text":"When submitting a job, users can override the default parameters configured for the queue ( click here to see a list of all parameters supported by SOCA ). For security, compliance or cost reasons, you may want prevent users to override these default parameters by configuring restricted_parameters on your queue_mapping.yml Prevent user to choose a different instance type \u00b6 Considering /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml queue_type: compute: queues: [\"normal\", \"low\"] instance_type: \"c5.large\" restricted_parameters: [\"instance_type\"] ... In this example, a job will be rejected if a user try to specify the instance_type parameter when using the normal or low queues. In this particular case, any job sent to the normal or low queue will be forced to use c5.large instance, which is the default instance type configured by HPC admins. qsub -q normal -l instance_type=m5.24xlarge -- /bin/echo test qsub: instance_type is a restricted parameter and can't be configure by the user. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml Need to approve more than one instance type/family? Read the documentation if you want to limit users to a list of multiple instance types Prevent user to provision additional storage \u00b6 Considering /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml queue_type: compute: queues: [\"normal\", \"low\"] scratch_size: \"200\" restricted_parameters: [\"scratch_size\"] ... In this example, a job will be rejected if a user try to specify the scratch_size parameter when using the normal or low queues. In this particular case, any job sent to the normal or low queue will be forced to use a 200 GB EBS disk as /scratch partition. Users are no longer able to provision more storage than what's allocated to them. qsub -q normal -l scratch_size=550 -- /bin/echo test qsub: scratch_size is a restricted parameter and can't be configure by the user. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml Combine multiple restrictions \u00b6 Considering /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml queue_type: compute: queues: [\"normal\", \"low\"] scratch_size: \"200\" restricted_parameters:[\"instance_type\", \"fsx_lustre_bucket\", \"scratch_size\"] ... In this example, a job will be rejected if a user try to change either instance_type , fsx_lustre_bucket or scratch_size parameters. qsub -q normal -l fsx_lustre_bucket=mybucket -- /bin/echo test qsub: fsx_lustre_bucket is a restricted parameter and can't be configure by the user. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml Check the logs \u00b6 Scheduler hooks are located on /var/spool/pbs/server_logs/ Code \u00b6 The hook file can be found under /apps/soca/cluster_hooks/$SOCA_CONFIGURATION/queuejob/check_queue_restricted_parameters.py on your Scale-Out Computing on AWS cluster) Disable the hook \u00b6 You can disable the hook by running the following command on the scheduler host (as root): user@host: qmgr -c \"delete hook check_queue_restricted_parameters event=queuejob\"","title":"Prevent user to change specific parameters"},{"location":"security/manage-queue-restricted-parameters/#prevent-user-to-choose-a-different-instance-type","text":"Considering /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml queue_type: compute: queues: [\"normal\", \"low\"] instance_type: \"c5.large\" restricted_parameters: [\"instance_type\"] ... In this example, a job will be rejected if a user try to specify the instance_type parameter when using the normal or low queues. In this particular case, any job sent to the normal or low queue will be forced to use c5.large instance, which is the default instance type configured by HPC admins. qsub -q normal -l instance_type=m5.24xlarge -- /bin/echo test qsub: instance_type is a restricted parameter and can't be configure by the user. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml Need to approve more than one instance type/family? Read the documentation if you want to limit users to a list of multiple instance types","title":"Prevent user to choose a different instance type"},{"location":"security/manage-queue-restricted-parameters/#prevent-user-to-provision-additional-storage","text":"Considering /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml queue_type: compute: queues: [\"normal\", \"low\"] scratch_size: \"200\" restricted_parameters: [\"scratch_size\"] ... In this example, a job will be rejected if a user try to specify the scratch_size parameter when using the normal or low queues. In this particular case, any job sent to the normal or low queue will be forced to use a 200 GB EBS disk as /scratch partition. Users are no longer able to provision more storage than what's allocated to them. qsub -q normal -l scratch_size=550 -- /bin/echo test qsub: scratch_size is a restricted parameter and can't be configure by the user. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml","title":"Prevent user to provision additional storage"},{"location":"security/manage-queue-restricted-parameters/#combine-multiple-restrictions","text":"Considering /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml queue_type: compute: queues: [\"normal\", \"low\"] scratch_size: \"200\" restricted_parameters:[\"instance_type\", \"fsx_lustre_bucket\", \"scratch_size\"] ... In this example, a job will be rejected if a user try to change either instance_type , fsx_lustre_bucket or scratch_size parameters. qsub -q normal -l fsx_lustre_bucket=mybucket -- /bin/echo test qsub: fsx_lustre_bucket is a restricted parameter and can't be configure by the user. Contact your HPC admin and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml","title":"Combine multiple restrictions"},{"location":"security/manage-queue-restricted-parameters/#check-the-logs","text":"Scheduler hooks are located on /var/spool/pbs/server_logs/","title":"Check the logs"},{"location":"security/manage-queue-restricted-parameters/#code","text":"The hook file can be found under /apps/soca/cluster_hooks/$SOCA_CONFIGURATION/queuejob/check_queue_restricted_parameters.py on your Scale-Out Computing on AWS cluster)","title":"Code"},{"location":"security/manage-queue-restricted-parameters/#disable-the-hook","text":"You can disable the hook by running the following command on the scheduler host (as root): user@host: qmgr -c \"delete hook check_queue_restricted_parameters event=queuejob\"","title":"Disable the hook"},{"location":"security/update-soca-dns-ssl-certificate/","text":"By default, Scale-Out Computing on AWS will use a non-friendly DNS name and create a unique certificate to enable access through your HTTPS endpoint. Because it's a self-signed certificate, browsers won't recognized it and you will get a security warning on your first connection. In this page, we will see how you can update Scale-Out Computing on AWS to match your company domain name. Create a new DNS record for Scale-Out Computing on AWS \u00b6 For this example, let's assume I want to use https://demo.soca.dev . First locate the DNS associated to your ALB endpoint using the AWS console. Create a new CNAME record which point to your ALB endpoint. Once done, validate your DNS is working properly using the nslookup command. user@host: nslookup demo.soca.dev Non-authoritative answer: demo.soca.dev canonical name = soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com. Name: soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com Address: 52 .40.2.185 Name: soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com Address: 54 .68.240.4 Name: soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com Address: 52 .27.180.89 Upload your SSL certificate to ACM \u00b6 Now that your friendly DNS is running, you will need to update the default ALB certificate to match your new domain. This assume you have a valid SSL certificate signed by a valid Certificate Authority (Symantec, Digicert ...) To upload your certificate, visit the AWS Certificate Manager (ACM) bash and click \"Import a Certificate\". Enter your private key, certificate and certificate chain (optional), then click Import. Once the import is complete, note your certificate identifier. Update your ALB with the new certificate \u00b6 Navigate to your Scale-Out Computing on AWS Load Balancer and choose \"Listeners\" tab. Select your HTTPS listener and click 'Edit' button. Change the default certificate to point to your new certificate and save your change. Update your default domain for DCV \u00b6 Now that you have updated your domain, you must also update DCV to point to the new DNS. Open your Secret Manager bash and select your Scale-Out Computing on AWS cluster configuration. Click \"Retrieve Secret Value\" and then \"Edit\". Find the entry \"LoadBalancerName\" and update the value with your new DNS name (demo.soca.dev in my case) then click Save Validate everything \u00b6 Now that you have your friendly DNS and SSL certificate configured, it's time to test. Visit your new DNS ( https://demo.soca.dev in my case) and make sure you can access Scale-Out Computing on AWS correctly. Make sure your browser is detecting your new SSL certificate correctly. Finally, create a new DCV session and verify the endpoint is using your new DNS name","title":"Change your DNS name and SSL certificate"},{"location":"security/update-soca-dns-ssl-certificate/#create-a-new-dns-record-for-scale-out-computing-on-aws","text":"For this example, let's assume I want to use https://demo.soca.dev . First locate the DNS associated to your ALB endpoint using the AWS console. Create a new CNAME record which point to your ALB endpoint. Once done, validate your DNS is working properly using the nslookup command. user@host: nslookup demo.soca.dev Non-authoritative answer: demo.soca.dev canonical name = soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com. Name: soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com Address: 52 .40.2.185 Name: soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com Address: 54 .68.240.4 Name: soca-cluster-test-rc-1-viewer-1928842383.us-west-2.elb.amazonaws.com Address: 52 .27.180.89","title":"Create a new DNS record for Scale-Out Computing on AWS"},{"location":"security/update-soca-dns-ssl-certificate/#upload-your-ssl-certificate-to-acm","text":"Now that your friendly DNS is running, you will need to update the default ALB certificate to match your new domain. This assume you have a valid SSL certificate signed by a valid Certificate Authority (Symantec, Digicert ...) To upload your certificate, visit the AWS Certificate Manager (ACM) bash and click \"Import a Certificate\". Enter your private key, certificate and certificate chain (optional), then click Import. Once the import is complete, note your certificate identifier.","title":"Upload your SSL certificate to ACM"},{"location":"security/update-soca-dns-ssl-certificate/#update-your-alb-with-the-new-certificate","text":"Navigate to your Scale-Out Computing on AWS Load Balancer and choose \"Listeners\" tab. Select your HTTPS listener and click 'Edit' button. Change the default certificate to point to your new certificate and save your change.","title":"Update your ALB with the new certificate"},{"location":"security/update-soca-dns-ssl-certificate/#update-your-default-domain-for-dcv","text":"Now that you have updated your domain, you must also update DCV to point to the new DNS. Open your Secret Manager bash and select your Scale-Out Computing on AWS cluster configuration. Click \"Retrieve Secret Value\" and then \"Edit\". Find the entry \"LoadBalancerName\" and update the value with your new DNS name (demo.soca.dev in my case) then click Save","title":"Update your default domain for DCV"},{"location":"security/update-soca-dns-ssl-certificate/#validate-everything","text":"Now that you have your friendly DNS and SSL certificate configured, it's time to test. Visit your new DNS ( https://demo.soca.dev in my case) and make sure you can access Scale-Out Computing on AWS correctly. Make sure your browser is detecting your new SSL certificate correctly. Finally, create a new DCV session and verify the endpoint is using your new DNS name","title":"Validate everything"},{"location":"security/use-custom-sgs-roles/","text":"On this page, we will see how you can deploy your compute nodes with custom security groups or IAM role Security Groups \u00b6 By default, compute nodes are provisioned with a single security group designed to enable all connectivity between the various services used by SOCA. You can extend this capability and specify up to 4 additional security groups for your jobs. For security reasons, HPC admins must manually enable a list of security group IDs authorized for the queue. To do that, edit /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml and make sure security_groups is not listed as restricted_parameters and verify that your security groups ids are safelisted under allowed_security_group_ids : queue_type: compute: queues: [\"myqueue\"] .... # make sure securitys_group is not a restricted parameter restricted_parameters: [] # List all the security group ids allowed for your queue allowed_security_group_ids: [\"sg-abcde\",\"sg-fghijk\"] Note We recommend to also configure queues access lists (ACLs) to limit which individuals/LDAP groups are authorized to configure custom security groups Submit a job with one extra security group \u00b6 Submit a job as you would normally do and add -l security_group=<Security_group_id> to the submission command. For example: qsub -l security_groups=sg-0401a055a0e503758 -- /bin/echo test 0.ip-70-0-0-54 You can validate your job configuration by running qstat -f qstat -f 0 | grep Resource ... Resource_List.security_groups = sg-0401a055a0e503758 ... Alternatively, you can also verify if the nodes provisioned have assigned your custom security group successfully by checking the \"Security\" tab on EC2 console. Please note the ComputeNodeSG will always be assigned to your job as this is the default security group created by SOCA. Security group not authorized qsub: Security group sg-0401a055a0e503758 is not authorized for this queue. Please enable it on /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml To fix this error, simply safelist the security group id on the queue_mapping.py section assigned to your queue: allowed_security_group_ids: [\"sg-0401a055a0e503758\"] Submit a job with more than one extra security group \u00b6 You can specify up to 4 additional security groups by using + delimiter. First, make sure all the security group ids are authorized on the queue. To do so, edit the queue_mapping.yaml and add all the security group ids you are planning to use: allowed_security_group_ids: [\"sg-0401a055a0e503758\",\"sg-0b8b0a16f5ecdae46\",\"sg-097df4ebb9eb7679a\"] Finally submit a test job and add -l security_groups=<sg_1>+<sg_2>+....+<sg_n> to your job submission command: qsub -l security_groups=sg-0401a055a0e503758+sg-0b8b0a16f5ecdae46+sg-097df4ebb9eb7679a -- /bin/echo test 1.ip-70-0-0-54 Validate the EC2 nodes use the correct security groups by running qstat -f or directly check the EC2 console. Please note the ComputeNodeSG will always be assigned to your job as this is the default security group created by SOCA. Note Due to AWS limitation, you can configure up to 4 additional security groups Errors and Troubleshooting \u00b6 If your jobs can't run, refer to this link for troubleshooting procedure or run qstat -f <job_id> and look for error_message . IAM role \u00b6 By default, compute nodes are provisioned with an IAM role designed to enable all connectivity between the various services used by SOCA. You can extend this capability and specify your own IAM role for your simulation. For security reasons, HPC admins must manually enable a list of IAM instance profile authorized for the queue. To do that, edit /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml and make sure instance_profile is not listed as restricted_parameters and verify that you instance profiles are safelisted under allowed_instance_profiles : queue_type: compute: queues: [\"myqueue\"] .... # make sure instance_profile is not a restricted parameter restricted_parameters: [] # List all the IAM instance profile name allowed for your queue allowed_instance_profiles: [\"MyProfile\"] Note We recommend to also configure queues access lists (ACLs) to limit which individuals/LDAP groups are authorized to configure custom IAM instance profiles Create your own IAM role \u00b6 Enable Trust Relationship for SSM \u00b6 On the custom IAM role, make sure to enable ssm.amazonaws.com as trusted entity (select IAM role and click Trust Relationships) Copy base permissions from default IAM role \u00b6 Then locate the default IAM role for your SOCA cluster and copy all the permissions to the new IAM role. The name of the base IAM role will always be soca-<YourClusterName>-ComputeNodeRole<UniqueID> . Locate the base role and copy the permissions below to your custom IAM role: We recommend opening the IAM policies as JSON and simply copy/paste the content to the custom IAM role Now your custom IAM share the required permission by SOCA (red section) as well as a list of unique permissions specific to this custom role (blue section) Enable iam:PassRole permission \u00b6 Finally, for security reasons, SOCA does not let you use other IAM roles by default, and you will have to grant permission to the Scheduler role (not ComputeNode). The name of the scheduler IAM role will always be soca-<YourClusterName>-SchedulerRole<UniqueID> . Once selected, click \"Add Inline Policy\", click JSON and paste the following (replace \"Resource\" with the ARN of your IAM role): { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : \"iam:PassRole\" , \"Resource\" : \"arn:aws:iam::<AWS_AccountID>:role/<RoleName>\" } ] } You can retrieve the ARN of the role you want to use via the IAM role console Submit a job with a custom IAM role \u00b6 Now that your IAM role is created, retrieve the name of the Instance Profile (note: name of IAM role and Instance profile can sometimes be different. Make sure you retrieve the instance profile name -in red- and not the actual role name) Enable this role on /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml allowed_instance_profiles: [\"MyCustomIAMSOCARole\"] Submit a job with -l instance_profile parameters: qsub -l instance_profile=MyCustomIAMSOCARole -- /bin/echo test 4.ip-70-0-0-54 You can validate your job configuration by running qstat -f qstat -f 4 | grep Resource ... Resource_List.instance_profile = MyCustomIAMSOCARole ... Alternatively, you can also verify if the nodes provisioned have assigned your custom IAM role successfully by checking the \"Security\" tab on EC2 console. Profile not authorized qsub: IAM instance profile MyCustomIAMSOCARole is not authorized for this queue. Please enable it on /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml To fix this error, simply safelist the profile name on the queue_mapping.py section assigned to your queue: allowed_instance_profiles: [\"MyCustomIAMSOCARole\"]","title":"Configure custom Security Groups or IAM role"},{"location":"security/use-custom-sgs-roles/#security-groups","text":"By default, compute nodes are provisioned with a single security group designed to enable all connectivity between the various services used by SOCA. You can extend this capability and specify up to 4 additional security groups for your jobs. For security reasons, HPC admins must manually enable a list of security group IDs authorized for the queue. To do that, edit /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml and make sure security_groups is not listed as restricted_parameters and verify that your security groups ids are safelisted under allowed_security_group_ids : queue_type: compute: queues: [\"myqueue\"] .... # make sure securitys_group is not a restricted parameter restricted_parameters: [] # List all the security group ids allowed for your queue allowed_security_group_ids: [\"sg-abcde\",\"sg-fghijk\"] Note We recommend to also configure queues access lists (ACLs) to limit which individuals/LDAP groups are authorized to configure custom security groups","title":"Security Groups"},{"location":"security/use-custom-sgs-roles/#submit-a-job-with-one-extra-security-group","text":"Submit a job as you would normally do and add -l security_group=<Security_group_id> to the submission command. For example: qsub -l security_groups=sg-0401a055a0e503758 -- /bin/echo test 0.ip-70-0-0-54 You can validate your job configuration by running qstat -f qstat -f 0 | grep Resource ... Resource_List.security_groups = sg-0401a055a0e503758 ... Alternatively, you can also verify if the nodes provisioned have assigned your custom security group successfully by checking the \"Security\" tab on EC2 console. Please note the ComputeNodeSG will always be assigned to your job as this is the default security group created by SOCA. Security group not authorized qsub: Security group sg-0401a055a0e503758 is not authorized for this queue. Please enable it on /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml To fix this error, simply safelist the security group id on the queue_mapping.py section assigned to your queue: allowed_security_group_ids: [\"sg-0401a055a0e503758\"]","title":"Submit a job with one extra security group"},{"location":"security/use-custom-sgs-roles/#submit-a-job-with-more-than-one-extra-security-group","text":"You can specify up to 4 additional security groups by using + delimiter. First, make sure all the security group ids are authorized on the queue. To do so, edit the queue_mapping.yaml and add all the security group ids you are planning to use: allowed_security_group_ids: [\"sg-0401a055a0e503758\",\"sg-0b8b0a16f5ecdae46\",\"sg-097df4ebb9eb7679a\"] Finally submit a test job and add -l security_groups=<sg_1>+<sg_2>+....+<sg_n> to your job submission command: qsub -l security_groups=sg-0401a055a0e503758+sg-0b8b0a16f5ecdae46+sg-097df4ebb9eb7679a -- /bin/echo test 1.ip-70-0-0-54 Validate the EC2 nodes use the correct security groups by running qstat -f or directly check the EC2 console. Please note the ComputeNodeSG will always be assigned to your job as this is the default security group created by SOCA. Note Due to AWS limitation, you can configure up to 4 additional security groups","title":"Submit a job with more than one extra security group"},{"location":"security/use-custom-sgs-roles/#errors-and-troubleshooting","text":"If your jobs can't run, refer to this link for troubleshooting procedure or run qstat -f <job_id> and look for error_message .","title":"Errors and Troubleshooting"},{"location":"security/use-custom-sgs-roles/#iam-role","text":"By default, compute nodes are provisioned with an IAM role designed to enable all connectivity between the various services used by SOCA. You can extend this capability and specify your own IAM role for your simulation. For security reasons, HPC admins must manually enable a list of IAM instance profile authorized for the queue. To do that, edit /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml and make sure instance_profile is not listed as restricted_parameters and verify that you instance profiles are safelisted under allowed_instance_profiles : queue_type: compute: queues: [\"myqueue\"] .... # make sure instance_profile is not a restricted parameter restricted_parameters: [] # List all the IAM instance profile name allowed for your queue allowed_instance_profiles: [\"MyProfile\"] Note We recommend to also configure queues access lists (ACLs) to limit which individuals/LDAP groups are authorized to configure custom IAM instance profiles","title":"IAM role"},{"location":"security/use-custom-sgs-roles/#create-your-own-iam-role","text":"","title":"Create your own IAM role"},{"location":"security/use-custom-sgs-roles/#enable-trust-relationship-for-ssm","text":"On the custom IAM role, make sure to enable ssm.amazonaws.com as trusted entity (select IAM role and click Trust Relationships)","title":"Enable Trust Relationship for SSM"},{"location":"security/use-custom-sgs-roles/#copy-base-permissions-from-default-iam-role","text":"Then locate the default IAM role for your SOCA cluster and copy all the permissions to the new IAM role. The name of the base IAM role will always be soca-<YourClusterName>-ComputeNodeRole<UniqueID> . Locate the base role and copy the permissions below to your custom IAM role: We recommend opening the IAM policies as JSON and simply copy/paste the content to the custom IAM role Now your custom IAM share the required permission by SOCA (red section) as well as a list of unique permissions specific to this custom role (blue section)","title":"Copy base permissions from default IAM role"},{"location":"security/use-custom-sgs-roles/#enable-iampassrole-permission","text":"Finally, for security reasons, SOCA does not let you use other IAM roles by default, and you will have to grant permission to the Scheduler role (not ComputeNode). The name of the scheduler IAM role will always be soca-<YourClusterName>-SchedulerRole<UniqueID> . Once selected, click \"Add Inline Policy\", click JSON and paste the following (replace \"Resource\" with the ARN of your IAM role): { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"VisualEditor0\" , \"Effect\" : \"Allow\" , \"Action\" : \"iam:PassRole\" , \"Resource\" : \"arn:aws:iam::<AWS_AccountID>:role/<RoleName>\" } ] } You can retrieve the ARN of the role you want to use via the IAM role console","title":"Enable iam:PassRole permission"},{"location":"security/use-custom-sgs-roles/#submit-a-job-with-a-custom-iam-role","text":"Now that your IAM role is created, retrieve the name of the Instance Profile (note: name of IAM role and Instance profile can sometimes be different. Make sure you retrieve the instance profile name -in red- and not the actual role name) Enable this role on /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml allowed_instance_profiles: [\"MyCustomIAMSOCARole\"] Submit a job with -l instance_profile parameters: qsub -l instance_profile=MyCustomIAMSOCARole -- /bin/echo test 4.ip-70-0-0-54 You can validate your job configuration by running qstat -f qstat -f 4 | grep Resource ... Resource_List.instance_profile = MyCustomIAMSOCARole ... Alternatively, you can also verify if the nodes provisioned have assigned your custom IAM role successfully by checking the \"Security\" tab on EC2 console. Profile not authorized qsub: IAM instance profile MyCustomIAMSOCARole is not authorized for this queue. Please enable it on /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml To fix this error, simply safelist the profile name on the queue_mapping.py section assigned to your queue: allowed_instance_profiles: [\"MyCustomIAMSOCARole\"]","title":"Submit a job with a custom IAM role"},{"location":"storage/","text":"About \u00b6 This section list all storage options available on SOCA (EFS, EBS, FSx ...) Refer to the left sidebar for more detailed resources","title":"About"},{"location":"storage/#about","text":"This section list all storage options available on SOCA (EFS, EBS, FSx ...) Refer to the left sidebar for more detailed resources","title":"About"},{"location":"storage/backend-storage-options/","text":"Scale-Out Computing on AWS gives you the flexibility to customize your storage backend based on your requirements You can customize the root partition size You can provision a local scratch partition You can deploy standard SSD (gp2) or IO Optimized SSD (io1) volumes Scale-Out Computing on AWS automatically leverages instance store disk(s) as scratch partition when applicable In term of performance: Instance Store > EBS SSD IO > EBS SSD Standard > EFS Refer to this link to learn more about EBS volumes. EFS (shared) partitions \u00b6 /data partition \u00b6 /data is an Elastic File System partition mounted on all hosts. This contains the home directory of your LDAP users ($HOME = /data/home/<USERNAME> ). This partition is persistent . Avoid using this partition if your simulation is disk I/O intensive (use /scratch instead) /apps partition \u00b6 /apps is an Elastic File System partition mounted on all hosts. This partition is designed to host all your CFD/FEA/EDA/Mathematical applications. This partition is persistent . Avoid using this partition if your simulation is disk I/O intensive (use /scratch instead) The corresponding EFS file system is deployed with bursting Throughput mode which depends on burst credits. /apps has cron jobs that monitor the status of the cluster, scale-up and scale-down the cluster, and is also running the web application. If the cluster stays up and running for 1+ months and not much additional applications are installed under /apps, then EFS file system might run out of burst credits which could impact the web application. Starting v2.6.0, the solution deploys CloudWatch Alarms to monitor the burst credits for /apps EFS file system. When burst credits are close to be depleted, a lambda function is triggered to change the Throughput mode to provisioned at 5 MiB/sec. This increases the monthly cost of EFS by ~$30/month (for us-west-2 and us-east-1 regions). After some time, the file system would have earned enough burst credits, so the lambda function changes Throughput mode back to bursting as it is more cost effective. Click here to learn more about EFS Throughput modes FSx \u00b6 Scale-Out Computing on AWS supports FSx natively. Click here to learn how to use FSx as backend storage for your jobs . Instance (local) partitions \u00b6 Below are the storage options you can configure at an instance level for your jobs. If needed, add/remove/modify the storage logic by editing ComputeNode.sh script to match your requirements. Root partition \u00b6 By default Scale-Out Computing on AWS provision a 10GB EBS disk for the root partition. This may be an issue if you are using a custom AMI configured with a bigger root disk size or if you simply want to allocate additional storage for the root partition. To expand the size of the volume, submit a simulation using -l root_size=<SIZE_IN_GB> parameter. user@host:qsub -l root_size = 25 -- /bin/sleep 600 Result: Root partition is now 25GB user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme0n1 259 :0 0 25G 0 disk \u251c\u2500nvme0n1p1 259 :1 0 25G 0 part / \u2514\u2500nvme0n1p128 259 :2 0 1M 0 part user@host: df -h / Filesystem Size Used Avail Use% Mounted on /dev/nvme0n1p1 25G 2 .2G 23G 9 % / Scratch Partition \u00b6 Info It's recommended to provision /scratch directory whenever your simulation is I/O intensive. /scratch is a local partition and will be deleted when you job complete. Make sure to copy the job output back to your $HOME /scratch is automatically created when Instance supports local ephemeral storage Request a /scratch partition with SSD disk \u00b6 During job submission, specify -l scratch_size=<SIZE_IN_GB> to provision a new EBS disk ( /dev/sdj ) mounted as /scratch user@host: qsub -l scratch_size = 150 -- /bin/sleep 600 Result: a 150 GB /scratch partition is available on all nodes user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme1n1 259 :0 0 150G 0 disk /scratch nvme0n1 259 :1 0 10G 0 disk \u251c\u2500nvme0n1p1 259 :2 0 10G 0 part / \u2514\u2500nvme0n1p128 259 :3 0 1M 0 part user@host: df -h /scratch Filesystem Size Used Avail Use% Mounted on /dev/nvme1n1 148G 61M 140G 1 % /scratch To verify the type of your EBS disk, simply go to your AWS bash > EC2 > Volumes and verify your EBS type is \"gp2\" (SSD). Refer to this link for more information about the various EBS types available. Request a /scratch partition with IO optimized disk \u00b6 To request an optimized SSD disk, use -l scratch_iops=<IOPS> along with -l scratch_size=<SIZE_IN_GB> . Refer to this link to get more details about burstable/IO EBS disks. user@host: qsub -l scratch_iops = 6000 -l scratch_size = 200 -- /bin/sleep 600 Looking at the EBS bash, the disk type is now \"io1\" and the number of IOPS match the value specified at job submission. Instance store partition \u00b6 Free storage is always good You are not charged for instance storage (included in the price of the instance) Some instances come with default instance storage . An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer and is removed as soon as the node is deleted. Scale-Out Computing on AWS automatically detects instance store disk and will use them as /scratch unless you specify -l scratch_size parameter for your job . In this case, Scale-Out Computing on AWS honors the user request and ignore the instance store volume(s). When node has 1 instance store volume \u00b6 For this example, I will use a \"c5d.9xlarge\" instance which is coming with a 900GB instance store disk. user@host: qsub -l instance_type = c5d.9xlarge -- /bin/sleep 600 Result: Default /scratch partition has been provisioned automatically using local instance storage user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme1n1 259 :0 0 838 .2G 0 disk /scratch nvme0n1 259 :1 0 10G 0 disk \u251c\u2500nvme0n1p1 259 :2 0 10G 0 part / \u2514\u2500nvme0n1p128 259 :3 0 1M 0 part user@host: df -h /scratch Filesystem Size Used Avail Use% Mounted on /dev/nvme1n1 825G 77M 783G 1 % /scratch When node has more than 1 instance store volumes \u00b6 In this special case, ComputeNode.sh script will create a raid0 partition using all instance store volumes available. For this example, I will use a \"m5dn.12xlarge\" instance which is shipped with a 2 * 900GB instance store disks (total 1.8Tb). user@host: qsub -l instance_type = m5dn.12xlarge -- /bin/sleep 600 Result: /scratch is a 1.7TB raid0 partition (using 2 instance store volumes) user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme1n1 259 :0 0 838 .2G 0 disk \u2514\u2500md127 9 :127 0 1 .7T 0 raid0 /scratch nvme2n1 259 :1 0 838 .2G 0 disk \u2514\u2500md127 9 :127 0 1 .7T 0 raid0 /scratch nvme0n1 259 :2 0 10G 0 disk \u251c\u2500nvme0n1p1 259 :3 0 10G 0 part / \u2514\u2500nvme0n1p128 259 :4 0 1M 0 part user@host: df -h /scratch Filesystem Size Used Avail Use% Mounted on /dev/md127 1 .7T 77M 1 .6T 1 % /scratch Combine custom scratch and root size \u00b6 You can combine parameters as needed. For example, qsub -l root_size=150 -l scratch_size=200 -l nodes=2 will provision 2 nodes with 150GB / and 200GB SSD /scratch Change the storage parameters at queue level \u00b6 Edit /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml to configure default storage settings at a queue level: queue_type : compute : # /root will be 30 GB and /scratch will be a standard 100GB SSD queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] instance_ami : \"ami-082b5a644766e0e6f\" instance_type : \"c5.large\" scratch_size : \"100\" root_size : \"30\" # .. Refer to the doc for more supported parameters memory : # /scratch will be a SSD with provisioned IO queues : [ \"queue4\" ] instance_ami : \"ami-082b5a644766e0e6f\" instance_type : \"r5.large\" scratch_size : \"300\" scratch_iops : \"5000\" instancestore : # /scratch will use the default instance store queues : [ \"queue5\" ] instance_ami : \"ami-082b5a644766e0e6f\" instance_type : \"m5dn.12large\" root_size : \"300\"","title":"Understand backend storage"},{"location":"storage/backend-storage-options/#efs-shared-partitions","text":"","title":"EFS (shared) partitions"},{"location":"storage/backend-storage-options/#data-partition","text":"/data is an Elastic File System partition mounted on all hosts. This contains the home directory of your LDAP users ($HOME = /data/home/<USERNAME> ). This partition is persistent . Avoid using this partition if your simulation is disk I/O intensive (use /scratch instead)","title":"/data partition"},{"location":"storage/backend-storage-options/#apps-partition","text":"/apps is an Elastic File System partition mounted on all hosts. This partition is designed to host all your CFD/FEA/EDA/Mathematical applications. This partition is persistent . Avoid using this partition if your simulation is disk I/O intensive (use /scratch instead) The corresponding EFS file system is deployed with bursting Throughput mode which depends on burst credits. /apps has cron jobs that monitor the status of the cluster, scale-up and scale-down the cluster, and is also running the web application. If the cluster stays up and running for 1+ months and not much additional applications are installed under /apps, then EFS file system might run out of burst credits which could impact the web application. Starting v2.6.0, the solution deploys CloudWatch Alarms to monitor the burst credits for /apps EFS file system. When burst credits are close to be depleted, a lambda function is triggered to change the Throughput mode to provisioned at 5 MiB/sec. This increases the monthly cost of EFS by ~$30/month (for us-west-2 and us-east-1 regions). After some time, the file system would have earned enough burst credits, so the lambda function changes Throughput mode back to bursting as it is more cost effective. Click here to learn more about EFS Throughput modes","title":"/apps partition"},{"location":"storage/backend-storage-options/#fsx","text":"Scale-Out Computing on AWS supports FSx natively. Click here to learn how to use FSx as backend storage for your jobs .","title":"FSx"},{"location":"storage/backend-storage-options/#instance-local-partitions","text":"Below are the storage options you can configure at an instance level for your jobs. If needed, add/remove/modify the storage logic by editing ComputeNode.sh script to match your requirements.","title":"Instance (local) partitions"},{"location":"storage/backend-storage-options/#root-partition","text":"By default Scale-Out Computing on AWS provision a 10GB EBS disk for the root partition. This may be an issue if you are using a custom AMI configured with a bigger root disk size or if you simply want to allocate additional storage for the root partition. To expand the size of the volume, submit a simulation using -l root_size=<SIZE_IN_GB> parameter. user@host:qsub -l root_size = 25 -- /bin/sleep 600 Result: Root partition is now 25GB user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme0n1 259 :0 0 25G 0 disk \u251c\u2500nvme0n1p1 259 :1 0 25G 0 part / \u2514\u2500nvme0n1p128 259 :2 0 1M 0 part user@host: df -h / Filesystem Size Used Avail Use% Mounted on /dev/nvme0n1p1 25G 2 .2G 23G 9 % /","title":"Root partition"},{"location":"storage/backend-storage-options/#scratch-partition","text":"Info It's recommended to provision /scratch directory whenever your simulation is I/O intensive. /scratch is a local partition and will be deleted when you job complete. Make sure to copy the job output back to your $HOME /scratch is automatically created when Instance supports local ephemeral storage","title":"Scratch Partition"},{"location":"storage/backend-storage-options/#request-a-scratch-partition-with-ssd-disk","text":"During job submission, specify -l scratch_size=<SIZE_IN_GB> to provision a new EBS disk ( /dev/sdj ) mounted as /scratch user@host: qsub -l scratch_size = 150 -- /bin/sleep 600 Result: a 150 GB /scratch partition is available on all nodes user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme1n1 259 :0 0 150G 0 disk /scratch nvme0n1 259 :1 0 10G 0 disk \u251c\u2500nvme0n1p1 259 :2 0 10G 0 part / \u2514\u2500nvme0n1p128 259 :3 0 1M 0 part user@host: df -h /scratch Filesystem Size Used Avail Use% Mounted on /dev/nvme1n1 148G 61M 140G 1 % /scratch To verify the type of your EBS disk, simply go to your AWS bash > EC2 > Volumes and verify your EBS type is \"gp2\" (SSD). Refer to this link for more information about the various EBS types available.","title":"Request a /scratch partition with SSD disk"},{"location":"storage/backend-storage-options/#request-a-scratch-partition-with-io-optimized-disk","text":"To request an optimized SSD disk, use -l scratch_iops=<IOPS> along with -l scratch_size=<SIZE_IN_GB> . Refer to this link to get more details about burstable/IO EBS disks. user@host: qsub -l scratch_iops = 6000 -l scratch_size = 200 -- /bin/sleep 600 Looking at the EBS bash, the disk type is now \"io1\" and the number of IOPS match the value specified at job submission.","title":"Request a /scratch partition with IO optimized disk"},{"location":"storage/backend-storage-options/#instance-store-partition","text":"Free storage is always good You are not charged for instance storage (included in the price of the instance) Some instances come with default instance storage . An instance store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer and is removed as soon as the node is deleted. Scale-Out Computing on AWS automatically detects instance store disk and will use them as /scratch unless you specify -l scratch_size parameter for your job . In this case, Scale-Out Computing on AWS honors the user request and ignore the instance store volume(s).","title":"Instance store partition"},{"location":"storage/backend-storage-options/#when-node-has-1-instance-store-volume","text":"For this example, I will use a \"c5d.9xlarge\" instance which is coming with a 900GB instance store disk. user@host: qsub -l instance_type = c5d.9xlarge -- /bin/sleep 600 Result: Default /scratch partition has been provisioned automatically using local instance storage user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme1n1 259 :0 0 838 .2G 0 disk /scratch nvme0n1 259 :1 0 10G 0 disk \u251c\u2500nvme0n1p1 259 :2 0 10G 0 part / \u2514\u2500nvme0n1p128 259 :3 0 1M 0 part user@host: df -h /scratch Filesystem Size Used Avail Use% Mounted on /dev/nvme1n1 825G 77M 783G 1 % /scratch","title":"When node has 1 instance store volume"},{"location":"storage/backend-storage-options/#when-node-has-more-than-1-instance-store-volumes","text":"In this special case, ComputeNode.sh script will create a raid0 partition using all instance store volumes available. For this example, I will use a \"m5dn.12xlarge\" instance which is shipped with a 2 * 900GB instance store disks (total 1.8Tb). user@host: qsub -l instance_type = m5dn.12xlarge -- /bin/sleep 600 Result: /scratch is a 1.7TB raid0 partition (using 2 instance store volumes) user@host: lsblk NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT nvme1n1 259 :0 0 838 .2G 0 disk \u2514\u2500md127 9 :127 0 1 .7T 0 raid0 /scratch nvme2n1 259 :1 0 838 .2G 0 disk \u2514\u2500md127 9 :127 0 1 .7T 0 raid0 /scratch nvme0n1 259 :2 0 10G 0 disk \u251c\u2500nvme0n1p1 259 :3 0 10G 0 part / \u2514\u2500nvme0n1p128 259 :4 0 1M 0 part user@host: df -h /scratch Filesystem Size Used Avail Use% Mounted on /dev/md127 1 .7T 77M 1 .6T 1 % /scratch","title":"When node has more than 1 instance store volumes"},{"location":"storage/backend-storage-options/#combine-custom-scratch-and-root-size","text":"You can combine parameters as needed. For example, qsub -l root_size=150 -l scratch_size=200 -l nodes=2 will provision 2 nodes with 150GB / and 200GB SSD /scratch","title":"Combine custom scratch and root size"},{"location":"storage/backend-storage-options/#change-the-storage-parameters-at-queue-level","text":"Edit /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml to configure default storage settings at a queue level: queue_type : compute : # /root will be 30 GB and /scratch will be a standard 100GB SSD queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] instance_ami : \"ami-082b5a644766e0e6f\" instance_type : \"c5.large\" scratch_size : \"100\" root_size : \"30\" # .. Refer to the doc for more supported parameters memory : # /scratch will be a SSD with provisioned IO queues : [ \"queue4\" ] instance_ami : \"ami-082b5a644766e0e6f\" instance_type : \"r5.large\" scratch_size : \"300\" scratch_iops : \"5000\" instancestore : # /scratch will use the default instance store queues : [ \"queue5\" ] instance_ami : \"ami-082b5a644766e0e6f\" instance_type : \"m5dn.12large\" root_size : \"300\"","title":"Change the storage parameters at queue level"},{"location":"storage/launch-job-with-fsx/","text":"What is FSx \u00b6 Amazon FSx provides you with the native compatibility of third-party file systems with feature sets for workloads such as high-performance computing (HPC), machine learning and electronic design automation (EDA). You don\u2019t have to worry about managing file servers and storage, as Amazon FSx automates the time-consuming administration tasks such as hardware provisioning, software configuration, patching, and backups. Amazon FSx provides FSx for Lustre for compute-intensive workloads. Please note the following when using FSx on Scale-Out Computing on AWS FSx is supported natively (Linux clients, security groups and backend configuration is automatically managed by Scale-Out Computing on AWS) You can launch an ephemeral FSx filesystem for your job You can connect to an existing FSx filesystem You can dynamically adjust the storage capacity of your FSx filesystem Exported files (if any) from FSx to S3 will be stored under s3://<YOUR_BUCKET_NAME>/<CLUSTER_ID>-fsxoutput/job-<JOB_ID>/ by default (you can change it if needed) Scale-Out automatically determines the actions to be taken based on the fsx_lustre value you specified during job submission If value is yes/true/on , a standard FSx for Lustre will be provisioned If value starts with s3:// or is a string, SOCA will try to mount the S3 bucket automatically as part of the FSx deployment If value starts with fs-xxx , SOCA will try to mount an existing FSx automatically How to provision an ephemeral FSx \u00b6 To provision an FSx for Lustre without S3 backend, simply specify -l fsx_lustre=True at job submission. If -l fsx_lustre_capacity is not set, the default storage provisioned will be 1.2 TB. The FSx will be mounted under \"/fsx\" by default, you can change this value by referring at the section at the end of this doc. How to provision an ephemeral FSx with S3 backend \u00b6 Pre-requisite \u00b6 S3 Backend This section is only required if you are planning to use S3 as a data backend for FSx You need to give Scale-Out Computing on AWS the permission to map the S3 bucket you want to mount on FSx. To do that, add a new inline policy to the scheduler IAM role . The Scheduler IAM role can be found on the IAM bash and is named <SOCA_AWS_STACK_NAME>-Security-<UUID>-SchedulerIAMRole-<UUID> . To create an inline policy, select your IAM role, click \"Add Inline Policy\": Select \"JSON\" tab Finally copy/paste the JSON policy listed below (make sure to adjust to your bucket name), click \"Review\" and \"Create Policy\". { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowAccessFSxtoS3\" , \"Effect\" : \"Allow\" , \"Action\" : \"s3:*\" , \"Resource\" : [ \"arn:aws:s3:::<YOUR_BUCKET_NAME>\" , \"arn:aws:s3:::<YOUR_BUCKET_NAME>/*\" ] } ] } To validate your policy is effective, access the scheduler host and run the following commmand: ## Example when IAM policy is not correct user@host: aws s3 ls s3://<YOUR_BUCKET_NAME> An error occurred ( AccessDenied ) when calling the ListObjectsV2 operation: Access Denied ## Example when IAM policy is valid (output will list content of your bucket) user@host: aws s3 ls s3://<YOUR_BUCKET_NAME> 2019 -11-02 04 :26:27 2209 dataset1.txt 2019 -11-02 04 :26:39 10285 dataset2.csv Warning This permission will give scheduler host access to your S3 bucket, therefore you want to limit access to this host to approved users only. DCV sessions or other compute nodes will not have access to the S3 bucket. Setup \u00b6 For this example, let's say I have my dataset available on S3 and I want to access them for my simulation. Submit a job using -l fsx_lustre=s3://<YOUR_BUCKET_NAME> . The bucket will then be mounted on all nodes provisioned for the job under /fsx mountpoint. user@host: qsub -l fsx_lustre = s3://<YOUR_BUCKET_NAME> -- /bin/sleep 600 This command will provision a new 1200 GB (smallest capacity available) FSx filesystem for your job: Your job will automatically start as soon as both your FSx filesystem and compute nodes are available. Your filesystem will be available on all nodes allocated to your job under /fsx user@host: df -h /fsx Filesystem Size Used Avail Use% Mounted on 200 .0.170.60@tcp:/fsx 1 .1T 4 .4M 1 .1T 1 % /fsx ## Verify the content of your bucket is accessible user@host: ls -ltr /fsx total 1 -rwxr-xr-x 1 root root 2209 Nov 2 04 :26 dataset1.txt -rwxr-xr-x 1 root root 10285 Nov 2 04 :26 dataset2.csv You can change the ImportPath / ExportPath by using the following syntax: -l fsx_lustre=<BUCKET>+<EXPORT_PATH>+<IMPORT_PATH> . If <IMPORT_PATH> is not set, value defaults to the bucket root level. The default <EXPORT_PATH> is <BUCKET>/<CLUSTER_ID>-fsxoutput/<JOBID> Your FSx filesystem will automatically be terminated when your job complete. Refer to this link to learn how to interact with FSx data repositories. How to connect to a permanent/existing FSx \u00b6 If you already have a running FSx, you can mount it using -l fsx_lustre variable. user@host: qsub -l fsx_lustre = <MY_FSX_DNS> -- /bin/sleep 60 To retrieve your FSx DNS, select your filesystem and select \"Network & Security\" Warning Make sure your FSx is running on the same VPC as Scale-Out Computing on AWS Make sure your FSx security group allow traffic from/to Scale-Out Computing on AWS ComputeNodes SG If you specify both \"fsx_lustre\" and \"fsx_lustre\", only \"fsx_lustre\" will be mounted. Change FSx capacity \u00b6 Use -l fsx_lustre_size=<SIZE_IN_GB> to specify the size of your FSx filesystem. Please note the following informations: - If not specified, Scale-Out Computing on AWS deploy the smallest possible capacity (1200GB) - Valid sizes (in GB) are 1200, 2400, 3600 and increments of 3600 user@host: qsub -l fsx_lustre_size = 3600 -l fsx_lustre = s3://<YOUR_S3_BUCKET> -- /bin/sleep 600 This command will mount a 3.6TB FSx filesystem on all nodes provisioned for your simulation. How to change the mountpoint \u00b6 By default Scale-Out Computing on AWS mounts fsx on /fsx . If you need to change this value, edit /apps/soca/$SOCA_CONFIGURATION/cluster_node_bootstrap/ComputeNodePostReboot.sh update the value of FSX_MOUNTPOINT . ... if [[ $SOCA_AWS_fsx_lustre ! = 'false' ]] ; then echo \"FSx request detected, installing FSX Lustre client ... \" FSX_MOUNTPOINT = \"/fsx\" ## <-- Update mountpoint here mkdir -p $FSX_MOUNTPOINT ... Learn about the other storage options on Scale-Out Computing on AWS \u00b6 Click here to learn about the other storage options offered by Scale-Out Computing on AWS. Troubleshooting and most common errors \u00b6 Like any other parameter, FSx options can be debugged using /apps/soca/$SOCA_CONFIGURATION/cluster_manager/logs/<QUEUE_NAME>.log [ Error while trying to create ASG: Scale-Out Computing on AWS does not have access to this bucket. Update IAM policy as described on the documentation ] Resolution : Scale-Out Computing on AWS does not have access to this S3 bucket. Update your IAM role with the policy listed above [ Error while trying to create ASG: fsx_lustre_size must be: 1200 , 2400 , 3600 , 7200 , 10800 ] Resolution : fsx_lustre_size must be 1200, 2400, 3600 and increments of 3600","title":"Launch a job with FSx for Lustre"},{"location":"storage/launch-job-with-fsx/#what-is-fsx","text":"Amazon FSx provides you with the native compatibility of third-party file systems with feature sets for workloads such as high-performance computing (HPC), machine learning and electronic design automation (EDA). You don\u2019t have to worry about managing file servers and storage, as Amazon FSx automates the time-consuming administration tasks such as hardware provisioning, software configuration, patching, and backups. Amazon FSx provides FSx for Lustre for compute-intensive workloads. Please note the following when using FSx on Scale-Out Computing on AWS FSx is supported natively (Linux clients, security groups and backend configuration is automatically managed by Scale-Out Computing on AWS) You can launch an ephemeral FSx filesystem for your job You can connect to an existing FSx filesystem You can dynamically adjust the storage capacity of your FSx filesystem Exported files (if any) from FSx to S3 will be stored under s3://<YOUR_BUCKET_NAME>/<CLUSTER_ID>-fsxoutput/job-<JOB_ID>/ by default (you can change it if needed) Scale-Out automatically determines the actions to be taken based on the fsx_lustre value you specified during job submission If value is yes/true/on , a standard FSx for Lustre will be provisioned If value starts with s3:// or is a string, SOCA will try to mount the S3 bucket automatically as part of the FSx deployment If value starts with fs-xxx , SOCA will try to mount an existing FSx automatically","title":"What is FSx"},{"location":"storage/launch-job-with-fsx/#how-to-provision-an-ephemeral-fsx","text":"To provision an FSx for Lustre without S3 backend, simply specify -l fsx_lustre=True at job submission. If -l fsx_lustre_capacity is not set, the default storage provisioned will be 1.2 TB. The FSx will be mounted under \"/fsx\" by default, you can change this value by referring at the section at the end of this doc.","title":"How to provision an ephemeral FSx"},{"location":"storage/launch-job-with-fsx/#how-to-provision-an-ephemeral-fsx-with-s3-backend","text":"","title":"How to provision an ephemeral FSx with S3 backend"},{"location":"storage/launch-job-with-fsx/#pre-requisite","text":"S3 Backend This section is only required if you are planning to use S3 as a data backend for FSx You need to give Scale-Out Computing on AWS the permission to map the S3 bucket you want to mount on FSx. To do that, add a new inline policy to the scheduler IAM role . The Scheduler IAM role can be found on the IAM bash and is named <SOCA_AWS_STACK_NAME>-Security-<UUID>-SchedulerIAMRole-<UUID> . To create an inline policy, select your IAM role, click \"Add Inline Policy\": Select \"JSON\" tab Finally copy/paste the JSON policy listed below (make sure to adjust to your bucket name), click \"Review\" and \"Create Policy\". { \"Version\" : \"2012-10-17\" , \"Statement\" : [ { \"Sid\" : \"AllowAccessFSxtoS3\" , \"Effect\" : \"Allow\" , \"Action\" : \"s3:*\" , \"Resource\" : [ \"arn:aws:s3:::<YOUR_BUCKET_NAME>\" , \"arn:aws:s3:::<YOUR_BUCKET_NAME>/*\" ] } ] } To validate your policy is effective, access the scheduler host and run the following commmand: ## Example when IAM policy is not correct user@host: aws s3 ls s3://<YOUR_BUCKET_NAME> An error occurred ( AccessDenied ) when calling the ListObjectsV2 operation: Access Denied ## Example when IAM policy is valid (output will list content of your bucket) user@host: aws s3 ls s3://<YOUR_BUCKET_NAME> 2019 -11-02 04 :26:27 2209 dataset1.txt 2019 -11-02 04 :26:39 10285 dataset2.csv Warning This permission will give scheduler host access to your S3 bucket, therefore you want to limit access to this host to approved users only. DCV sessions or other compute nodes will not have access to the S3 bucket.","title":"Pre-requisite"},{"location":"storage/launch-job-with-fsx/#setup","text":"For this example, let's say I have my dataset available on S3 and I want to access them for my simulation. Submit a job using -l fsx_lustre=s3://<YOUR_BUCKET_NAME> . The bucket will then be mounted on all nodes provisioned for the job under /fsx mountpoint. user@host: qsub -l fsx_lustre = s3://<YOUR_BUCKET_NAME> -- /bin/sleep 600 This command will provision a new 1200 GB (smallest capacity available) FSx filesystem for your job: Your job will automatically start as soon as both your FSx filesystem and compute nodes are available. Your filesystem will be available on all nodes allocated to your job under /fsx user@host: df -h /fsx Filesystem Size Used Avail Use% Mounted on 200 .0.170.60@tcp:/fsx 1 .1T 4 .4M 1 .1T 1 % /fsx ## Verify the content of your bucket is accessible user@host: ls -ltr /fsx total 1 -rwxr-xr-x 1 root root 2209 Nov 2 04 :26 dataset1.txt -rwxr-xr-x 1 root root 10285 Nov 2 04 :26 dataset2.csv You can change the ImportPath / ExportPath by using the following syntax: -l fsx_lustre=<BUCKET>+<EXPORT_PATH>+<IMPORT_PATH> . If <IMPORT_PATH> is not set, value defaults to the bucket root level. The default <EXPORT_PATH> is <BUCKET>/<CLUSTER_ID>-fsxoutput/<JOBID> Your FSx filesystem will automatically be terminated when your job complete. Refer to this link to learn how to interact with FSx data repositories.","title":"Setup"},{"location":"storage/launch-job-with-fsx/#how-to-connect-to-a-permanentexisting-fsx","text":"If you already have a running FSx, you can mount it using -l fsx_lustre variable. user@host: qsub -l fsx_lustre = <MY_FSX_DNS> -- /bin/sleep 60 To retrieve your FSx DNS, select your filesystem and select \"Network & Security\" Warning Make sure your FSx is running on the same VPC as Scale-Out Computing on AWS Make sure your FSx security group allow traffic from/to Scale-Out Computing on AWS ComputeNodes SG If you specify both \"fsx_lustre\" and \"fsx_lustre\", only \"fsx_lustre\" will be mounted.","title":"How to connect to a permanent/existing FSx"},{"location":"storage/launch-job-with-fsx/#change-fsx-capacity","text":"Use -l fsx_lustre_size=<SIZE_IN_GB> to specify the size of your FSx filesystem. Please note the following informations: - If not specified, Scale-Out Computing on AWS deploy the smallest possible capacity (1200GB) - Valid sizes (in GB) are 1200, 2400, 3600 and increments of 3600 user@host: qsub -l fsx_lustre_size = 3600 -l fsx_lustre = s3://<YOUR_S3_BUCKET> -- /bin/sleep 600 This command will mount a 3.6TB FSx filesystem on all nodes provisioned for your simulation.","title":"Change FSx capacity"},{"location":"storage/launch-job-with-fsx/#how-to-change-the-mountpoint","text":"By default Scale-Out Computing on AWS mounts fsx on /fsx . If you need to change this value, edit /apps/soca/$SOCA_CONFIGURATION/cluster_node_bootstrap/ComputeNodePostReboot.sh update the value of FSX_MOUNTPOINT . ... if [[ $SOCA_AWS_fsx_lustre ! = 'false' ]] ; then echo \"FSx request detected, installing FSX Lustre client ... \" FSX_MOUNTPOINT = \"/fsx\" ## <-- Update mountpoint here mkdir -p $FSX_MOUNTPOINT ...","title":"How to change the mountpoint"},{"location":"storage/launch-job-with-fsx/#learn-about-the-other-storage-options-on-scale-out-computing-on-aws","text":"Click here to learn about the other storage options offered by Scale-Out Computing on AWS.","title":"Learn about the other storage options on Scale-Out Computing on AWS"},{"location":"storage/launch-job-with-fsx/#troubleshooting-and-most-common-errors","text":"Like any other parameter, FSx options can be debugged using /apps/soca/$SOCA_CONFIGURATION/cluster_manager/logs/<QUEUE_NAME>.log [ Error while trying to create ASG: Scale-Out Computing on AWS does not have access to this bucket. Update IAM policy as described on the documentation ] Resolution : Scale-Out Computing on AWS does not have access to this S3 bucket. Update your IAM role with the policy listed above [ Error while trying to create ASG: fsx_lustre_size must be: 1200 , 2400 , 3600 , 7200 , 10800 ] Resolution : fsx_lustre_size must be 1200, 2400, 3600 and increments of 3600","title":"Troubleshooting and most common errors"},{"location":"troubleshooting/","text":"Note Submit a ticket if your question is not listed here Job & Scheduler \u00b6 JS1) I submitted a job but the job stays in the Q state Virtual Desktops \u00b6 DCV1) I cannot access my Linux/Windows DCV session","title":"Knowledge Base"},{"location":"troubleshooting/#job-scheduler","text":"JS1) I submitted a job but the job stays in the Q state","title":"Job &amp; Scheduler"},{"location":"troubleshooting/#virtual-desktops","text":"DCV1) I cannot access my Linux/Windows DCV session","title":"Virtual Desktops"},{"location":"troubleshooting/troubleshoot-dcv/","text":"NICE DCV is a high-performance remote display protocol. It lets you securely deliver remote desktops and application streaming from any cloud or data center to any device, over varying network conditions. Official documentation can be found here. How DCV is integrated to SOCA \u00b6 1 - User creates a Windows or Linux DCV session via SOCA web interface 2 - SOCA executes a couple of actions via this function : Verify the user inputs (e.g: run ec2 dry-run, sanitize session name ...) Generate a cloudformation template via dcv_cloudformation_builder.py A new entry will be added to database. Check this link for Linux DCV and this one for Windows DCV 3 - Cloudformation will launch a EC2 instance and apply DCV related tags such as soca:NodeType=dcv , soca:DCVSessionUUID and soca:DCVSystem 3 - dcv_alb_manager.py is a script running every minute on the scheduler. This script is responsible to update the Load Balancer with the new DCV instance as well as clean old/un-used load balancer rules. Log can be found under /apps/soca/$SOCA_CONFIGURATION/cluster_manager/dcv_alb_manager.py.log Script will detect the DCV host via EC2 tags A new Target Group will be created DCV instance will be registered to the newly created target group Finally, a new load balancer rule will be created . Rule name will always be /ip-x-x-x-x where x-x-x-x is the private IP of the DCV instance 4 - Every time you access your DCV portal, SOCA will query the database and retrieve your associated DCV information from the database . To determine if your session is alive, a cURL command will be executed to the DCV host . Based on the return code, the database entry will be updated and your session state can take the following values: state=running if the curl command return 200 state=stopped when you stop a running DCV session state=impaired if the ec2-describe-instance determine your EC2 instance is not reachable state=pending If the DCV session was created or restarted but the cURL command is not returning an HTTP/200 yet For more information, refer to remote_desktop.py file or remote_desktop_windows.py if yo uare debugging a Windows DCV Understand what service is failing \u00b6 It's recommended to run cURL -k -I (uppercase i) on DCV session and look for the headers returned to understand what to look for: # curl -k -I https://<SOCA_URL>.us-west-2.elb.amazonaws.com/ip-50-0-159-130/?authToken = <TOKEN>#<SESSION_ID> HTTP/2 200 date: Fri, 13 Aug 2021 19:56:12 GMT content-type: text/html; charset=\"utf-8\" content-length: 439 server: dcv x-frame-options: DENY strict-transport-security: max-age=31536000; includeSubDomains server=dcv : Application Load Balancer (ALB) and DCV service are operating correctly, however your DCV session is not reachable. Refer to the Debug section below to understand why your DCV service is not operating correctly # curl -k -I https://<SOCA_URL>.us-west-2.elb.amazonaws.com/ip-50-0-159-130/?authToken = <TOKEN>#<SESSION_ID> HTTP/2 502 server: awselb/2.0 date: Fri, 13 Aug 2021 20:00:08 GMT content-type: text/html content-length: 122 server=awselb/2.0 : Either your ALB is not configured correctly or the DCV service is not running on the DCV host Verify your CloudFormation stack is created correctly \u00b6 Once you have triggered your virtual desktop via SOCA web interface , verify that your CloudFormation stack is created correctly. Now that you confirmed CloudFormation stack has been created successfully, make sure the EC2 instance associated to the stack is up and running. Make sure your instance is in running state and Status check is 2/2 Info CloudFormation stack name and Instance name will always be soca-<CLUSTER_NAME>-<SESSION_NAME>-<SESSION_OWNER> Verify the EC2 instance has been added to the load balancer correctly \u00b6 Retrieve the private IP of your DCV host then navigate to EC2 Console > Load Balancing > Target Groups. Search for soca-ip-x-x-x-x where x-x-x-x is the private IP of your DCV host. Click \"Targets\" and make sure your DCV instance is registered successfully with a healthy state. If you do not see the target group, refer to /apps/soca/$SOCA_CONFIGURATION/cluster_manager/dcv_alb_manager.py.log and review any error messages Verify the Application Load Balancer (ALB) is updated correctly \u00b6 Now that you have validated the target group, navigate to your ALB. Load balancer name will always be soca-<CLUSTER_NAME>-viewer . Once located, click \"Listeners\" tab then \"View/Edit\" rules for the HTTPS:443 listener. Verify you have a new ALB rule named /ip-x-x-x-x (where x-x-x-x is the private IP of the DCV host) pointing to the target group created for your instance. If you do not see the ALB rule, refer to /apps/soca/$SOCA_CONFIGURATION/cluster_manager/dcv_alb_manager.py.log and review any error messages. Debug your Linux DCV session \u00b6 You can find Linux DCV logs on /apps partition under /apps/soca/$SOCA_CONFIGURATION/cluster_node_boostrap/logs/desktop/<USER_NAME>/<SESSION_NAME>/<DCV_HOST>/ . Note If you delete a DCV session and recreate a new one with the same name, a new <DCV_HOST> directory will be created. ls /apps/soca/$SOCA_CONFIGURATION/cluster_node_bootstrap/logs/desktop/mickael/LinuxDesktop1/ip-50-0-159-130/ ComputeNodeConfigureMetrics.log ComputeNodeInstallDCV.log ComputeNodePostReboot.log ComputeNode.sh.log ComputeNodeUserCustomization.log ComputeNode.sh.log and ComputeNodePostReboot.log are the two base script responsible for EC2 configuration ComputeNodeInstallDCV.log is responsible for DCV installation and configuration. ComputeNodeConfigureMetrics.log and ComputeNodeUserCustomization.log are optional and do not impact DCV sessions. What if I don't see the folder log? \u00b6 If /apps/soca/$SOCA_CONFIGURATION/cluster_node_boostrap/logs/desktop/<USER_NAME>/<SESSION_NAME>/<DCV_HOST>/ is not created, then most likely something is wrong with the EC2 user data executed on the DCV host . Use SSM to connect to the host if you cannot SSH to the DCV host via its private IP. Once you are connected to the ssh shell, sudo as root and navigate to /root/ . Make sure all required packages are there: [root@ip-50-0-159-130 ~]# ls -ltr total 14136 drwxr-xr-x 2 3925172 users 4096 Apr 12 15:34 nice-dcv-2021.0-10242-el7-x86_64 -rw-r--r-- 1 root root 10762563 Apr 12 15:34 nice-dcv-2021.0-10242-el7-x86_64.tgz -rw-r--r-- 1 root root 5375 Aug 12 18:44 config.cfg -rw-r--r-- 1 root root 3686531 Aug 12 18:46 v20.0.1.tar.gz drwxrwxr-x 13 root root 4096 Aug 12 18:46 openpbs-20.0.1 -rw-r--r-- 1 root root 3406 Aug 12 18:48 open_ssl_ldap Check if /apps and /data partitions are mounted correctly [root@ip-50-0-159-130 ~]# df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 3.8G 0 3.8G 0% /dev tmpfs 3.8G 0 3.8G 0% /dev/shm tmpfs 3.8G 860K 3.8G 1% /run tmpfs 3.8G 0 3.8G 0% /sys/fs/cgroup /dev/nvme0n1p1 10G 3.9G 6.2G 39% / fs-xxxxxxx.efs.us-west-2.amazonaws.com:/ 8.0E 3.0M 8.0E 1% /data fs-yyyyyyy.efs.us-west-2.amazonaws.com:/ 8.0E 1.2G 8.0E 1% /apps tmpfs 777M 28K 777M 1% /run/user/21245 tmpfs 777M 0 777M 0% /run/user/0 Debug EC2 User Data \u00b6 If something is wrong, check te following logs to retrieve any potential errors during the initial execution: /var/log/cloud-init /var/log/cloud-init-output /var/log/messages An easy way to make retrieve any user-data error is to run cat /var/log/messages | grep cloud-init . Manually interact with DCV \u00b6 If the DCV system seems ok, verify that your DCV session is up and running by running dcv list-sessions . dcv list-sessions Session: '79e5feb7-b36d-48f4-b630-42753788d375' (owner:mickael type:virtual) If you do not see any session, run dcv create-session --owner <USER_NAME> --storage-root \"/data/home/<USER_NAME>/storage-root\" <DCV_SESSION_ID> where <DCV_SESSION_ID> is the ID of your session that you can retrieve via the web ui. Check for any logs under /var/log/dcv/ if you get any errors during your session creation Verify DCV autostart after reboot \u00b6 Sudo as the DCV user then run crontab -l to confirm the crontab is active [root@ip-50-0-159-130 ~]# sudo su mickael [mickael@ip-50-0-159-130 root]$ crontab -l @reboot dcv create-session --owner mickael --storage-root \"/data/home/mickael/storage-root\" 79e5feb7-b36d-48f4-b630-42753788d375 # Do Not Delete Debug your Windows DCV session \u00b6 DCV is installed on Windows via EC2 User-Data . Once your EC2 host is up and running, use SSM to connect the EC2 instance to open a powershell environment. Instance name will always be soca-<CLUSTER_NAME>-<SESSION_NAME>-<SESSION_OWNER> . Make sure your instance is in running state and Status check is 2/2 # Run this command to get the EC2 UserData log Get-Content C :\\ ProgramData \\ Amazon \\ EC2-Windows \\ Launch \\ Log \\ UserdataExecution . log # Run this command to get SOCA related log Get-Content C :\\ ProgramData \\ Amazon \\ EC2-Windows \\ Launch \\ Log \\ UserdataExecutionSOCA . log You can also verify the different registry entry configured for DCV. # SOCA Bootstrap will set custom Regedit such as: New-ItemProperty -Path \"Microsoft.PowerShell.Core\\Registry::\\HKEY_USERS\\S-1-5-18\\Software\\GSettings\\com\\nicesoftware\\dcv\\security\" -Name \"auth-token-verifier\" -PropertyType \"String\" -Value \"https://%SOCA_SchedulerPrivateIP%/api/dcv/authenticator\" -Force # To verify the value, run Get-ItemProperty followed by the path name Get-ItemProperty -Path \"Microsoft.PowerShell.Core\\Registry::\\HKEY_USERS\\S-1-5-18\\Software\\GSettings\\com\\nicesoftware\\dcv\\security\" Refer to the EC2 data script to get a list of all registry entries. Also verify your DCV session is up and running # Verify if dcvserver is up and running PS C :\\ Windows \\ system32 > Get-Service dcvserver Status Name DisplayName ------ ---- ----------- Running dcvserver DCV Server # Verify if your session is active via \"dcv.exe describe-session console\" PS C :\\ Windows \\ system32 > & 'C:\\Program Files\\NICE\\DCV\\Server\\bin\\dcv.exe' describe-session console Session : id : console owner : mickael display layout : 1280x1024 + 0 + 0 Retrieve your DCV information on the database \u00b6 Connect to the scheduler instance and sudo as root user and run sqlite3 /apps/soca/$SOCA_CONFIGURATION/cluster_web_ui/db.sqlite # sqlite3 /apps/soca/ $SOCA_CONFIGURATION /cluster_web_ui/db.sqlite SQLite version 3.7.17 2013-05-20 00:56:22 Enter \".help\" for instructions Enter SQL statements terminated with a \";\" sqlite> Type .tables to list all SOCA SQL tables. DCV sessions information are stored under windows_dcv_sessions and linux_dcv_session sqlite> .tables ami_list flask_sessions windows_dcv_sessions api_keys linux_dcv_sessions application_profiles project_list You can now run your own SQL queries to retrieve all data ( select * from linux_dcv_sessions; ) or use more complex queries such as: -- Return all active Linux session for user select * from linux_dcv_sessions where user = \"<USER>\" and is_active = 1 ; -- Return all inactive Linux sessions select * from linux_dcv_sessions where is_active = 0 ; -- Return specific session select * from linux_dcv_sessions where tag_uuid = \"79e5feb7-b36d-48f4-b630-42753788d375\" ; -- Return all sessions that have schedule disabled for saturday select * from linux_dcv_sessions where schedule_saturday_start = 0 and schedule_saturday_stop = 0 ; SQL Schema Column names can be found on models.py Most Common Errors \u00b6 Custom Python environment \u00b6 You must use Python provided by the system by default. This issue usually happen when you installed tools such as Anaconda3 which override the default profile and configure Anaconda3's Python environment as default. To verify what version of Python you are using, run which python3 # Invalid Python, your DCV session will not start $ which python3 /apps/anaconda3/bin/python3 # Correct Python environment $ which python3 /bin/python3 To fix this issue, edit your .bash_profile or .bash_rc and make sure your PATH is updated correctly # Incorrect Path as Anaconda3 will override the default Python export PATH=/apps/anaconda3/bin:$PATH # Correct Path. export PATH=$PATH:/apps/anaconda3/bin","title":"Debug your Linux DCV session"},{"location":"troubleshooting/troubleshoot-dcv/#how-dcv-is-integrated-to-soca","text":"1 - User creates a Windows or Linux DCV session via SOCA web interface 2 - SOCA executes a couple of actions via this function : Verify the user inputs (e.g: run ec2 dry-run, sanitize session name ...) Generate a cloudformation template via dcv_cloudformation_builder.py A new entry will be added to database. Check this link for Linux DCV and this one for Windows DCV 3 - Cloudformation will launch a EC2 instance and apply DCV related tags such as soca:NodeType=dcv , soca:DCVSessionUUID and soca:DCVSystem 3 - dcv_alb_manager.py is a script running every minute on the scheduler. This script is responsible to update the Load Balancer with the new DCV instance as well as clean old/un-used load balancer rules. Log can be found under /apps/soca/$SOCA_CONFIGURATION/cluster_manager/dcv_alb_manager.py.log Script will detect the DCV host via EC2 tags A new Target Group will be created DCV instance will be registered to the newly created target group Finally, a new load balancer rule will be created . Rule name will always be /ip-x-x-x-x where x-x-x-x is the private IP of the DCV instance 4 - Every time you access your DCV portal, SOCA will query the database and retrieve your associated DCV information from the database . To determine if your session is alive, a cURL command will be executed to the DCV host . Based on the return code, the database entry will be updated and your session state can take the following values: state=running if the curl command return 200 state=stopped when you stop a running DCV session state=impaired if the ec2-describe-instance determine your EC2 instance is not reachable state=pending If the DCV session was created or restarted but the cURL command is not returning an HTTP/200 yet For more information, refer to remote_desktop.py file or remote_desktop_windows.py if yo uare debugging a Windows DCV","title":"How DCV is integrated to SOCA"},{"location":"troubleshooting/troubleshoot-dcv/#understand-what-service-is-failing","text":"It's recommended to run cURL -k -I (uppercase i) on DCV session and look for the headers returned to understand what to look for: # curl -k -I https://<SOCA_URL>.us-west-2.elb.amazonaws.com/ip-50-0-159-130/?authToken = <TOKEN>#<SESSION_ID> HTTP/2 200 date: Fri, 13 Aug 2021 19:56:12 GMT content-type: text/html; charset=\"utf-8\" content-length: 439 server: dcv x-frame-options: DENY strict-transport-security: max-age=31536000; includeSubDomains server=dcv : Application Load Balancer (ALB) and DCV service are operating correctly, however your DCV session is not reachable. Refer to the Debug section below to understand why your DCV service is not operating correctly # curl -k -I https://<SOCA_URL>.us-west-2.elb.amazonaws.com/ip-50-0-159-130/?authToken = <TOKEN>#<SESSION_ID> HTTP/2 502 server: awselb/2.0 date: Fri, 13 Aug 2021 20:00:08 GMT content-type: text/html content-length: 122 server=awselb/2.0 : Either your ALB is not configured correctly or the DCV service is not running on the DCV host","title":"Understand what service is failing"},{"location":"troubleshooting/troubleshoot-dcv/#verify-your-cloudformation-stack-is-created-correctly","text":"Once you have triggered your virtual desktop via SOCA web interface , verify that your CloudFormation stack is created correctly. Now that you confirmed CloudFormation stack has been created successfully, make sure the EC2 instance associated to the stack is up and running. Make sure your instance is in running state and Status check is 2/2 Info CloudFormation stack name and Instance name will always be soca-<CLUSTER_NAME>-<SESSION_NAME>-<SESSION_OWNER>","title":"Verify your CloudFormation stack is created correctly"},{"location":"troubleshooting/troubleshoot-dcv/#verify-the-ec2-instance-has-been-added-to-the-load-balancer-correctly","text":"Retrieve the private IP of your DCV host then navigate to EC2 Console > Load Balancing > Target Groups. Search for soca-ip-x-x-x-x where x-x-x-x is the private IP of your DCV host. Click \"Targets\" and make sure your DCV instance is registered successfully with a healthy state. If you do not see the target group, refer to /apps/soca/$SOCA_CONFIGURATION/cluster_manager/dcv_alb_manager.py.log and review any error messages","title":"Verify the EC2 instance has been added to the load balancer correctly"},{"location":"troubleshooting/troubleshoot-dcv/#verify-the-application-load-balancer-alb-is-updated-correctly","text":"Now that you have validated the target group, navigate to your ALB. Load balancer name will always be soca-<CLUSTER_NAME>-viewer . Once located, click \"Listeners\" tab then \"View/Edit\" rules for the HTTPS:443 listener. Verify you have a new ALB rule named /ip-x-x-x-x (where x-x-x-x is the private IP of the DCV host) pointing to the target group created for your instance. If you do not see the ALB rule, refer to /apps/soca/$SOCA_CONFIGURATION/cluster_manager/dcv_alb_manager.py.log and review any error messages.","title":"Verify the Application Load Balancer (ALB) is updated correctly"},{"location":"troubleshooting/troubleshoot-dcv/#debug-your-linux-dcv-session","text":"You can find Linux DCV logs on /apps partition under /apps/soca/$SOCA_CONFIGURATION/cluster_node_boostrap/logs/desktop/<USER_NAME>/<SESSION_NAME>/<DCV_HOST>/ . Note If you delete a DCV session and recreate a new one with the same name, a new <DCV_HOST> directory will be created. ls /apps/soca/$SOCA_CONFIGURATION/cluster_node_bootstrap/logs/desktop/mickael/LinuxDesktop1/ip-50-0-159-130/ ComputeNodeConfigureMetrics.log ComputeNodeInstallDCV.log ComputeNodePostReboot.log ComputeNode.sh.log ComputeNodeUserCustomization.log ComputeNode.sh.log and ComputeNodePostReboot.log are the two base script responsible for EC2 configuration ComputeNodeInstallDCV.log is responsible for DCV installation and configuration. ComputeNodeConfigureMetrics.log and ComputeNodeUserCustomization.log are optional and do not impact DCV sessions.","title":"Debug your Linux DCV session"},{"location":"troubleshooting/troubleshoot-dcv/#what-if-i-dont-see-the-folder-log","text":"If /apps/soca/$SOCA_CONFIGURATION/cluster_node_boostrap/logs/desktop/<USER_NAME>/<SESSION_NAME>/<DCV_HOST>/ is not created, then most likely something is wrong with the EC2 user data executed on the DCV host . Use SSM to connect to the host if you cannot SSH to the DCV host via its private IP. Once you are connected to the ssh shell, sudo as root and navigate to /root/ . Make sure all required packages are there: [root@ip-50-0-159-130 ~]# ls -ltr total 14136 drwxr-xr-x 2 3925172 users 4096 Apr 12 15:34 nice-dcv-2021.0-10242-el7-x86_64 -rw-r--r-- 1 root root 10762563 Apr 12 15:34 nice-dcv-2021.0-10242-el7-x86_64.tgz -rw-r--r-- 1 root root 5375 Aug 12 18:44 config.cfg -rw-r--r-- 1 root root 3686531 Aug 12 18:46 v20.0.1.tar.gz drwxrwxr-x 13 root root 4096 Aug 12 18:46 openpbs-20.0.1 -rw-r--r-- 1 root root 3406 Aug 12 18:48 open_ssl_ldap Check if /apps and /data partitions are mounted correctly [root@ip-50-0-159-130 ~]# df -h Filesystem Size Used Avail Use% Mounted on devtmpfs 3.8G 0 3.8G 0% /dev tmpfs 3.8G 0 3.8G 0% /dev/shm tmpfs 3.8G 860K 3.8G 1% /run tmpfs 3.8G 0 3.8G 0% /sys/fs/cgroup /dev/nvme0n1p1 10G 3.9G 6.2G 39% / fs-xxxxxxx.efs.us-west-2.amazonaws.com:/ 8.0E 3.0M 8.0E 1% /data fs-yyyyyyy.efs.us-west-2.amazonaws.com:/ 8.0E 1.2G 8.0E 1% /apps tmpfs 777M 28K 777M 1% /run/user/21245 tmpfs 777M 0 777M 0% /run/user/0","title":"What if I don't see the folder log?"},{"location":"troubleshooting/troubleshoot-dcv/#debug-ec2-user-data","text":"If something is wrong, check te following logs to retrieve any potential errors during the initial execution: /var/log/cloud-init /var/log/cloud-init-output /var/log/messages An easy way to make retrieve any user-data error is to run cat /var/log/messages | grep cloud-init .","title":"Debug EC2 User Data"},{"location":"troubleshooting/troubleshoot-dcv/#manually-interact-with-dcv","text":"If the DCV system seems ok, verify that your DCV session is up and running by running dcv list-sessions . dcv list-sessions Session: '79e5feb7-b36d-48f4-b630-42753788d375' (owner:mickael type:virtual) If you do not see any session, run dcv create-session --owner <USER_NAME> --storage-root \"/data/home/<USER_NAME>/storage-root\" <DCV_SESSION_ID> where <DCV_SESSION_ID> is the ID of your session that you can retrieve via the web ui. Check for any logs under /var/log/dcv/ if you get any errors during your session creation","title":"Manually interact with DCV"},{"location":"troubleshooting/troubleshoot-dcv/#verify-dcv-autostart-after-reboot","text":"Sudo as the DCV user then run crontab -l to confirm the crontab is active [root@ip-50-0-159-130 ~]# sudo su mickael [mickael@ip-50-0-159-130 root]$ crontab -l @reboot dcv create-session --owner mickael --storage-root \"/data/home/mickael/storage-root\" 79e5feb7-b36d-48f4-b630-42753788d375 # Do Not Delete","title":"Verify DCV autostart after reboot"},{"location":"troubleshooting/troubleshoot-dcv/#debug-your-windows-dcv-session","text":"DCV is installed on Windows via EC2 User-Data . Once your EC2 host is up and running, use SSM to connect the EC2 instance to open a powershell environment. Instance name will always be soca-<CLUSTER_NAME>-<SESSION_NAME>-<SESSION_OWNER> . Make sure your instance is in running state and Status check is 2/2 # Run this command to get the EC2 UserData log Get-Content C :\\ ProgramData \\ Amazon \\ EC2-Windows \\ Launch \\ Log \\ UserdataExecution . log # Run this command to get SOCA related log Get-Content C :\\ ProgramData \\ Amazon \\ EC2-Windows \\ Launch \\ Log \\ UserdataExecutionSOCA . log You can also verify the different registry entry configured for DCV. # SOCA Bootstrap will set custom Regedit such as: New-ItemProperty -Path \"Microsoft.PowerShell.Core\\Registry::\\HKEY_USERS\\S-1-5-18\\Software\\GSettings\\com\\nicesoftware\\dcv\\security\" -Name \"auth-token-verifier\" -PropertyType \"String\" -Value \"https://%SOCA_SchedulerPrivateIP%/api/dcv/authenticator\" -Force # To verify the value, run Get-ItemProperty followed by the path name Get-ItemProperty -Path \"Microsoft.PowerShell.Core\\Registry::\\HKEY_USERS\\S-1-5-18\\Software\\GSettings\\com\\nicesoftware\\dcv\\security\" Refer to the EC2 data script to get a list of all registry entries. Also verify your DCV session is up and running # Verify if dcvserver is up and running PS C :\\ Windows \\ system32 > Get-Service dcvserver Status Name DisplayName ------ ---- ----------- Running dcvserver DCV Server # Verify if your session is active via \"dcv.exe describe-session console\" PS C :\\ Windows \\ system32 > & 'C:\\Program Files\\NICE\\DCV\\Server\\bin\\dcv.exe' describe-session console Session : id : console owner : mickael display layout : 1280x1024 + 0 + 0","title":"Debug your Windows DCV session"},{"location":"troubleshooting/troubleshoot-dcv/#retrieve-your-dcv-information-on-the-database","text":"Connect to the scheduler instance and sudo as root user and run sqlite3 /apps/soca/$SOCA_CONFIGURATION/cluster_web_ui/db.sqlite # sqlite3 /apps/soca/ $SOCA_CONFIGURATION /cluster_web_ui/db.sqlite SQLite version 3.7.17 2013-05-20 00:56:22 Enter \".help\" for instructions Enter SQL statements terminated with a \";\" sqlite> Type .tables to list all SOCA SQL tables. DCV sessions information are stored under windows_dcv_sessions and linux_dcv_session sqlite> .tables ami_list flask_sessions windows_dcv_sessions api_keys linux_dcv_sessions application_profiles project_list You can now run your own SQL queries to retrieve all data ( select * from linux_dcv_sessions; ) or use more complex queries such as: -- Return all active Linux session for user select * from linux_dcv_sessions where user = \"<USER>\" and is_active = 1 ; -- Return all inactive Linux sessions select * from linux_dcv_sessions where is_active = 0 ; -- Return specific session select * from linux_dcv_sessions where tag_uuid = \"79e5feb7-b36d-48f4-b630-42753788d375\" ; -- Return all sessions that have schedule disabled for saturday select * from linux_dcv_sessions where schedule_saturday_start = 0 and schedule_saturday_stop = 0 ; SQL Schema Column names can be found on models.py","title":"Retrieve your DCV information on the database"},{"location":"troubleshooting/troubleshoot-dcv/#most-common-errors","text":"","title":"Most Common Errors"},{"location":"troubleshooting/troubleshoot-dcv/#custom-python-environment","text":"You must use Python provided by the system by default. This issue usually happen when you installed tools such as Anaconda3 which override the default profile and configure Anaconda3's Python environment as default. To verify what version of Python you are using, run which python3 # Invalid Python, your DCV session will not start $ which python3 /apps/anaconda3/bin/python3 # Correct Python environment $ which python3 /bin/python3 To fix this issue, edit your .bash_profile or .bash_rc and make sure your PATH is updated correctly # Incorrect Path as Anaconda3 will override the default Python export PATH=/apps/anaconda3/bin:$PATH # Correct Path. export PATH=$PATH:/apps/anaconda3/bin","title":"Custom Python environment"},{"location":"troubleshooting/troubleshoot-job-queue/","text":"Jobs in dynamic queue \u00b6 First of all, unless you submit a job on the \"alwayson\" queue, it will usually take between 5 to 10 minutes before your job can start as Scale-Out Computing on AWS needs to provision your capacity. This can vary based on the type and number of EC2 instances you have requested for your job. Verify the log \u00b6 If your job is not starting, first verify the queue log under /apps/soca/$SOCA_CONFIGURATION/cluster_manager/logs/<queue_name>.log If the log is not created or you don't see any update on it even though you submitted a job, try to run the dispatcher.py command manually. On the scheduler, list all crontabs as root crontab - and refer to \"Automatic Host Provisioning\" section: ## Automatic Host Provisioning */3 * * * * source /etc/environment ; /apps/soca/ $SOCA_CONFIGURATION /python/latest/bin/python3 /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/dispatcher.py -c /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/settings/queue_mapping.yml -t compute */3 * * * * source /etc/environment ; /apps/soca/ $SOCA_CONFIGURATION /python/latest/bin/python3 /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/dispatcher.py -c /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/settings/queue_mapping.yml -t desktop */3 * * * * source /etc/environment ; /apps/soca/ $SOCA_CONFIGURATION /python/latest/bin/python3 /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/dispatcher.py -c /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/settings/queue_mapping.yml -t test Run the command manually (ex source /etc/environment; /apps/soca/$SOCA_CONFIGURATION/python/latest/bin/python3 /apps/soca/$SOCA_CONFIGURATIONcluster_manager/dispatcher.py -c /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml -t compute ) and look for any error. Common errors include malformed yaml files. Verify the job resource \u00b6 This guide assume you have created your queue correctly Run qstat -f <job_id> | grep -i resource and try to locate compute_node or stack_id resource. When your job is launched, these resources does not exist. The script dispatcher.py . running as a crontab and executed every 3 minutes will create these resources automatically. Example of job having all resources configured correctly # Job with Scale-Out Computing on AWS resources bash-4.2$ qstat -f 2 | grep -i resource Resource_List.instance_type = m5.large Resource_List.ncpus = 3 Resource_List.nodect = 3 Resource_List.nodes = 3 Resource_List.place = scatter Resource_List.select = 3 :ncpus = 1 :compute_node = job2 Resource_List.stack_id = soca-fpgaami-job-2 Please note these resources are created by dispatcher.py so allow a maximum of 3 minutes between job is submitted and resources are visibles on qstat output # Job without Scale-Out Computing on AWS resources created yet bash-4.2$ qstat -f 2 | grep -i resource Resource_List.instance_type = m5.large Resource_List.ncpus = 3 Resource_List.nodect = 3 Resource_List.nodes = 3 Resource_List.place = scatter Resource_List.select = 3 :ncpus = 1 If you see a compute_node different than tbd as well as stack_id , that means Scale-Out Computing on AWS triggered capacity provisioning by creating a new CloudFormation stack. If you go to your CloudFormation console, you should see a new stack being created using the following naming convention: soca-<cluster_name>-job-<job_id> Retrieve node logs \u00b6 On the scheduler host, access /apps/soca/$SOCA_CONFIGURATION/cluster_node_bootstrap/logs/ . This folder contains the output of all logs for all hosts provisioned by SOCA # Retrieve logs for the most recent (2 weeks) jobs ls -ltr /apps/soca/ $SOCA_CONFIGURATION /cluster_node_bootstrap/logs/ | tail -n 5 drwxr-xr-x 3 root root 6144 Jul 21 17 :02 19607 drwxr-xr-x 3 root root 6144 Jul 21 17 :16 19608 drwxr-xr-x 3 root root 6144 Jul 21 17 :21 19609 drwxr-xr-x 6 root root 6144 Jul 21 17 :40 19575 drwxr-xr-x 10 root root 6144 Jul 21 17 :44 19606 # Filter for a specific job id. Each nodes provisioned for this job will show up on the directory ls -ltr /apps/soca/ $SOCA_CONFIGURATION /cluster_node_bootstrap/logs/19606 | tail -n 5 drwxr-xr-x 2 root root 6144 Jul 21 17 :47 ip-10-10-99-2 drwxr-xr-x 2 root root 6144 Jul 21 17 :47 ip-10-10-102-78 drwxr-xr-x 2 root root 6144 Jul 21 17 :48 ip-10-10-101-45 drwxr-xr-x 2 root root 6144 Jul 21 17 :48 ip-10-10-85-64 drwxr-xr-x 2 root root 6144 Jul 21 17 :48 ip-10-10-77-184 # For each hosts, you will be able to retrieve the install logs and do any troubleshooting ls -ltr /apps/soca/ $SOCA_CONFIGURATION /cluster_node_bootstrap/logs/19606/ip-10-10-85-64 -rw-r--r-- 1 root root 77326 Jul 21 17 :47 ComputeNode.sh.log -rw-r--r-- 1 root root 864 Jul 21 17 :48 ComputeNodePostReboot.log -rw-r--r-- 1 root root 12 Jul 21 17 :48 ComputeNodeUserCustom.log -rw-r--r-- 1 root root 73009 Jul 21 17 :48 ComputeNodeUserCustomization.log -rw-r--r-- 1 root root 77995 Jul 21 17 :48 ComputeNodeConfigureMetrics.log If CloudFormation stack is NOT \"CREATE_COMPLETE\" \u00b6 Click on the stack name then check the \"Events\" tab and refer to any \"CREATE_FAILED\" errors In this example, the size of root device is too small and can be fixed by specify a bigger EBS disk using -l root_size=75 If CloudFormation stack is \"CREATE_COMPLETE\" \u00b6 First, make sure CloudFormation has created a new \"Launch Template\" for your job. Then navigate to AutoScaling console, select your AutoScaling group and click \"Activity\". You will see any EC2 errors related in this tab. Here is an example of capacity being provisioned correctly Here is an example of capacity provisioning errors: If capacity is being provisioned correctly, go back to Scale-Out Computing on AWS and run pbsnodes -a . Verify the capacity assigned to your job ID (refer to resources_available.compute_node ) is in state = free . pbsnodes -a ip-60-0-174-166 Mom = ip-60-0-174-166.us-west-2.compute.internal Port = 15002 pbs_version = 18.1.4 ntype = PBS state = free pcpus = 1 resources_available.arch = linux resources_available.availability_zone = us-west-2c resources_available.compute_node = job2 resources_available.host = ip-60-0-174-166 resources_available.instance_type = m5.large resources_available.mem = 7706180kb resources_available.ncpus = 1 resources_available.subnet_id = subnet-0af93e96ed9c4377d resources_available.vnode = ip-60-0-174-166 resources_assigned.accelerator_memory = 0kb resources_assigned.hbmem = 0kb resources_assigned.mem = 0kb resources_assigned.naccelerators = 0 resources_assigned.ncpus = 0 resources_assigned.vmem = 0kb queue = normal resv_enable = True sharing = default_shared last_state_change_time = Sat Oct 12 17:37:28 2019 If host is not in state = free after 10 minutes, SSH to the host, sudo as root and check the log file located under /root as well as /var/log/message | grep cloud-init","title":"Debug why your jobs are not starting"},{"location":"troubleshooting/troubleshoot-job-queue/#jobs-in-dynamic-queue","text":"First of all, unless you submit a job on the \"alwayson\" queue, it will usually take between 5 to 10 minutes before your job can start as Scale-Out Computing on AWS needs to provision your capacity. This can vary based on the type and number of EC2 instances you have requested for your job.","title":"Jobs in dynamic queue"},{"location":"troubleshooting/troubleshoot-job-queue/#verify-the-log","text":"If your job is not starting, first verify the queue log under /apps/soca/$SOCA_CONFIGURATION/cluster_manager/logs/<queue_name>.log If the log is not created or you don't see any update on it even though you submitted a job, try to run the dispatcher.py command manually. On the scheduler, list all crontabs as root crontab - and refer to \"Automatic Host Provisioning\" section: ## Automatic Host Provisioning */3 * * * * source /etc/environment ; /apps/soca/ $SOCA_CONFIGURATION /python/latest/bin/python3 /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/dispatcher.py -c /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/settings/queue_mapping.yml -t compute */3 * * * * source /etc/environment ; /apps/soca/ $SOCA_CONFIGURATION /python/latest/bin/python3 /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/dispatcher.py -c /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/settings/queue_mapping.yml -t desktop */3 * * * * source /etc/environment ; /apps/soca/ $SOCA_CONFIGURATION /python/latest/bin/python3 /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/dispatcher.py -c /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/settings/queue_mapping.yml -t test Run the command manually (ex source /etc/environment; /apps/soca/$SOCA_CONFIGURATION/python/latest/bin/python3 /apps/soca/$SOCA_CONFIGURATIONcluster_manager/dispatcher.py -c /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml -t compute ) and look for any error. Common errors include malformed yaml files.","title":"Verify the log"},{"location":"troubleshooting/troubleshoot-job-queue/#verify-the-job-resource","text":"This guide assume you have created your queue correctly Run qstat -f <job_id> | grep -i resource and try to locate compute_node or stack_id resource. When your job is launched, these resources does not exist. The script dispatcher.py . running as a crontab and executed every 3 minutes will create these resources automatically. Example of job having all resources configured correctly # Job with Scale-Out Computing on AWS resources bash-4.2$ qstat -f 2 | grep -i resource Resource_List.instance_type = m5.large Resource_List.ncpus = 3 Resource_List.nodect = 3 Resource_List.nodes = 3 Resource_List.place = scatter Resource_List.select = 3 :ncpus = 1 :compute_node = job2 Resource_List.stack_id = soca-fpgaami-job-2 Please note these resources are created by dispatcher.py so allow a maximum of 3 minutes between job is submitted and resources are visibles on qstat output # Job without Scale-Out Computing on AWS resources created yet bash-4.2$ qstat -f 2 | grep -i resource Resource_List.instance_type = m5.large Resource_List.ncpus = 3 Resource_List.nodect = 3 Resource_List.nodes = 3 Resource_List.place = scatter Resource_List.select = 3 :ncpus = 1 If you see a compute_node different than tbd as well as stack_id , that means Scale-Out Computing on AWS triggered capacity provisioning by creating a new CloudFormation stack. If you go to your CloudFormation console, you should see a new stack being created using the following naming convention: soca-<cluster_name>-job-<job_id>","title":"Verify the job resource"},{"location":"troubleshooting/troubleshoot-job-queue/#retrieve-node-logs","text":"On the scheduler host, access /apps/soca/$SOCA_CONFIGURATION/cluster_node_bootstrap/logs/ . This folder contains the output of all logs for all hosts provisioned by SOCA # Retrieve logs for the most recent (2 weeks) jobs ls -ltr /apps/soca/ $SOCA_CONFIGURATION /cluster_node_bootstrap/logs/ | tail -n 5 drwxr-xr-x 3 root root 6144 Jul 21 17 :02 19607 drwxr-xr-x 3 root root 6144 Jul 21 17 :16 19608 drwxr-xr-x 3 root root 6144 Jul 21 17 :21 19609 drwxr-xr-x 6 root root 6144 Jul 21 17 :40 19575 drwxr-xr-x 10 root root 6144 Jul 21 17 :44 19606 # Filter for a specific job id. Each nodes provisioned for this job will show up on the directory ls -ltr /apps/soca/ $SOCA_CONFIGURATION /cluster_node_bootstrap/logs/19606 | tail -n 5 drwxr-xr-x 2 root root 6144 Jul 21 17 :47 ip-10-10-99-2 drwxr-xr-x 2 root root 6144 Jul 21 17 :47 ip-10-10-102-78 drwxr-xr-x 2 root root 6144 Jul 21 17 :48 ip-10-10-101-45 drwxr-xr-x 2 root root 6144 Jul 21 17 :48 ip-10-10-85-64 drwxr-xr-x 2 root root 6144 Jul 21 17 :48 ip-10-10-77-184 # For each hosts, you will be able to retrieve the install logs and do any troubleshooting ls -ltr /apps/soca/ $SOCA_CONFIGURATION /cluster_node_bootstrap/logs/19606/ip-10-10-85-64 -rw-r--r-- 1 root root 77326 Jul 21 17 :47 ComputeNode.sh.log -rw-r--r-- 1 root root 864 Jul 21 17 :48 ComputeNodePostReboot.log -rw-r--r-- 1 root root 12 Jul 21 17 :48 ComputeNodeUserCustom.log -rw-r--r-- 1 root root 73009 Jul 21 17 :48 ComputeNodeUserCustomization.log -rw-r--r-- 1 root root 77995 Jul 21 17 :48 ComputeNodeConfigureMetrics.log","title":"Retrieve node logs"},{"location":"troubleshooting/troubleshoot-job-queue/#if-cloudformation-stack-is-not-create_complete","text":"Click on the stack name then check the \"Events\" tab and refer to any \"CREATE_FAILED\" errors In this example, the size of root device is too small and can be fixed by specify a bigger EBS disk using -l root_size=75","title":"If CloudFormation stack is NOT \"CREATE_COMPLETE\""},{"location":"troubleshooting/troubleshoot-job-queue/#if-cloudformation-stack-is-create_complete","text":"First, make sure CloudFormation has created a new \"Launch Template\" for your job. Then navigate to AutoScaling console, select your AutoScaling group and click \"Activity\". You will see any EC2 errors related in this tab. Here is an example of capacity being provisioned correctly Here is an example of capacity provisioning errors: If capacity is being provisioned correctly, go back to Scale-Out Computing on AWS and run pbsnodes -a . Verify the capacity assigned to your job ID (refer to resources_available.compute_node ) is in state = free . pbsnodes -a ip-60-0-174-166 Mom = ip-60-0-174-166.us-west-2.compute.internal Port = 15002 pbs_version = 18.1.4 ntype = PBS state = free pcpus = 1 resources_available.arch = linux resources_available.availability_zone = us-west-2c resources_available.compute_node = job2 resources_available.host = ip-60-0-174-166 resources_available.instance_type = m5.large resources_available.mem = 7706180kb resources_available.ncpus = 1 resources_available.subnet_id = subnet-0af93e96ed9c4377d resources_available.vnode = ip-60-0-174-166 resources_assigned.accelerator_memory = 0kb resources_assigned.hbmem = 0kb resources_assigned.mem = 0kb resources_assigned.naccelerators = 0 resources_assigned.ncpus = 0 resources_assigned.vmem = 0kb queue = normal resv_enable = True sharing = default_shared last_state_change_time = Sat Oct 12 17:37:28 2019 If host is not in state = free after 10 minutes, SSH to the host, sudo as root and check the log file located under /root as well as /var/log/message | grep cloud-init","title":"If CloudFormation stack is \"CREATE_COMPLETE\""},{"location":"tutorials/","text":"About \u00b6 This section covers SOCA tutorial such as integration with Simple Email Services for job notifications, AMI creation, troubleshooting guide and much more! Refer to the left sidebar for more detailed resources","title":"About"},{"location":"tutorials/#about","text":"This section covers SOCA tutorial such as integration with Simple Email Services for job notifications, AMI creation, troubleshooting guide and much more! Refer to the left sidebar for more detailed resources","title":"About"},{"location":"tutorials/access-soca-cluster/","text":"Info Backend storage on Scale-Out Computing on AWS is persistent. You will have access to the same filesystem ($HOME, /data and /apps) whether you access your cluster using SSH, Web Remote Desktop or Native Remote Desktop SSH access \u00b6 To access your Scale-Out Computing on AWS cluster using SSH protocol, simply click \"SSH Access\" on the left sidebar and follow the instructions. Scale-Out Computing on AWS will let you download your private key either in PEM or PPK format. SSH to an instance in a Private Subnet If you need to access an instance that is in a Private (non-routable) Subnet, you can use ssh-agent to do this: $ ssh-add -K ~/Keys/my_key_region.pem Identity added: /Users/username/Keys/my_key_region.pem ( /Users/username/Keys/my_key_region.pem ) $ ssh-add -L <you should see your ssh key here> Now use -A with ssh and this will forward the key with your ssh login: $ ssh -A -i ~/Keys/my_key_region.pem centos@111.222.333.444 Now that you have your key forwarded, you can login to an instance that is in the Private Subnet: $ ssh <USERNAME>@<PRIVATE_IP> Graphical access using Windows/Linux virtual desktop \u00b6 Refer to this page to learn how to launch your own Windows/Linux session and access SOCA via your virtual desktop","title":"How to access Scale-Out Computing on AWS"},{"location":"tutorials/access-soca-cluster/#ssh-access","text":"To access your Scale-Out Computing on AWS cluster using SSH protocol, simply click \"SSH Access\" on the left sidebar and follow the instructions. Scale-Out Computing on AWS will let you download your private key either in PEM or PPK format. SSH to an instance in a Private Subnet If you need to access an instance that is in a Private (non-routable) Subnet, you can use ssh-agent to do this: $ ssh-add -K ~/Keys/my_key_region.pem Identity added: /Users/username/Keys/my_key_region.pem ( /Users/username/Keys/my_key_region.pem ) $ ssh-add -L <you should see your ssh key here> Now use -A with ssh and this will forward the key with your ssh login: $ ssh -A -i ~/Keys/my_key_region.pem centos@111.222.333.444 Now that you have your key forwarded, you can login to an instance that is in the Private Subnet: $ ssh <USERNAME>@<PRIVATE_IP>","title":"SSH access"},{"location":"tutorials/access-soca-cluster/#graphical-access-using-windowslinux-virtual-desktop","text":"Refer to this page to learn how to launch your own Windows/Linux session and access SOCA via your virtual desktop","title":"Graphical access using Windows/Linux virtual desktop"},{"location":"tutorials/install-soca-cluster-legacy/","text":"Info This document is applicable to SOCA versions up to 2.6.1. For newer versions, please refer to the updated documentation 1-Click installer \u00b6 You can use the 1-Click installer for quick proof-of-concept (PoC), demo and/or development work . This installer is hosted on an AWS controlled S3 bucket and customization is limited, so we recommend downloading building your own SOCA (see below) for your production. Always refers to the Github repository for the latest SOCA version. 1-Click Install Download Scale-Out Computing on AWS \u00b6 Option 1: Build your own version \u00b6 Scale-Out Computing on AWS is open-source and available on Github ( https://github.com/awslabs/scale-out-computing-on-aws . To get started, simply clone the repository: # Clone using HTTPS user@host: git clone https://github.com/awslabs/scale-out-computing-on-aws . # Clone using SSH user@host: git clone git@github.com:awslabs/scale-out-computing-on-aws.git . Build your release Once you have cloned your repository, install dependencies with pip and then execute source/manual_build.py using either python2 or python3. In the following example we use python3: IAM permissions required Your IAM user invoked by awscli must have the permission to list and upload to S3 user@host: pip3 install -r source/requirements.txt user@host: python3 source/manual_build.py ====== Scale-Out Computing on AWS Build ====== > Generated unique ID for build: r6l1 > Creating temporary build folder ... > Copying required files ... > Creating archive for build id: r6l1 ====== Uploading to S3 ====== > Please enter the AWS region youd like to build SOCA in : us-east-1 > Please enter the name of an S3 bucket you own: your-bucket > Uploading required files ... ' [+] Uploading /home/you/scale-out-computing-on-aws/source/dist/r6l1/install-with-existing-resources.template to s3://your-bucket/soca-installer-r6l1/install-with-existing-resources.template ... [+] Uploading /home/you/scale-out-computing-on-aws/source/dist/r6l1/templates/Security.template to s3://your-bucket/soca-installer-r6l1/templates/Security.template' ====== Upload COMPLETE ====== ====== Installation Instructions ====== 1 . Click on the following link: https://console.aws.amazon.com/cloudformation/home?region = us-east-1#/stacks/create/review? & templateURL = https://your-bucket.s3.amazonaws.com/soca-installer-r6l1/scale-out-computing-on-aws.template & param_S3InstallBucket = your-bucket & param_S3InstallFolder = soca-installer-r6l1 2 . The 'Install Location' parameters are pre-filled for you, fill out the rest of the parameters. For more information: https://awslabs.github.io/scale-out-computing-on-aws/install-soca-cluster/ Press Enter key to close .. This command builds and uploads the required files to Amazon S3, then outputs a 1-Click url to launch the SOCA CloudFormation Stack. Info You can use the same bucket to host multiple Scale-Out Computing on AWS clusters. Each build generates a unique ID and uses that as the S3 key. Option 2: Download the latest release (.tar.gz) \u00b6 Download the tarball from https://github.com/awslabs/scale-out-computing-on-aws/releases Upload to S3 Go to your Amazon S3 console and click \"Create Bucket\" Choose a name and a region then click \"Create\" Avoid un-necessary charge It's recommended to create your bucket in the same region as your are planning to use Scale-Out Computing on AWS to avoid Cross-Regions charge ( See Data Transfer ) Once your bucket is created, select it and click \"Upload\". Simply drag and drop your build folder ( r6l1 in this example) to upload the content of the folder to S3. Info You can use the same bucket to host multiple Scale-Out Computing on AWS clusters Locate the install template On your S3 bucket, click on the folder you just uploaded. Your install template is located under <S3_BUCKET_NAME>/<BUILD_ID>/scale-out-computing-on-aws.template . Click on the object to retrieve the \"Object URL\" Want to use your existing AWS resources? Refer to install-with-existing-resources.template if you want to use Scale-Out Computing on AWS with your existing resources. Check out the web installer to verify your setup Install Scale-Out Computing on AWS \u00b6 Clicking on the link will open the CloudFormation console and pre-fill the Install Location parameters: Under stack details, choose the stack name (do not use uppercase or it will break your OpenSearch (formerly Elasticsearch) cluster). Requirements No uppercase in stack name Stack name is limited to 20 characters maximum (note: we automatically add soca- prefix) Not supported on regions with less than 3 AZs (Northern California / us-west-1) Environment Parameters: Choose your Linux Distribution, instance type for your scheduler host, VPC CIDR, your IP which will be allowed to access port 22, 80 and 443 as well as the root SSH keypair you want to use LDAP Parameters: Create a default LDAP user Marketplace AMIs If you choose to use the CentOS 7 image, you must subscribe to CentOS 7 in the AWS Marketplace , to allow the installer to access the AMI during installation. This solution supports a heterogeneous environment. After installation, administrators and users can specify a custom AMI per job and queue. Disable Rollback on Failure if needed If you face any challenge during the installation and need to do some troubleshooting, it's recommended to disable \"Rollback On Failure\" (under Advanced section) Click Next two times and make sure to check \"Capabilities\" section. One done simply click \"Create Stack\". The installation procedure will take about 45 minutes. CREATE_FAILED If you hit any issue during the installation, refer to the 'CREATE_FAILED' component and find the root cause by referring at \"Physical ID\" Post Install Verifications \u00b6 Wait for CloudFormation stacks to be \"CREATE_COMPLETE\", then select your base stack and click \"Outputs\" Output tabs give you information about the SSH IP for the scheduler host, link to the web interface or OpenSearch (formerly Elasticsearch). Even though Cloudformation resources are created, your environment might not be completely ready. To confirm whether or not Scale-Out Computing on AWS is ready, try to SSH to the scheduler IP. If your Scale-Out Computing on AWS cluster is not ready, your SSH will be rejected as shown below: 38f9d34dde89:~ mcrozes$ ssh -i mcrozes-personal-aws.pem ec2-user@<IP> ************* Scale-Out Computing on AWS FIRST TIME CONFIGURATION ************* Hold on, cluster is not ready yet. Please wait ~30 minutes as Scale-Out Computing on AWS is being installed. Once cluster is ready to use, this message will be replaced automatically and you will be able to SSH. ********************************************************* Connection Closed. If your Scale-Out Computing on AWS cluster is ready, your SSH session will be accepted. 38f9d34dde89:~ mcrozes$ ssh -i mcrozes-personal-aws.pem ec2-user@<IP> Last login: Mon Oct 7 21 :37:21 2019 from <IP> _____ ____ ______ ___ / ___/ / __ \\ / ____// | \\_ _ \\ / / / // / / / | | ___/ // /_/ // /___ / ___ | /____/ \\_ ___/ \\_ ___//_/ | _ | Cluster: soca-cluster-v1 > source /etc/environment to load Scale-Out Computing on AWS paths [ ec2-user@ip-20-0-5-212 ~ ] $ At this point, you will be able to access the web interface and log in with the default LDAP user you specified at launch creation What if SSH port (22) is blocked by your IT? \u00b6 Scale-Out Computing on AWS supports AWS Session Manager in case you corporate firewall is blocking SSH port (22). SSM let you open a secure shell on your EC2 instance through a secure web-based session. First, access your AWS EC2 Console and select your Scheduler instance, then click \"Connect\" button Select \"Session Manager\" and click Connect You now have access to a secure shell directly within your browser Enable Termination Protection \u00b6 This step is optional yet higly recommended. AWS CloudFormation allows you to protect a stack from being accidently deleted. If you attempt to delete a stack with termination protection enabled, the deletion fails and the stack, including its status, will remain unchanged. To enable \"Termination Protect\" select your Primary template and click \"Stack Action\" button then \"Edit Termination Protection\". Choose \"Enabled\" and click Save. Choose \"Disabled\" if you want to be able to delete the stack again. Important Services \u00b6 Note All services on SOCA will automatically restart if you restart your scheduler instance Run the following command (as root) if you want to restart any service: Scheduler: service pbs start SSSD: service sssd start OpenLDAP: service openldap start Web UI /apps/soca/$SOCA_CONFIGURATION/cluster_web_ui/socawebui.sh start NFS partitions mount -a (mount configuration is available on /etc/fstab ) Operational Metrics \u00b6 This solution includes an option to send anonymous operational metrics to AWS. We use this data to better understand how customers use this solution and related services and products. Note that AWS will own the data gathered via this survey. Data collection will be subject to the AWS Privacy Policy . To opt out of this feature, modify the /apps/soca/$SOCA_CONFIGURATION/cluster_manager/cloudformation_builder and set allow_anonymous_data_collection variable to False When enabled, the following information is collected and sent to AWS: - Solution ID: The AWS solution identifier - Base Operating System: The operating system selected for the solution deployment - Unique ID ( UUID ) : Randomly generated , unique identifier for each solution deployment - Timestamp: Data - collection timestamp - Instance Data: Type or count of the state and type of instances that are provided for by the Amazon EC2 scheduler instance for each job in each AWS Region - Keep Forever: If instances are running when no job is running - EFA Support: If EFA support was selected - Spot Support: If Spot support was invoked for new auto - scaling stacks - Stack Creation Version: The version of the stack that is created or deleted - Status: The status of the stack ( stack_created or stack_deleted ) - Scratch Disk Size: The size of the scratch disk selected for each solution deployment - Region: The region where the stack is deployed - FSxLustre: If the job is using FSx for Lustre What's next ? \u00b6 Learn how to access your cluster , how to submit your first job or even how to change your Scale-Out Computing on AWS DNS to match your personal domain name.","title":"(Legacy) Install your Scale-Out Computing on AWS cluster"},{"location":"tutorials/install-soca-cluster-legacy/#1-click-installer","text":"You can use the 1-Click installer for quick proof-of-concept (PoC), demo and/or development work . This installer is hosted on an AWS controlled S3 bucket and customization is limited, so we recommend downloading building your own SOCA (see below) for your production. Always refers to the Github repository for the latest SOCA version. 1-Click Install","title":"1-Click installer"},{"location":"tutorials/install-soca-cluster-legacy/#download-scale-out-computing-on-aws","text":"","title":"Download Scale-Out Computing on AWS"},{"location":"tutorials/install-soca-cluster-legacy/#option-1-build-your-own-version","text":"Scale-Out Computing on AWS is open-source and available on Github ( https://github.com/awslabs/scale-out-computing-on-aws . To get started, simply clone the repository: # Clone using HTTPS user@host: git clone https://github.com/awslabs/scale-out-computing-on-aws . # Clone using SSH user@host: git clone git@github.com:awslabs/scale-out-computing-on-aws.git . Build your release Once you have cloned your repository, install dependencies with pip and then execute source/manual_build.py using either python2 or python3. In the following example we use python3: IAM permissions required Your IAM user invoked by awscli must have the permission to list and upload to S3 user@host: pip3 install -r source/requirements.txt user@host: python3 source/manual_build.py ====== Scale-Out Computing on AWS Build ====== > Generated unique ID for build: r6l1 > Creating temporary build folder ... > Copying required files ... > Creating archive for build id: r6l1 ====== Uploading to S3 ====== > Please enter the AWS region youd like to build SOCA in : us-east-1 > Please enter the name of an S3 bucket you own: your-bucket > Uploading required files ... ' [+] Uploading /home/you/scale-out-computing-on-aws/source/dist/r6l1/install-with-existing-resources.template to s3://your-bucket/soca-installer-r6l1/install-with-existing-resources.template ... [+] Uploading /home/you/scale-out-computing-on-aws/source/dist/r6l1/templates/Security.template to s3://your-bucket/soca-installer-r6l1/templates/Security.template' ====== Upload COMPLETE ====== ====== Installation Instructions ====== 1 . Click on the following link: https://console.aws.amazon.com/cloudformation/home?region = us-east-1#/stacks/create/review? & templateURL = https://your-bucket.s3.amazonaws.com/soca-installer-r6l1/scale-out-computing-on-aws.template & param_S3InstallBucket = your-bucket & param_S3InstallFolder = soca-installer-r6l1 2 . The 'Install Location' parameters are pre-filled for you, fill out the rest of the parameters. For more information: https://awslabs.github.io/scale-out-computing-on-aws/install-soca-cluster/ Press Enter key to close .. This command builds and uploads the required files to Amazon S3, then outputs a 1-Click url to launch the SOCA CloudFormation Stack. Info You can use the same bucket to host multiple Scale-Out Computing on AWS clusters. Each build generates a unique ID and uses that as the S3 key.","title":"Option 1: Build your own version"},{"location":"tutorials/install-soca-cluster-legacy/#option-2-download-the-latest-release-targz","text":"Download the tarball from https://github.com/awslabs/scale-out-computing-on-aws/releases Upload to S3 Go to your Amazon S3 console and click \"Create Bucket\" Choose a name and a region then click \"Create\" Avoid un-necessary charge It's recommended to create your bucket in the same region as your are planning to use Scale-Out Computing on AWS to avoid Cross-Regions charge ( See Data Transfer ) Once your bucket is created, select it and click \"Upload\". Simply drag and drop your build folder ( r6l1 in this example) to upload the content of the folder to S3. Info You can use the same bucket to host multiple Scale-Out Computing on AWS clusters Locate the install template On your S3 bucket, click on the folder you just uploaded. Your install template is located under <S3_BUCKET_NAME>/<BUILD_ID>/scale-out-computing-on-aws.template . Click on the object to retrieve the \"Object URL\" Want to use your existing AWS resources? Refer to install-with-existing-resources.template if you want to use Scale-Out Computing on AWS with your existing resources. Check out the web installer to verify your setup","title":"Option 2: Download the latest release (.tar.gz)"},{"location":"tutorials/install-soca-cluster-legacy/#install-scale-out-computing-on-aws","text":"Clicking on the link will open the CloudFormation console and pre-fill the Install Location parameters: Under stack details, choose the stack name (do not use uppercase or it will break your OpenSearch (formerly Elasticsearch) cluster). Requirements No uppercase in stack name Stack name is limited to 20 characters maximum (note: we automatically add soca- prefix) Not supported on regions with less than 3 AZs (Northern California / us-west-1) Environment Parameters: Choose your Linux Distribution, instance type for your scheduler host, VPC CIDR, your IP which will be allowed to access port 22, 80 and 443 as well as the root SSH keypair you want to use LDAP Parameters: Create a default LDAP user Marketplace AMIs If you choose to use the CentOS 7 image, you must subscribe to CentOS 7 in the AWS Marketplace , to allow the installer to access the AMI during installation. This solution supports a heterogeneous environment. After installation, administrators and users can specify a custom AMI per job and queue. Disable Rollback on Failure if needed If you face any challenge during the installation and need to do some troubleshooting, it's recommended to disable \"Rollback On Failure\" (under Advanced section) Click Next two times and make sure to check \"Capabilities\" section. One done simply click \"Create Stack\". The installation procedure will take about 45 minutes. CREATE_FAILED If you hit any issue during the installation, refer to the 'CREATE_FAILED' component and find the root cause by referring at \"Physical ID\"","title":"Install Scale-Out Computing on AWS"},{"location":"tutorials/install-soca-cluster-legacy/#post-install-verifications","text":"Wait for CloudFormation stacks to be \"CREATE_COMPLETE\", then select your base stack and click \"Outputs\" Output tabs give you information about the SSH IP for the scheduler host, link to the web interface or OpenSearch (formerly Elasticsearch). Even though Cloudformation resources are created, your environment might not be completely ready. To confirm whether or not Scale-Out Computing on AWS is ready, try to SSH to the scheduler IP. If your Scale-Out Computing on AWS cluster is not ready, your SSH will be rejected as shown below: 38f9d34dde89:~ mcrozes$ ssh -i mcrozes-personal-aws.pem ec2-user@<IP> ************* Scale-Out Computing on AWS FIRST TIME CONFIGURATION ************* Hold on, cluster is not ready yet. Please wait ~30 minutes as Scale-Out Computing on AWS is being installed. Once cluster is ready to use, this message will be replaced automatically and you will be able to SSH. ********************************************************* Connection Closed. If your Scale-Out Computing on AWS cluster is ready, your SSH session will be accepted. 38f9d34dde89:~ mcrozes$ ssh -i mcrozes-personal-aws.pem ec2-user@<IP> Last login: Mon Oct 7 21 :37:21 2019 from <IP> _____ ____ ______ ___ / ___/ / __ \\ / ____// | \\_ _ \\ / / / // / / / | | ___/ // /_/ // /___ / ___ | /____/ \\_ ___/ \\_ ___//_/ | _ | Cluster: soca-cluster-v1 > source /etc/environment to load Scale-Out Computing on AWS paths [ ec2-user@ip-20-0-5-212 ~ ] $ At this point, you will be able to access the web interface and log in with the default LDAP user you specified at launch creation","title":"Post Install Verifications"},{"location":"tutorials/install-soca-cluster-legacy/#what-if-ssh-port-22-is-blocked-by-your-it","text":"Scale-Out Computing on AWS supports AWS Session Manager in case you corporate firewall is blocking SSH port (22). SSM let you open a secure shell on your EC2 instance through a secure web-based session. First, access your AWS EC2 Console and select your Scheduler instance, then click \"Connect\" button Select \"Session Manager\" and click Connect You now have access to a secure shell directly within your browser","title":"What if SSH port (22) is blocked by your IT?"},{"location":"tutorials/install-soca-cluster-legacy/#enable-termination-protection","text":"This step is optional yet higly recommended. AWS CloudFormation allows you to protect a stack from being accidently deleted. If you attempt to delete a stack with termination protection enabled, the deletion fails and the stack, including its status, will remain unchanged. To enable \"Termination Protect\" select your Primary template and click \"Stack Action\" button then \"Edit Termination Protection\". Choose \"Enabled\" and click Save. Choose \"Disabled\" if you want to be able to delete the stack again.","title":"Enable Termination Protection"},{"location":"tutorials/install-soca-cluster-legacy/#important-services","text":"Note All services on SOCA will automatically restart if you restart your scheduler instance Run the following command (as root) if you want to restart any service: Scheduler: service pbs start SSSD: service sssd start OpenLDAP: service openldap start Web UI /apps/soca/$SOCA_CONFIGURATION/cluster_web_ui/socawebui.sh start NFS partitions mount -a (mount configuration is available on /etc/fstab )","title":"Important Services"},{"location":"tutorials/install-soca-cluster-legacy/#operational-metrics","text":"This solution includes an option to send anonymous operational metrics to AWS. We use this data to better understand how customers use this solution and related services and products. Note that AWS will own the data gathered via this survey. Data collection will be subject to the AWS Privacy Policy . To opt out of this feature, modify the /apps/soca/$SOCA_CONFIGURATION/cluster_manager/cloudformation_builder and set allow_anonymous_data_collection variable to False When enabled, the following information is collected and sent to AWS: - Solution ID: The AWS solution identifier - Base Operating System: The operating system selected for the solution deployment - Unique ID ( UUID ) : Randomly generated , unique identifier for each solution deployment - Timestamp: Data - collection timestamp - Instance Data: Type or count of the state and type of instances that are provided for by the Amazon EC2 scheduler instance for each job in each AWS Region - Keep Forever: If instances are running when no job is running - EFA Support: If EFA support was selected - Spot Support: If Spot support was invoked for new auto - scaling stacks - Stack Creation Version: The version of the stack that is created or deleted - Status: The status of the stack ( stack_created or stack_deleted ) - Scratch Disk Size: The size of the scratch disk selected for each solution deployment - Region: The region where the stack is deployed - FSxLustre: If the job is using FSx for Lustre","title":"Operational Metrics"},{"location":"tutorials/install-soca-cluster-legacy/#whats-next","text":"Learn how to access your cluster , how to submit your first job or even how to change your Scale-Out Computing on AWS DNS to match your personal domain name.","title":"What's next ?"},{"location":"tutorials/install-soca-cluster/","text":"Info This document is applicable to SOCA versions 2.7.0 and newer. For older versions, please refer to the legacy documentation IAM policies required to install SOCA \u00b6 You can find the list of all required IAM policies to install SOCA via installer/SOCAInstallerIamPolicy.json . If needed, you can create an IAM policy and assign it to the IAM user/role you are planning to use to install SOCA. 1) Go to IAM console, select \"Policies\" in the left sidebar menu then click \"Create Policy\" 2) Select \"JSON\" and copy/paste the content of installer/SOCAInstallerIamPolicy.json 3) Select the IAM User/Role you are using to install SOCA 4) Click \"Add Permissions\" and attach the policy you just created Note You can specify a local IAM profile during SOCA installation via soca_installer.sh --profile <YOUR_IAM_PROFILE> Download the latest SOCA revision \u00b6 Scale-Out Computing on AWS is open-source and available on Github ( https://github.com/awslabs/scale-out-computing-on-aws ). To get started, simply clone the repository: # Clone using HTTPS user@host: git clone https://github.com/awslabs/scale-out-computing-on-aws . # Clone using SSH user@host: git clone git@github.com:awslabs/scale-out-computing-on-aws.git . Install SOCA \u00b6 Once you have cloned your repository, execute installer/soca_installer.sh script. The installer will perform the following tasks: Check if Python3 is available on your system Create a custom Python virtual-environment and install required libraries Install NodeJS, NPM, CDK and AWS CLI if needed Setup your SOCA cluster Installer is built with AWS Cloud Development Kit (CDK). Learn more information about CDK here . Execute soca_install.sh script located in the installer folder: # Assuming your current working directory is the root level of SOCA ./installer/soca_install.sh You will then be prompted for your cluster parameters. Follow the instructions and choose a S3 bucket you own, the name of your cluster, the SSH keypair to use and other cluster parameters. Silent Installation You can pass all parameters via arguments to automate the installation process. Run ./soca_installer.sh --help to see all options available Once all the parameters are specified, installer will run cdk bootstrap . This action will create a staging S3 bucket and store all assets generated by CDK . No actions will be performed if you already have your environment enabled for CDK. SOCA will then upload the scripts (<100 mb) required to configure the scheduler to the S3 bucket you specified during installation. Finally, the installer will trigger a cdk deploy command and the deployment will start. This will create a new CloudFormation stack on your AWS account. Once the cloudformation stack is created, the installer will verify if your SOCA cluster is configured correctly. The installer will exit once your SOCA is fully configured and reachable. Use existing AWS resources \u00b6 If needed, you can tell SOCA to re-use existing AWS resources running on your AWS account. This is particularly useful for blue/green deployment or when you want to upgrade your SOCA cluster without affecting your production workflows. Here is a list of all existing AWS resources you can specify when installing a new SOCA cluster: VPC Subnets Security Groups IAM roles AWS Directory Service AWS OpenSearch (formerly Elasticsearch) To re-use existing resources, enter \"existing\" when asked. SOCA installer will automatically scan your AWS resources and provide you with options Configuration checks Installer will verify your security groups configuration and provide you with recommendation if your security groups are missing key rules Installer will automatically append required SOCA policies when re-using IAM roles (Optional) Customize default values \u00b6 SOCA gives you the ability to customize all resources created during the installation. For example, you can choose how many NAT Gateways to deploy (default to 1), the KMS encryption to use for your filesystems (default to aws/key), the instance type (default to m5.large) to provision for the scheduler and more. Edit installer/default_config.yml if you want to change the default values Uninstall SOCA \u00b6 SOCA is managed by CloudFormation. To uninstall SOCA, simply delete the stack associated to your cluster. As a safety measure SOCA backups (EFS, Scheduler) are not deleted by default and you will have to remove them manually from AWS Backups. Post Install \u00b6 What if SSH port (22) is blocked by your IT? \u00b6 Scale-Out Computing on AWS supports AWS Session Manager in case you corporate firewall is blocking SSH port (22). SSM let you open a secure shell on your EC2 instance through a secure web-based session. First, access your AWS EC2 Console and select your Scheduler instance, then click \"Connect\" button Select \"Session Manager\" and click Connect You now have access to a secure shell directly within your browser Important Services \u00b6 Note All services on SOCA will automatically restart if you restart your scheduler instance Run the following command (as root) if you want to restart any service: Scheduler: service pbs start SSSD: service sssd start OpenLDAP: service openldap start Web UI /apps/soca/$SOCA_CONFIGURATION/cluster_web_ui/socawebui.sh start NFS partitions mount -a (mount configuration is available on /etc/fstab ) 1-Click installer for Demo/POc \u00b6 You can use the 1-Click installer for quick proof-of-concept (PoC), demo and/or development work . This installer is hosted on an AWS controlled S3 bucket and customization is limited, so we do not recommend this for your production cluster. Always refers to the Github repository for the latest SOCA version. 1-Click Install Operational Metrics \u00b6 This solution includes an option to send anonymous operational metrics to AWS. We use this data to better understand how customers use this solution and related services and products. Note that AWS will own the data gathered via this survey. Data collection will be subject to the AWS Privacy Policy . To opt out of this feature, modify the /apps/soca/$SOCA_CONFIGURATION/cluster_manager/cloudformation_builder and set allow_anonymous_data_collection variable to False When enabled, the following information is collected and sent to AWS: - Solution ID: The AWS solution identifier - Base Operating System: The operating system selected for the solution deployment - Unique ID ( UUID ) : Randomly generated , unique identifier for each solution deployment - Timestamp: Data - collection timestamp - Instance Data: Type or count of the state and type of instances that are provided for by the Amazon EC2 scheduler instance for each job in each AWS Region - Keep Forever: If instances are running when no job is running - EFA Support: If EFA support was selected - Spot Support: If Spot support was invoked for new auto - scaling stacks - Stack Creation Version: The version of the stack that is created or deleted - Status: The status of the stack ( stack_created or stack_deleted ) - Scratch Disk Size: The size of the scratch disk selected for each solution deployment - Region: The region where the stack is deployed - FSxLustre: If the job is using FSx for Lustre What's next ? \u00b6 Learn how to access your cluster , how to submit your first job or even how to change your Scale-Out Computing on AWS DNS to match your personal domain name.","title":"Install your Scale-Out Computing on AWS cluster"},{"location":"tutorials/install-soca-cluster/#iam-policies-required-to-install-soca","text":"You can find the list of all required IAM policies to install SOCA via installer/SOCAInstallerIamPolicy.json . If needed, you can create an IAM policy and assign it to the IAM user/role you are planning to use to install SOCA. 1) Go to IAM console, select \"Policies\" in the left sidebar menu then click \"Create Policy\" 2) Select \"JSON\" and copy/paste the content of installer/SOCAInstallerIamPolicy.json 3) Select the IAM User/Role you are using to install SOCA 4) Click \"Add Permissions\" and attach the policy you just created Note You can specify a local IAM profile during SOCA installation via soca_installer.sh --profile <YOUR_IAM_PROFILE>","title":"IAM policies required to install SOCA"},{"location":"tutorials/install-soca-cluster/#download-the-latest-soca-revision","text":"Scale-Out Computing on AWS is open-source and available on Github ( https://github.com/awslabs/scale-out-computing-on-aws ). To get started, simply clone the repository: # Clone using HTTPS user@host: git clone https://github.com/awslabs/scale-out-computing-on-aws . # Clone using SSH user@host: git clone git@github.com:awslabs/scale-out-computing-on-aws.git .","title":"Download the latest SOCA revision"},{"location":"tutorials/install-soca-cluster/#install-soca","text":"Once you have cloned your repository, execute installer/soca_installer.sh script. The installer will perform the following tasks: Check if Python3 is available on your system Create a custom Python virtual-environment and install required libraries Install NodeJS, NPM, CDK and AWS CLI if needed Setup your SOCA cluster Installer is built with AWS Cloud Development Kit (CDK). Learn more information about CDK here . Execute soca_install.sh script located in the installer folder: # Assuming your current working directory is the root level of SOCA ./installer/soca_install.sh You will then be prompted for your cluster parameters. Follow the instructions and choose a S3 bucket you own, the name of your cluster, the SSH keypair to use and other cluster parameters. Silent Installation You can pass all parameters via arguments to automate the installation process. Run ./soca_installer.sh --help to see all options available Once all the parameters are specified, installer will run cdk bootstrap . This action will create a staging S3 bucket and store all assets generated by CDK . No actions will be performed if you already have your environment enabled for CDK. SOCA will then upload the scripts (<100 mb) required to configure the scheduler to the S3 bucket you specified during installation. Finally, the installer will trigger a cdk deploy command and the deployment will start. This will create a new CloudFormation stack on your AWS account. Once the cloudformation stack is created, the installer will verify if your SOCA cluster is configured correctly. The installer will exit once your SOCA is fully configured and reachable.","title":"Install SOCA"},{"location":"tutorials/install-soca-cluster/#use-existing-aws-resources","text":"If needed, you can tell SOCA to re-use existing AWS resources running on your AWS account. This is particularly useful for blue/green deployment or when you want to upgrade your SOCA cluster without affecting your production workflows. Here is a list of all existing AWS resources you can specify when installing a new SOCA cluster: VPC Subnets Security Groups IAM roles AWS Directory Service AWS OpenSearch (formerly Elasticsearch) To re-use existing resources, enter \"existing\" when asked. SOCA installer will automatically scan your AWS resources and provide you with options Configuration checks Installer will verify your security groups configuration and provide you with recommendation if your security groups are missing key rules Installer will automatically append required SOCA policies when re-using IAM roles","title":"Use existing AWS resources"},{"location":"tutorials/install-soca-cluster/#optional-customize-default-values","text":"SOCA gives you the ability to customize all resources created during the installation. For example, you can choose how many NAT Gateways to deploy (default to 1), the KMS encryption to use for your filesystems (default to aws/key), the instance type (default to m5.large) to provision for the scheduler and more. Edit installer/default_config.yml if you want to change the default values","title":"(Optional) Customize default values"},{"location":"tutorials/install-soca-cluster/#uninstall-soca","text":"SOCA is managed by CloudFormation. To uninstall SOCA, simply delete the stack associated to your cluster. As a safety measure SOCA backups (EFS, Scheduler) are not deleted by default and you will have to remove them manually from AWS Backups.","title":"Uninstall SOCA"},{"location":"tutorials/install-soca-cluster/#post-install","text":"","title":"Post Install"},{"location":"tutorials/install-soca-cluster/#what-if-ssh-port-22-is-blocked-by-your-it","text":"Scale-Out Computing on AWS supports AWS Session Manager in case you corporate firewall is blocking SSH port (22). SSM let you open a secure shell on your EC2 instance through a secure web-based session. First, access your AWS EC2 Console and select your Scheduler instance, then click \"Connect\" button Select \"Session Manager\" and click Connect You now have access to a secure shell directly within your browser","title":"What if SSH port (22) is blocked by your IT?"},{"location":"tutorials/install-soca-cluster/#important-services","text":"Note All services on SOCA will automatically restart if you restart your scheduler instance Run the following command (as root) if you want to restart any service: Scheduler: service pbs start SSSD: service sssd start OpenLDAP: service openldap start Web UI /apps/soca/$SOCA_CONFIGURATION/cluster_web_ui/socawebui.sh start NFS partitions mount -a (mount configuration is available on /etc/fstab )","title":"Important Services"},{"location":"tutorials/install-soca-cluster/#1-click-installer-for-demopoc","text":"You can use the 1-Click installer for quick proof-of-concept (PoC), demo and/or development work . This installer is hosted on an AWS controlled S3 bucket and customization is limited, so we do not recommend this for your production cluster. Always refers to the Github repository for the latest SOCA version. 1-Click Install","title":"1-Click installer for Demo/POc"},{"location":"tutorials/install-soca-cluster/#operational-metrics","text":"This solution includes an option to send anonymous operational metrics to AWS. We use this data to better understand how customers use this solution and related services and products. Note that AWS will own the data gathered via this survey. Data collection will be subject to the AWS Privacy Policy . To opt out of this feature, modify the /apps/soca/$SOCA_CONFIGURATION/cluster_manager/cloudformation_builder and set allow_anonymous_data_collection variable to False When enabled, the following information is collected and sent to AWS: - Solution ID: The AWS solution identifier - Base Operating System: The operating system selected for the solution deployment - Unique ID ( UUID ) : Randomly generated , unique identifier for each solution deployment - Timestamp: Data - collection timestamp - Instance Data: Type or count of the state and type of instances that are provided for by the Amazon EC2 scheduler instance for each job in each AWS Region - Keep Forever: If instances are running when no job is running - EFA Support: If EFA support was selected - Spot Support: If Spot support was invoked for new auto - scaling stacks - Stack Creation Version: The version of the stack that is created or deleted - Status: The status of the stack ( stack_created or stack_deleted ) - Scratch Disk Size: The size of the scratch disk selected for each solution deployment - Region: The region where the stack is deployed - FSxLustre: If the job is using FSx for Lustre","title":"Operational Metrics"},{"location":"tutorials/install-soca-cluster/#whats-next","text":"Learn how to access your cluster , how to submit your first job or even how to change your Scale-Out Computing on AWS DNS to match your personal domain name.","title":"What's next ?"},{"location":"tutorials/integration-ec2-job-parameters/","text":"Scale-Out Computing on AWS made job submission on EC2 very easy and is fully integrated with EC2. Below is a list of parameters you can specify when you request your simulation to ensure the hardware provisioned will exactly match your simulation requirements. Note If you don't specify them, your job will use the default values configured for your queue (see /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml ) You can use the web-based simulator to generate your qsub command very easily. Compute \u00b6 base_os \u00b6 Description: Reference to the base OS of the AMI you are using Allowed Values: amazonlinux2 centos7 rhel7 Default: If not specified, value default to the OS of the install AMI Examples: -l base_os=centos7 : Instances provisioned will be deployed against CentOS manifest ht_support \u00b6 Disabled by default Description: Enable support for hyper-threading Allowed Values: yes true no false (case insensitive) Examples: -l ht_support=True : Enable hyper-threading for all instances -l ht_support=False : Disable hyper-threading for all instances (default) instance_ami \u00b6 Description: Reference to a custom AMI you want to use Default: If not specified, value default to the AMI specified during installation Examples: -l instance_ami=ami-abcde123 : Capacity provisioned for the job will use the specific AMI Note If you are planning to use an AMI which is not using the same OS as the scheduler, you will need to specify base_os parameter instance_profile \u00b6 Description: Reference to a custom IAM role to use. Make sure to specify the Instance Profile name and not IAM role name. Refer to the pre-requisites below before using custom IAM role. Default: Use the default IAM instance profile configured by SOCA Examples: -l instance_profile=CustomInstanceProfileName : Compute nodes will a custom IAM Role. Pre-requisites Click here to learn how to enable instance_profile option instance_type \u00b6 Description: The type of instance to provision for the simulation Examples: -l instance_type=c5.large : Provision a c5.large for the simulation -l instance_type=c5.large+m5.large : Provision c5.large and m5.large (if needed) for the simulation. Note You can specify multiple instances type using \"+\" sign. When using more than 1 instance type, AWS will prioritize the capacity based on the order (eg: launch c5.large first and switch to m5.large if AWS can't provision c5.large anymore) nodes \u00b6 Description:The number of EC2 instance to provision Examples: -l nodes=5 : Provision 5 EC2 instances force_ri \u00b6 Description: Restrict a job to run on Reserved Instance Allowed Values: True False Default: False Examples: -l force_ri=False : Job can use RI, On-Demand or Spot -l force_ri=True : Job will only use Reserved Instance. Job will stay in the queue if there is not enough reserved instance available security_groups \u00b6 Description: Attach additional security groups (max 4) to the compute nodes Allowed Values: sg-xxxxxx Default: False Examples: -l security_groups=sg-abcde : Will attach sg-abcde on top of the default existing security group (ComputeNodeSG) -l security_groups=sg-abcd+sg-efgh : Will attach sg-abcde and sg-efgh on top of the default existing security group (ComputeNodeSG) Pre-requisites Click here to learn how to enable security_groups option Note You can specify a maximum of 4 additional security groups spot_allocation_count \u00b6 Description: Specify the number of SPOT instances to launch when provisioning both OD (On Demand) and SPOT instances Allowed Values: Integer Examples: -l nodes=10 -l spot_price=auto -l spot_allocation_count=8 : Provision 10 instances, 2 OD and 8 SPOT with max spot price capped to OD price -l nodes=10 -l spot_price=1.4 -l spot_allocation_count=5 : Provision 10 instances, 5 OD and 5 SPOT with max spot price set to $1.4 -l nodes=10 -l spot_price=auto : Only provision SPOT instances -l nodes=10 : Only provision OD instances Note This parameter is ignored if spot_price is not specified spot_allocation_count must be lower that the total number of nodes you are requesting (eg: you can not do -l nodes=5 -l spot_allocation_count=15 ) spot_allocation_strategy \u00b6 Description: Choose allocation strategy when using multiple SPOT instances type Allowed Valuess: capacity-optimized or lowest-price or diversified (only for SpotFleet deployments) Default Value: capacity-optimized Examples: -l spot_allocation_strategy=capacity-optimized : AWS will provision Spot compute nodes for both EC2 Auto Scaling and EC2 Fleet from the most-available Spot Instance pools by analyzing capacity metrics. spot_price \u00b6 Description: Enable support for SPOT instances Allowed Values: any float value or auto Examples: -l spot_price=auto : Max price will be capped to the On-Demand price -l spot_price=1.4 : Max price you are willing to pay for this instance will be $1.4 an hour. Note spot_price is capped to On-Demand price (e.g: Assuming you are provisioning a t3.medium, AWS will default maximum spot price to 0.418 (OD price) even though you specified -l spot_price=15 ) subnet_id \u00b6 Description: Reference to a subnet ID to use Default: If not specified, value default to one of the three private subnets created during installation Examples: -l subnet_id=sub-123 : Will provision capacity on sub-123 subnet -l subnet_id=sub-123+sub-456+sub-789 : + separated list of private subnets. Specifying more than 1 subnet is useful when requesting large number of instances -l subnet_id=2 : SOCA will provision capacity in 2 private subnets chosen randomly Note If you specify more than 1 subnet and have placement_group set to True, SOCA will automatically provision capacity and placement group on the first subnet from the list Note Capacity provisioning is limited to private subnets. Storage \u00b6 EBS \u00b6 keep_ebs \u00b6 Disabled by default Description: Retain or not the EBS disks once the simulation is complete Allowed Values: yes true false no (case insensitive) Default Value: False Example: -l keep_ebs=False : (Default) All EBS disks associated to the job will be deleted -l keep_ebs=True : Retain EBS disks after the simulation has terminated (mostly for debugging/troubleshooting procedures) root_size \u00b6 Description: Define the size of the local root volume Unit: GB Example: -l root_size=300 : Provision a 300 GB SSD disk for / (either sda1 or xvda1 ) scratch_size \u00b6 Description: Define the size of the local root volume Unit: GB Example: -l scratch_size=500 : Provision a 500 GB SSD disk for /scratch Note scratch disk is automatically mounted on all nodes associated to the simulation under /scratch instance_store \u00b6 Note SOCA automatically mount instance storage when available. For instances having more than 1 volume, SOCA will create a raid device In all cases, instance store volumes will be mounted on /scratch scratch_iops \u00b6 Description: Define the number of provisioned IOPS to allocate for your /scratch device Unit: IOPS Example: -l scratch_iops=3000 : Your EBS disks provisioned for /scratch will have 3000 dedicated IOPS Note It is recommended to set the IOPs to 3x storage capacity of your EBS disk FSx for Lustre \u00b6 fsx_lustre \u00b6 With no S3 backend \u00b6 Example: -l fsx_lustre=True : Create a new FSx for Lustre and mount it accross all nodes Note FSx partitions are mounted as /fsx . This can be changed if needed If fsx_lustre_size is not specified, default to 1200 GB With S3 backend \u00b6 Example: -l fsx_lustre=my-bucket-name or -l fsx_lustre=s3://my-bucket-name : Create a new FSx for Lustre and mount it across all nodes Note FSx partitions are mounted as /fsx . This can be changed if needed You need to give IAM permission first If not specified, SOCA automatically prefix your bucket name with s3:// If fsx_lustre_size is not specified, default to 1200 GB You can configure custom ImportPath and ExportPath Mount existing FSx \u00b6 Description: Mount an existing FSx to all compute nodes if fsx_lustre points to a FSx filesystem's DNS name Example: -l fsx_lustre=fs-xxxx.fsx.region.amazonaws.com Note FSx partitions are mounted as /fsx . This can be changed if needed Make sure your FSx for Luster configuration is correct (use SOCA VPC and correct IAM roles) Make sure to use the Filesytem's DNS name fsx_lustre_size \u00b6 Description: Create an ephemeral FSx for your job and mount the S3 bucket specified Unit: GB Example: -l fsx_lustre_size=3600 : Provision a 3.6TB EFS disk Note If fsx_lustre_size is not specified, default to 1200 GB (smallest size supported) Pre-Requisite This parameter is ignored unless you have specified fsx_lustre=True fsx_lustre_deployment_type \u00b6 Description: Choose what type of FSx for Lustre you want to deploy Allowed Valuess: SCRATCH_1 SCRATCH_2 PERSISTENT_1 (case insensitive) Default Value: SCRATCH_2 Example: -l fsx_lustre_deployment_type=scratch_2 : Provision a FSx for Lustre with SCRATCH_2 type Note If fsx_lustre_size is not specified, default to 1200 GB (smallest size supported) Pre-Requisite This parameter is ignored unless you have specified fsx_lustre=True fsx_lustre_per_unit_throughput \u00b6 Description: Select the baseline disk throughput available for that file system Allowed Values: 50 100 200 Unit: MB/s Example: -l fsx_lustre_per_unit_throughput=250 : Note Per Unit Throughput is only avaible when using PERSISTENT_1 FSx for Lustre Pre-Requisite This parameter is ignored unless you have specified fsx_lustre=True Network \u00b6 efa_support \u00b6 Description: Enable EFA support Allowed Values: yes, true, True Example: -l efa_support=True : Deploy an EFA device on all the nodes Note You must use an EFA compatible instance, otherwise your job will stay in the queue ht_support \u00b6 Disabled by default Description: Enable support for hyper-threading Allowed Values: yes true (case insensitive) Example: -l ht_support=True : Enable hyper-threading for all instances placement_group \u00b6 Enabled by default Description: Disable placement group Allowed Values: yes true (case insensitive) Example: -l placement_group=True : Instances will use placement groups Note Placement group is enabled by default as long as the number of nodes provisioned is greated than 1 Others \u00b6 system_metrics \u00b6 Default to False Description: Send host level metrics to your OpenSearch (formerly Elasticsearch) backend Allowed Values: yes no true false (case insensitive) Example: -l system_metrics=False Warning Enabling system_metrics generate a lot of data (especially if you are tracking 1000s of nodes). If needed, you can add more storage to your AWS OpenSearch (formerly Elasticsearch) cluster anonymous_metrics \u00b6 Default to the value specified during SOCA installation Description: Send anonymous operational metrics to AWS Allowed Values: yes true no false (case insensitive) Example: -l anonymous_metrics=True How to use custom parameters \u00b6 Example Here is an example about how to use a custom AMI at job or queue level. This example is applicable to all other parameters (simply change the parameter name to the one you one to use). For a single job \u00b6 Use -l instance_ami parameter if you want to only change the AMI for a single job $ qsub -l instance_ami = ami-082b... -- /bin/echo Hello Priority Job resources have the highest priorities. Your job will always use the AMI specified at submission time even if it's different thant the one configure at queue level. For an entire queue \u00b6 Edit /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml and update the default instance_ami parameter if you want all jobs in this queue to use your new AMI: queue_type : compute : queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] instance_ami : \"<YOUR_AMI_ID>\" # <- Add your new AMI instance_type : ... root_size : ... scratch_size : ... efa : ... .... View Examples \u00b6","title":"Job customization for EC2"},{"location":"tutorials/integration-ec2-job-parameters/#compute","text":"","title":"Compute"},{"location":"tutorials/integration-ec2-job-parameters/#base_os","text":"Description: Reference to the base OS of the AMI you are using Allowed Values: amazonlinux2 centos7 rhel7 Default: If not specified, value default to the OS of the install AMI Examples: -l base_os=centos7 : Instances provisioned will be deployed against CentOS manifest","title":"base_os"},{"location":"tutorials/integration-ec2-job-parameters/#ht_support","text":"Disabled by default Description: Enable support for hyper-threading Allowed Values: yes true no false (case insensitive) Examples: -l ht_support=True : Enable hyper-threading for all instances -l ht_support=False : Disable hyper-threading for all instances (default)","title":"ht_support"},{"location":"tutorials/integration-ec2-job-parameters/#instance_ami","text":"Description: Reference to a custom AMI you want to use Default: If not specified, value default to the AMI specified during installation Examples: -l instance_ami=ami-abcde123 : Capacity provisioned for the job will use the specific AMI Note If you are planning to use an AMI which is not using the same OS as the scheduler, you will need to specify base_os parameter","title":"instance_ami"},{"location":"tutorials/integration-ec2-job-parameters/#instance_profile","text":"Description: Reference to a custom IAM role to use. Make sure to specify the Instance Profile name and not IAM role name. Refer to the pre-requisites below before using custom IAM role. Default: Use the default IAM instance profile configured by SOCA Examples: -l instance_profile=CustomInstanceProfileName : Compute nodes will a custom IAM Role. Pre-requisites Click here to learn how to enable instance_profile option","title":"instance_profile"},{"location":"tutorials/integration-ec2-job-parameters/#instance_type","text":"Description: The type of instance to provision for the simulation Examples: -l instance_type=c5.large : Provision a c5.large for the simulation -l instance_type=c5.large+m5.large : Provision c5.large and m5.large (if needed) for the simulation. Note You can specify multiple instances type using \"+\" sign. When using more than 1 instance type, AWS will prioritize the capacity based on the order (eg: launch c5.large first and switch to m5.large if AWS can't provision c5.large anymore)","title":"instance_type"},{"location":"tutorials/integration-ec2-job-parameters/#nodes","text":"Description:The number of EC2 instance to provision Examples: -l nodes=5 : Provision 5 EC2 instances","title":"nodes"},{"location":"tutorials/integration-ec2-job-parameters/#force_ri","text":"Description: Restrict a job to run on Reserved Instance Allowed Values: True False Default: False Examples: -l force_ri=False : Job can use RI, On-Demand or Spot -l force_ri=True : Job will only use Reserved Instance. Job will stay in the queue if there is not enough reserved instance available","title":"force_ri"},{"location":"tutorials/integration-ec2-job-parameters/#security_groups","text":"Description: Attach additional security groups (max 4) to the compute nodes Allowed Values: sg-xxxxxx Default: False Examples: -l security_groups=sg-abcde : Will attach sg-abcde on top of the default existing security group (ComputeNodeSG) -l security_groups=sg-abcd+sg-efgh : Will attach sg-abcde and sg-efgh on top of the default existing security group (ComputeNodeSG) Pre-requisites Click here to learn how to enable security_groups option Note You can specify a maximum of 4 additional security groups","title":"security_groups"},{"location":"tutorials/integration-ec2-job-parameters/#spot_allocation_count","text":"Description: Specify the number of SPOT instances to launch when provisioning both OD (On Demand) and SPOT instances Allowed Values: Integer Examples: -l nodes=10 -l spot_price=auto -l spot_allocation_count=8 : Provision 10 instances, 2 OD and 8 SPOT with max spot price capped to OD price -l nodes=10 -l spot_price=1.4 -l spot_allocation_count=5 : Provision 10 instances, 5 OD and 5 SPOT with max spot price set to $1.4 -l nodes=10 -l spot_price=auto : Only provision SPOT instances -l nodes=10 : Only provision OD instances Note This parameter is ignored if spot_price is not specified spot_allocation_count must be lower that the total number of nodes you are requesting (eg: you can not do -l nodes=5 -l spot_allocation_count=15 )","title":"spot_allocation_count"},{"location":"tutorials/integration-ec2-job-parameters/#spot_allocation_strategy","text":"Description: Choose allocation strategy when using multiple SPOT instances type Allowed Valuess: capacity-optimized or lowest-price or diversified (only for SpotFleet deployments) Default Value: capacity-optimized Examples: -l spot_allocation_strategy=capacity-optimized : AWS will provision Spot compute nodes for both EC2 Auto Scaling and EC2 Fleet from the most-available Spot Instance pools by analyzing capacity metrics.","title":"spot_allocation_strategy"},{"location":"tutorials/integration-ec2-job-parameters/#spot_price","text":"Description: Enable support for SPOT instances Allowed Values: any float value or auto Examples: -l spot_price=auto : Max price will be capped to the On-Demand price -l spot_price=1.4 : Max price you are willing to pay for this instance will be $1.4 an hour. Note spot_price is capped to On-Demand price (e.g: Assuming you are provisioning a t3.medium, AWS will default maximum spot price to 0.418 (OD price) even though you specified -l spot_price=15 )","title":"spot_price"},{"location":"tutorials/integration-ec2-job-parameters/#subnet_id","text":"Description: Reference to a subnet ID to use Default: If not specified, value default to one of the three private subnets created during installation Examples: -l subnet_id=sub-123 : Will provision capacity on sub-123 subnet -l subnet_id=sub-123+sub-456+sub-789 : + separated list of private subnets. Specifying more than 1 subnet is useful when requesting large number of instances -l subnet_id=2 : SOCA will provision capacity in 2 private subnets chosen randomly Note If you specify more than 1 subnet and have placement_group set to True, SOCA will automatically provision capacity and placement group on the first subnet from the list Note Capacity provisioning is limited to private subnets.","title":"subnet_id"},{"location":"tutorials/integration-ec2-job-parameters/#storage","text":"","title":"Storage"},{"location":"tutorials/integration-ec2-job-parameters/#ebs","text":"","title":"EBS"},{"location":"tutorials/integration-ec2-job-parameters/#keep_ebs","text":"Disabled by default Description: Retain or not the EBS disks once the simulation is complete Allowed Values: yes true false no (case insensitive) Default Value: False Example: -l keep_ebs=False : (Default) All EBS disks associated to the job will be deleted -l keep_ebs=True : Retain EBS disks after the simulation has terminated (mostly for debugging/troubleshooting procedures)","title":"keep_ebs"},{"location":"tutorials/integration-ec2-job-parameters/#root_size","text":"Description: Define the size of the local root volume Unit: GB Example: -l root_size=300 : Provision a 300 GB SSD disk for / (either sda1 or xvda1 )","title":"root_size"},{"location":"tutorials/integration-ec2-job-parameters/#scratch_size","text":"Description: Define the size of the local root volume Unit: GB Example: -l scratch_size=500 : Provision a 500 GB SSD disk for /scratch Note scratch disk is automatically mounted on all nodes associated to the simulation under /scratch","title":"scratch_size"},{"location":"tutorials/integration-ec2-job-parameters/#instance_store","text":"Note SOCA automatically mount instance storage when available. For instances having more than 1 volume, SOCA will create a raid device In all cases, instance store volumes will be mounted on /scratch","title":"instance_store"},{"location":"tutorials/integration-ec2-job-parameters/#scratch_iops","text":"Description: Define the number of provisioned IOPS to allocate for your /scratch device Unit: IOPS Example: -l scratch_iops=3000 : Your EBS disks provisioned for /scratch will have 3000 dedicated IOPS Note It is recommended to set the IOPs to 3x storage capacity of your EBS disk","title":"scratch_iops"},{"location":"tutorials/integration-ec2-job-parameters/#fsx-for-lustre","text":"","title":"FSx for Lustre"},{"location":"tutorials/integration-ec2-job-parameters/#fsx_lustre","text":"","title":"fsx_lustre"},{"location":"tutorials/integration-ec2-job-parameters/#with-no-s3-backend","text":"Example: -l fsx_lustre=True : Create a new FSx for Lustre and mount it accross all nodes Note FSx partitions are mounted as /fsx . This can be changed if needed If fsx_lustre_size is not specified, default to 1200 GB","title":"With no S3 backend"},{"location":"tutorials/integration-ec2-job-parameters/#with-s3-backend","text":"Example: -l fsx_lustre=my-bucket-name or -l fsx_lustre=s3://my-bucket-name : Create a new FSx for Lustre and mount it across all nodes Note FSx partitions are mounted as /fsx . This can be changed if needed You need to give IAM permission first If not specified, SOCA automatically prefix your bucket name with s3:// If fsx_lustre_size is not specified, default to 1200 GB You can configure custom ImportPath and ExportPath","title":"With S3 backend"},{"location":"tutorials/integration-ec2-job-parameters/#mount-existing-fsx","text":"Description: Mount an existing FSx to all compute nodes if fsx_lustre points to a FSx filesystem's DNS name Example: -l fsx_lustre=fs-xxxx.fsx.region.amazonaws.com Note FSx partitions are mounted as /fsx . This can be changed if needed Make sure your FSx for Luster configuration is correct (use SOCA VPC and correct IAM roles) Make sure to use the Filesytem's DNS name","title":"Mount existing FSx"},{"location":"tutorials/integration-ec2-job-parameters/#fsx_lustre_size","text":"Description: Create an ephemeral FSx for your job and mount the S3 bucket specified Unit: GB Example: -l fsx_lustre_size=3600 : Provision a 3.6TB EFS disk Note If fsx_lustre_size is not specified, default to 1200 GB (smallest size supported) Pre-Requisite This parameter is ignored unless you have specified fsx_lustre=True","title":"fsx_lustre_size"},{"location":"tutorials/integration-ec2-job-parameters/#fsx_lustre_deployment_type","text":"Description: Choose what type of FSx for Lustre you want to deploy Allowed Valuess: SCRATCH_1 SCRATCH_2 PERSISTENT_1 (case insensitive) Default Value: SCRATCH_2 Example: -l fsx_lustre_deployment_type=scratch_2 : Provision a FSx for Lustre with SCRATCH_2 type Note If fsx_lustre_size is not specified, default to 1200 GB (smallest size supported) Pre-Requisite This parameter is ignored unless you have specified fsx_lustre=True","title":"fsx_lustre_deployment_type"},{"location":"tutorials/integration-ec2-job-parameters/#fsx_lustre_per_unit_throughput","text":"Description: Select the baseline disk throughput available for that file system Allowed Values: 50 100 200 Unit: MB/s Example: -l fsx_lustre_per_unit_throughput=250 : Note Per Unit Throughput is only avaible when using PERSISTENT_1 FSx for Lustre Pre-Requisite This parameter is ignored unless you have specified fsx_lustre=True","title":"fsx_lustre_per_unit_throughput"},{"location":"tutorials/integration-ec2-job-parameters/#network","text":"","title":"Network"},{"location":"tutorials/integration-ec2-job-parameters/#efa_support","text":"Description: Enable EFA support Allowed Values: yes, true, True Example: -l efa_support=True : Deploy an EFA device on all the nodes Note You must use an EFA compatible instance, otherwise your job will stay in the queue","title":"efa_support"},{"location":"tutorials/integration-ec2-job-parameters/#ht_support_1","text":"Disabled by default Description: Enable support for hyper-threading Allowed Values: yes true (case insensitive) Example: -l ht_support=True : Enable hyper-threading for all instances","title":"ht_support"},{"location":"tutorials/integration-ec2-job-parameters/#placement_group","text":"Enabled by default Description: Disable placement group Allowed Values: yes true (case insensitive) Example: -l placement_group=True : Instances will use placement groups Note Placement group is enabled by default as long as the number of nodes provisioned is greated than 1","title":"placement_group"},{"location":"tutorials/integration-ec2-job-parameters/#others","text":"","title":"Others"},{"location":"tutorials/integration-ec2-job-parameters/#system_metrics","text":"Default to False Description: Send host level metrics to your OpenSearch (formerly Elasticsearch) backend Allowed Values: yes no true false (case insensitive) Example: -l system_metrics=False Warning Enabling system_metrics generate a lot of data (especially if you are tracking 1000s of nodes). If needed, you can add more storage to your AWS OpenSearch (formerly Elasticsearch) cluster","title":"system_metrics"},{"location":"tutorials/integration-ec2-job-parameters/#anonymous_metrics","text":"Default to the value specified during SOCA installation Description: Send anonymous operational metrics to AWS Allowed Values: yes true no false (case insensitive) Example: -l anonymous_metrics=True","title":"anonymous_metrics"},{"location":"tutorials/integration-ec2-job-parameters/#how-to-use-custom-parameters","text":"Example Here is an example about how to use a custom AMI at job or queue level. This example is applicable to all other parameters (simply change the parameter name to the one you one to use).","title":"How to use custom parameters"},{"location":"tutorials/integration-ec2-job-parameters/#for-a-single-job","text":"Use -l instance_ami parameter if you want to only change the AMI for a single job $ qsub -l instance_ami = ami-082b... -- /bin/echo Hello Priority Job resources have the highest priorities. Your job will always use the AMI specified at submission time even if it's different thant the one configure at queue level.","title":"For a single job"},{"location":"tutorials/integration-ec2-job-parameters/#for-an-entire-queue","text":"Edit /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml and update the default instance_ami parameter if you want all jobs in this queue to use your new AMI: queue_type : compute : queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] instance_ami : \"<YOUR_AMI_ID>\" # <- Add your new AMI instance_type : ... root_size : ... scratch_size : ... efa : ... ....","title":"For an entire queue"},{"location":"tutorials/integration-ec2-job-parameters/#view-examples","text":"","title":"View Examples"},{"location":"tutorials/job-configuration-generator/","text":"Automatic parameter selection You can manually specify parameters at job submission using the command below. If needed, all parameters can also be automatically configured at queue level . Job will use the default parameters configured for its queue unless the parameters are explicitly specified during submission ( job parameters override queue parameters ). Refer to this page for additional examples. * { box-sizing: border-box; } .input2 { padding: 12px; width: 85%; border: 1px solid #ccc; border-radius: 4px; resize: vertical; font-size: 15px; margin-top: 6px; } .container { border-radius: 5px; background-color: #f2f2f2; padding: 5px; } .col-25 { float: left; width: 25%; margin-top: 6px; } .col-75 { float: left; width: 75%; margin-top: 6px; } /* Responsive layout - when the screen is less than 600px wide, make the two columns stack on top of each other instead of next to each other */ @media screen and (max-width: 600px) { .col-25, .col-75, input[type=submit] { width: 100%; margin-top: 0; } } .md-content { margin-right: 0; } Your Command user@host: qsub {{qsub_instance_ami}} {{qsub_instance_type}} {{qsub_subnet_id}} {{qsub_spot_price}} {{qsub_efa_support}} {{qsub_placement_group}} {{qsub_root_size}} {{qsub_scratch_size}} {{qsub_scratch_iops}} {{qsub_fsx_lustre}} {{qsub_fsx_lustre_size}} {{qsub_ht_support}} {{qsub_spot_allocation_count}} {{qsub_spot_allocation_strategy}} {{qsub_nodes}} {{qsub_base_os}} {{qsub_keep_ebs}} {{qsub_force_ri}} myscript.sh Job Parameters Compute parameters: Documentation Must be a number greater than 0 Documentation Documentation Image name must start with \"ami-\" Documentation Must be centos7, rhel7 or amazonlinux2 {{base_os_error}} Documentation Subnet name must start with \"sub-\" Documentation Spot Price must be a float (eg 1.2) or auto (match OD price) Documentation Must be a number {{spot_allocation_error}} {{spot_allocation_error_price}} Documentation Must be either lowest-price or capacity-optimized (default) {{spot_allocation_strategy_price}} Storage parameters: Documentation Root Size must be a number Documentation Scratch Size must be a number Documentation Provisioned IO/s must be a number Documentation Documentation Size must be a number Flags: I want to use EFA Documentation I do not want to use Placement Group (enabled by default) Documentation I want to enable HyperThreading (disabled by default) Documentation I want to retain my EBS disks (disabled by default) Documentation I want my job to only run on Reserved instances Documentation angular.module('myApp', ['ngMessages']) .controller('myCtrl', ['$scope', function($scope) { $scope.count = 0; $scope.myFunc = function() { if($scope.nodes){$scope.qsub_nodes = \"-l nodes=\" + $scope.nodes;}else{$scope.qsub_nodes = \"\";} if($scope.instance_ami){$scope.qsub_instance_ami = \"-l instance_ami=\" + $scope.instance_ami;}else{$scope.qsub_instance_ami = \"\";} if($scope.base_os){$scope.qsub_base_os = \"-l base_os=\" + $scope.base_os;}else{$scope.qsub_base_os = \"\";} if($scope.instance_type){$scope.qsub_instance_type = \"-l instance_type=\" + $scope.instance_type;}else{$scope.qsub_instance_type = \"\";} if($scope.subnet_id){$scope.qsub_subnet_id = \"-l subnet_id=\" + $scope.subnet_id;}else{$scope.qsub_subnet_id = \"\";} if($scope.spot_price){$scope.qsub_spot_price = \"-l spot_price=\" + $scope.spot_price;}else{$scope.qsub_spot_price= \"\";} if($scope.root_size){$scope.qsub_root_size= \"-l root_size=\" + $scope.root_size;}else{$scope.qsub_root_size= \"\";} if($scope.scratch_size){$scope.qsub_scratch_size = \"-l scratch_size=\" + $scope.scratch_size;}else{$scope.qsub_scratch_size= \"\";} if($scope.scratch_iops){$scope.qsub_scratch_iops= \"-l scratch_iops=\" + $scope.scratch_iops;}else{$scope.qsub_scratch_iops= \"\";} if($scope.spot_allocation_count){$scope.qsub_spot_allocation_count= \"-l spot_allocation_count=\" + $scope.spot_allocation_count;}else{$scope.qsub_spot_allocation_count= \"\";} if($scope.spot_allocation_strategy){$scope.qsub_spot_allocation_strategy= \"-l spot_allocation_strategy=\" + $scope.spot_allocation_strategy;}else{$scope.qsub_spot_allocation_strategy= \"\";} if($scope.keep_ebs){$scope.qsub_keep_ebs = \"-l keep_ebs=True\";}else{$scope.qsub_keep_ebs= \"\";} if($scope.efa_support){$scope.qsub_efa_support = \"-l efa_support=True\";}else{$scope.qsub_efa_support= \"\";} if($scope.placement_group){$scope.qsub_placement_group = \"-l placement_group=False\";}else{$scope.qsub_placement_group= \"\";} if($scope.ht_support){$scope.qsub_ht_support = \"-l ht_support=True\";}else{$scope.qsub_ht_support= \"\";} if($scope.fsx_lustre){$scope.qsub_fsx_lustre = \"-l fsx_lustre=\" + $scope.fsx_lustre;}else{$scope.qsub_fsx_lustre = \"\";} if($scope.fsx_lustre_size){$scope.qsub_fsx_lustre_size = \"-l fsx_lustre_size=\" + $scope.fsx_lustre_size;}else{$scope.qsub_fsx_lustre_size = \"\";} if($scope.force_ri){$scope.qsub_force_ri = \"-l force_ri=True\";}else{$scope.qsub_fsx_lustre = \"\";} if (!$scope.nodes){$scope.nodes_count=0;}else{$scope.nodes_count=$scope.nodes} if (+$scope.spot_allocation_count >= +$scope.nodes_count) { $scope.spot_allocation_error = \"Allocated spots must be lower than the number of nodes provisioned for your job\"; $scope.qsub_spot_allocation_count= \"\"; } else { $scope.spot_allocation_error = \"\"; } if ($scope.spot_allocation_count && !$scope.spot_price) { $scope.spot_allocation_error_price = \"Spot Price must be specified\"; $scope.qsub_spot_allocation_count= \"\"; } else { $scope.spot_allocation_error_price = \"\"; } if ($scope.spot_allocation_strategy && !$scope.spot_price) { $scope.spot_allocation_strategy_price = \"Spot Price must be specified\"; $scope.qsub_spot_allocation_strategy= \"\"; } else { $scope.spot_allocation_strategy_price = \"\"; } if ($scope.base_os && !$scope.instance_ami) { $scope.base_os_error = \"No need to specify base_os if you don't use a custom AMI\"; $scope.qsub_base_os= \"\"; } else { $scope.base_os_error = \"\"; } }; }]);","title":"Job Submission Generator"},{"location":"tutorials/job-licenses-flexlm/","text":"In this page, we will see how Scale-Out Computing on AWS manages job and capacity provisioning based on license availabilities. Example configuration Test settings used for all examples: License Server Hostname: licenses.soca.dev License Server port: 5000 License Daemon port: 5001 Feature to check: Audio_System_Toolbox Scale-Out Computing on AWS cluster name: rctest Firewall Configuration \u00b6 Depending your configuration, you may need to edit the security groups to allow traffic to/from your license servers. FlexLM server installed on Scheduler host No further actions are required if you have installed your FlexLM server on the scheduler host as Scale-Out Computing on AWS automatically allow all traffic between the scheduler and the compute nodes. Warning FlexLM configure two ports for each application (DAEMON and SERVER ports). You need to allow both of them. Allow traffic from your license server IP to Scale-Out Computing on AWS Assuming my license server IP is 10.0.15.18 , simply go to the EC2 console, locate your Scheduler and ComputeNode security groups (filter by your cluster name) associated to your Scale-Out Computing on AWS cluster and allow both SERVER and DAEMON ports: Allow traffic from Scale-Out Computing on AWS to your license server Since FlexLM use client/server protocol, you will need to authorize traffic coming from Scale-Out Computing on AWS to your license servers for both SERVER and DAEMON ports. You will need to allow the IP for your scheduler as well as the NAT Gateway used by the compute nodes. Your Scheduler Public IP is listed on CloudFormation, to retrieve your NAT Gateway IP, visit VPC console, select NAT Gateway and find the NAT Gateway IP associated to your Scale-Out Computing on AWS cluster. Upload your lmutil \u00b6 lmutil binary is not included with Scale-Out Computing on AWS. You are required to upload it manually and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py with the location of your file. arg = parser . parse_args () lmstat_path = \"PATH_TO_LMUTIL\" if lmstat_path == \"PATH_TO_LMUTIL\" : print ( 'Please specify a link to your lmutil binary (edit line 19 of this file' ) sys . exit ( 1 ) Note You do not need to install FlexLM server manager. Only lmutil binary is required. lmutil and RHEL based distro FlexLM may requires 32 bits lib depending your system. If launching lmutil returns an ELF version mismatch, simply install yum install redhat-lsb (or equivalent) How to retrieve number of licenses available \u00b6 Scale-Out Computing on AWS includes a script ( /apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py ) which output the number of FlexLM available for a given feature. This script takes the following arguments: -s: The license server hostname -p: The port used by your flexlm deamon -f: The feature name (case sensitive) (Optional) -m: Reserve licenses number for non HPC usage Let say you have 30 Audio_System_Toolbox licenses and 4 are currently in use. The command below will list how many licenses are currently available to use for your jobs: license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox 26 Now let's say you want to reserve 15 licenses for non HPC/Scale-Out Computing on AWS usage: license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox -m 15 11 Info license_check.py is simply a lmutil wrapper. You can get the same output by running the command below and adding some regex validations. You can edit the script to match your own requirements if needed lmutil lmstat -a -c 5000 @licenses-soca.dev | grep \"Users of Audio_System_Toolbox:\" Integration with Scale-Out Computing on AWS \u00b6 IMPORTANT The name of the resource must be *_lic_* . We recommend using <application>_lic_<feature_name> Update your /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/licenses_mapping.yml and create a new resource. This file must follow the YAML syntax. # There is no requirements for section names, but I recommend having 1 section = 1 application matlab : matlab_lic_audiosystemtoolbox : \"/apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox\" # Example for other daemons/features comsol : comsol_lic_acoustic : \"/apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py -s licenses.soca.dev -p 27718 -f ACOUSTICS\" comsol_lic_cadimport : \"/apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 27718 -f CADIMPORT\" synopsys : synopsys_lic_testbenchruntime : \"/apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VT_TestbenchRuntime\" synopsys_lic_vcsruntime : \"/apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VCSRuntime_Net\" synopsys_lic_vipambaaxisvt : \"/apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VIP-AMBA-AXI-SVT\" This parameter will let Scale-Out Computing on AWS knows your license mapping and capacity will only be provisioned if enough licenses are available based on job's requirements. Since you are about to create a new custom resource, additional configuration is required at the scheduler level. On the scheduler host, edit /var/spool/pbs/sched_priv/sched_config and add a new server_dyn_res server_dyn_res: \"matlab_lic_audiosystemtoolbox !/apps/soca/ $SOCA_CONFIGURATION /cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox\" On the same file, add your new resource under resources section. This section will not allow a job to run if the amount of assigned resources exceeds the available amount. resources: \"matlab_lic_audiosystemtoolbox, ncpus, mem, arch, host, vnode, aoe, eoe, compute_node\" Finally, edit /var/spool/pbs/server_priv/resourcedef and add your new resource with type=long ... ht_support type = string base_os type = string fsx_lustre_bucket type = string fsx_lustre_size type = string fsx_lustre_dns type = string matlab_lic_audiosystemtoolbox type = long Once done, restart the scheduler using service pbs restart Test \u00b6 For this example, let's assume we do have 3 \"Audio_System_Toolbox\" licenses available /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox 3 Let's try to submit a job which require 5 licenses qsub -l matlab_lic_audiosystemtoolbox = 5 -- /bin/sleep 600 31 .ip-20-0-2-69 Let's check the log files under /apps/soca/$SOCA_CONFIGURATION/cluster_manager/log/<QUEUE_NAME> . Scale-Out Computing on AWS will ignore this job due to the lack of licenses available [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 3}] [157] [INFO] [Next User is mickael] [157] [INFO] [Next Job for user is ['31']] [157] [INFO] [Checking if we have enough resources available to run job_31] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] [157] [INFO] [Ignoring job_31 as we we dont have enough: matlab_lic_audiosystemtoolbox] If you have multiple jobs in the queue, the license counter is dynamically updated each time the dispatcher script is running (every 3 minutes): [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 10}] [157] [INFO] [Checking if we have enough resources available to run job_31] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] # Next job in in queue [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 5}] [157] [INFO] [Checking if we have enough resources available to run job_32] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] # Next job in in queue [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 0}] [157] [INFO] [Checking if we have enough resources available to run job_33] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] [157] [INFO] [Ignoring job_33 as we we dont have enough: matlab_lic_audiosystemtoolbox] Scale-Out Computing on AWS ensures licenses provisioned for given jobs are in use before provisioning capacity for new jobs Let say you have 10 licenses available and you submit job1 and job2 which both have a requirement of 5 licenses. Scale-Out Computing on AWS will determine licenses are available and will start provision the capacity. Shortly after you submit job3 which require another 5 licenses. The first 2 jobs may not have started yet, meaning you still have 10 licenses available (even though the 10 licenses will be used by job1 and job2 as soon as they start). In that case we skip job3 until both job1 and job2 and in running state. Invalid Resource You can not submit if you are using an invalid resource (aka: resource not recognized by the scheduler). If you are getting this error, refer to section Integration with Scale-Out Computing on AWS ) qsub -l matlab_lic_fakeresource = 3 -- /bin/sleep 60 qsub: Unknown resource Resource_List.matlab_lic_fakeresource","title":"Job with FlexLM licenses requirements"},{"location":"tutorials/job-licenses-flexlm/#firewall-configuration","text":"Depending your configuration, you may need to edit the security groups to allow traffic to/from your license servers. FlexLM server installed on Scheduler host No further actions are required if you have installed your FlexLM server on the scheduler host as Scale-Out Computing on AWS automatically allow all traffic between the scheduler and the compute nodes. Warning FlexLM configure two ports for each application (DAEMON and SERVER ports). You need to allow both of them. Allow traffic from your license server IP to Scale-Out Computing on AWS Assuming my license server IP is 10.0.15.18 , simply go to the EC2 console, locate your Scheduler and ComputeNode security groups (filter by your cluster name) associated to your Scale-Out Computing on AWS cluster and allow both SERVER and DAEMON ports: Allow traffic from Scale-Out Computing on AWS to your license server Since FlexLM use client/server protocol, you will need to authorize traffic coming from Scale-Out Computing on AWS to your license servers for both SERVER and DAEMON ports. You will need to allow the IP for your scheduler as well as the NAT Gateway used by the compute nodes. Your Scheduler Public IP is listed on CloudFormation, to retrieve your NAT Gateway IP, visit VPC console, select NAT Gateway and find the NAT Gateway IP associated to your Scale-Out Computing on AWS cluster.","title":"Firewall Configuration"},{"location":"tutorials/job-licenses-flexlm/#upload-your-lmutil","text":"lmutil binary is not included with Scale-Out Computing on AWS. You are required to upload it manually and update /apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py with the location of your file. arg = parser . parse_args () lmstat_path = \"PATH_TO_LMUTIL\" if lmstat_path == \"PATH_TO_LMUTIL\" : print ( 'Please specify a link to your lmutil binary (edit line 19 of this file' ) sys . exit ( 1 ) Note You do not need to install FlexLM server manager. Only lmutil binary is required. lmutil and RHEL based distro FlexLM may requires 32 bits lib depending your system. If launching lmutil returns an ELF version mismatch, simply install yum install redhat-lsb (or equivalent)","title":"Upload your lmutil"},{"location":"tutorials/job-licenses-flexlm/#how-to-retrieve-number-of-licenses-available","text":"Scale-Out Computing on AWS includes a script ( /apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py ) which output the number of FlexLM available for a given feature. This script takes the following arguments: -s: The license server hostname -p: The port used by your flexlm deamon -f: The feature name (case sensitive) (Optional) -m: Reserve licenses number for non HPC usage Let say you have 30 Audio_System_Toolbox licenses and 4 are currently in use. The command below will list how many licenses are currently available to use for your jobs: license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox 26 Now let's say you want to reserve 15 licenses for non HPC/Scale-Out Computing on AWS usage: license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox -m 15 11 Info license_check.py is simply a lmutil wrapper. You can get the same output by running the command below and adding some regex validations. You can edit the script to match your own requirements if needed lmutil lmstat -a -c 5000 @licenses-soca.dev | grep \"Users of Audio_System_Toolbox:\"","title":"How to retrieve number of licenses available"},{"location":"tutorials/job-licenses-flexlm/#integration-with-scale-out-computing-on-aws","text":"IMPORTANT The name of the resource must be *_lic_* . We recommend using <application>_lic_<feature_name> Update your /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/licenses_mapping.yml and create a new resource. This file must follow the YAML syntax. # There is no requirements for section names, but I recommend having 1 section = 1 application matlab : matlab_lic_audiosystemtoolbox : \"/apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox\" # Example for other daemons/features comsol : comsol_lic_acoustic : \"/apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py -s licenses.soca.dev -p 27718 -f ACOUSTICS\" comsol_lic_cadimport : \"/apps/soca/cluster_manager/license_check.py -s licenses.soca.dev -p 27718 -f CADIMPORT\" synopsys : synopsys_lic_testbenchruntime : \"/apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VT_TestbenchRuntime\" synopsys_lic_vcsruntime : \"/apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VCSRuntime_Net\" synopsys_lic_vipambaaxisvt : \"/apps/soca/$SOCA_CONFIGURATION/cluster_manager/license_check.py -s licenses.soca.dev -p 27020 -f VIP-AMBA-AXI-SVT\" This parameter will let Scale-Out Computing on AWS knows your license mapping and capacity will only be provisioned if enough licenses are available based on job's requirements. Since you are about to create a new custom resource, additional configuration is required at the scheduler level. On the scheduler host, edit /var/spool/pbs/sched_priv/sched_config and add a new server_dyn_res server_dyn_res: \"matlab_lic_audiosystemtoolbox !/apps/soca/ $SOCA_CONFIGURATION /cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox\" On the same file, add your new resource under resources section. This section will not allow a job to run if the amount of assigned resources exceeds the available amount. resources: \"matlab_lic_audiosystemtoolbox, ncpus, mem, arch, host, vnode, aoe, eoe, compute_node\" Finally, edit /var/spool/pbs/server_priv/resourcedef and add your new resource with type=long ... ht_support type = string base_os type = string fsx_lustre_bucket type = string fsx_lustre_size type = string fsx_lustre_dns type = string matlab_lic_audiosystemtoolbox type = long Once done, restart the scheduler using service pbs restart","title":"Integration with Scale-Out Computing on AWS"},{"location":"tutorials/job-licenses-flexlm/#test","text":"For this example, let's assume we do have 3 \"Audio_System_Toolbox\" licenses available /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/license_check.py -s licenses.soca.dev -p 5000 -f Audio_System_Toolbox 3 Let's try to submit a job which require 5 licenses qsub -l matlab_lic_audiosystemtoolbox = 5 -- /bin/sleep 600 31 .ip-20-0-2-69 Let's check the log files under /apps/soca/$SOCA_CONFIGURATION/cluster_manager/log/<QUEUE_NAME> . Scale-Out Computing on AWS will ignore this job due to the lack of licenses available [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 3}] [157] [INFO] [Next User is mickael] [157] [INFO] [Next Job for user is ['31']] [157] [INFO] [Checking if we have enough resources available to run job_31] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] [157] [INFO] [Ignoring job_31 as we we dont have enough: matlab_lic_audiosystemtoolbox] If you have multiple jobs in the queue, the license counter is dynamically updated each time the dispatcher script is running (every 3 minutes): [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 10}] [157] [INFO] [Checking if we have enough resources available to run job_31] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] # Next job in in queue [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 5}] [157] [INFO] [Checking if we have enough resources available to run job_32] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] # Next job in in queue [157] [INFO] [License Available: {'matlab_lic_audiosystemtoolbox': 0}] [157] [INFO] [Checking if we have enough resources available to run job_33] .... [157] [INFO] [No default value for matlab_lic_audiosystemtoolbox. Creating new entry with value: 5] [157] [INFO] [Ignoring job_33 as we we dont have enough: matlab_lic_audiosystemtoolbox] Scale-Out Computing on AWS ensures licenses provisioned for given jobs are in use before provisioning capacity for new jobs Let say you have 10 licenses available and you submit job1 and job2 which both have a requirement of 5 licenses. Scale-Out Computing on AWS will determine licenses are available and will start provision the capacity. Shortly after you submit job3 which require another 5 licenses. The first 2 jobs may not have started yet, meaning you still have 10 licenses available (even though the 10 licenses will be used by job1 and job2 as soon as they start). In that case we skip job3 until both job1 and job2 and in running state. Invalid Resource You can not submit if you are using an invalid resource (aka: resource not recognized by the scheduler). If you are getting this error, refer to section Integration with Scale-Out Computing on AWS ) qsub -l matlab_lic_fakeresource = 3 -- /bin/sleep 60 qsub: Unknown resource Resource_List.matlab_lic_fakeresource","title":"Test"},{"location":"tutorials/job-start-stop-email-notification/","text":"In this page, I will show you how to configure email notification when your job start/stop. For this example, I will use Simple Email Service (SES) , but you can use any SMTP provider. Info Note1: By default, the Scale-Out Computing on AWS admin user you created during the installation does not have any associated email address. If you want to use this account you must edit LDAP and add the \"mail\" attribute. Note2: All qmgr command must be executed on the scheduler host Configure SES sender domain \u00b6 Open the SES console and verify your domain (or specific email addresses). For this example I will verify my entire domain (soca.dev) and enable DKIM support to prevent email spoofing. Click 'Verify this domain', you will get list of DNS records to update for verification. Once done, wait a couple of hours and you will receive a confirmation when your DNS are validated. Configure Recipients addresses \u00b6 By default SES limits you to send email to unique recipients which you will need verify manually If you want to be able to send email to any addresses, you need to request production access . Notification code \u00b6 Create a hook file (note: this file can be found under /apps/soca/$SOCA_CONFIGURATION/cluster_hooks/job_notifications.py on your Scale-Out Computing on AWS cluster) Edit the following section to match your SES settings ses_sender_email = '<SES_SENDER_EMAIL_ADDRESS_HERE>' ses_region = '<YOUR_SES_REGION_HERE>' Create the hooks \u00b6 Once your script is created, configure your scheduler hooks by running the following commands: user@host: qmgr -c \"create hook notify_job_start event=runjob\" user@host: qmgr -c \"create hook notify_job_complete event=execjob_end\" user@host: qmgr -c \"import hook notify_job_start application/x-python default /apps/soca/ $SOCA_CONFIGURATION /cluster_hooks/job_notifications.py\" user@host: qmgr -c \"import hook notify_job_complete application/x-python default /apps/soca/ $SOCA_CONFIGURATION /cluster_hooks/job_notifications.py\" Note: If you make any change to the python file, you must re-run the import hook command Test \u00b6 Let's submit a test job which will last 5 minutes qsub -N mytestjob -- /bin/sleep 300 Now let's verify if I received the alerts correctly. Job start: 5 minutes later: Add/Update email \u00b6 Run ldapsearch -x uid=<USER> command to verify if your user has a valid mail attribute and if this attribute is pointing to the correct email address. The example below shows a user without email attribute. user@host: ldapsearch -x uid = mickael ## mickael, People, soca.local dn: uid = mickael,ou = People,dc = soca,dc = local objectClass: top objectClass: person objectClass: posixAccount objectClass: shadowAccount objectClass: inetOrgPerson objectClass: organizationalPerson uid: mickael uidNumber: 5001 gidNumber: 5001 cn: mickael sn: mickael loginShell: /bin/bash homeDirectory: /data/home/mickael To add/update an email address, create a new ldif file (eg: update_email.ldif) and add the following content dn: uid=mickael,ou=People,dc=soca,dc=local changetype: modify add: mail mail: mickael@soca.dev Then execute the ldapadd as root user@host: ldapadd -x -D cn = admin,dc = soca,dc = local -y /root/OpenLdapAdminPassword.txt -f update_email.ldif modifying entry \"uid=mickael,ou=People,dc=soca,dc=local\" Finally re-run the ldapsearch command and validate your user now has mail attribute user@host: ldapsearch -x uid = mickael dn: uid = mickael,ou = People,dc = soca,dc = local objectClass: top objectClass: person objectClass: posixAccount objectClass: shadowAccount objectClass: inetOrgPerson objectClass: organizationalPerson uid: mickael uidNumber: 5001 gidNumber: 5001 cn: mickael sn: mickael loginShell: /bin/bash homeDirectory: /data/home/mickael mail: mickael@soca.dev Check the logs \u00b6 Scheduler hooks are located: /var/spool/pbs/server_logs/ for notify_job_start on the Scheduler /var/spool/pbs/mom_logs/ for notify_job_complete on the Execution Host(s)","title":"Automatic emails when your job start/stop"},{"location":"tutorials/job-start-stop-email-notification/#configure-ses-sender-domain","text":"Open the SES console and verify your domain (or specific email addresses). For this example I will verify my entire domain (soca.dev) and enable DKIM support to prevent email spoofing. Click 'Verify this domain', you will get list of DNS records to update for verification. Once done, wait a couple of hours and you will receive a confirmation when your DNS are validated.","title":"Configure SES sender domain"},{"location":"tutorials/job-start-stop-email-notification/#configure-recipients-addresses","text":"By default SES limits you to send email to unique recipients which you will need verify manually If you want to be able to send email to any addresses, you need to request production access .","title":"Configure Recipients addresses"},{"location":"tutorials/job-start-stop-email-notification/#notification-code","text":"Create a hook file (note: this file can be found under /apps/soca/$SOCA_CONFIGURATION/cluster_hooks/job_notifications.py on your Scale-Out Computing on AWS cluster) Edit the following section to match your SES settings ses_sender_email = '<SES_SENDER_EMAIL_ADDRESS_HERE>' ses_region = '<YOUR_SES_REGION_HERE>'","title":"Notification code"},{"location":"tutorials/job-start-stop-email-notification/#create-the-hooks","text":"Once your script is created, configure your scheduler hooks by running the following commands: user@host: qmgr -c \"create hook notify_job_start event=runjob\" user@host: qmgr -c \"create hook notify_job_complete event=execjob_end\" user@host: qmgr -c \"import hook notify_job_start application/x-python default /apps/soca/ $SOCA_CONFIGURATION /cluster_hooks/job_notifications.py\" user@host: qmgr -c \"import hook notify_job_complete application/x-python default /apps/soca/ $SOCA_CONFIGURATION /cluster_hooks/job_notifications.py\" Note: If you make any change to the python file, you must re-run the import hook command","title":"Create the hooks"},{"location":"tutorials/job-start-stop-email-notification/#test","text":"Let's submit a test job which will last 5 minutes qsub -N mytestjob -- /bin/sleep 300 Now let's verify if I received the alerts correctly. Job start: 5 minutes later:","title":"Test"},{"location":"tutorials/job-start-stop-email-notification/#addupdate-email","text":"Run ldapsearch -x uid=<USER> command to verify if your user has a valid mail attribute and if this attribute is pointing to the correct email address. The example below shows a user without email attribute. user@host: ldapsearch -x uid = mickael ## mickael, People, soca.local dn: uid = mickael,ou = People,dc = soca,dc = local objectClass: top objectClass: person objectClass: posixAccount objectClass: shadowAccount objectClass: inetOrgPerson objectClass: organizationalPerson uid: mickael uidNumber: 5001 gidNumber: 5001 cn: mickael sn: mickael loginShell: /bin/bash homeDirectory: /data/home/mickael To add/update an email address, create a new ldif file (eg: update_email.ldif) and add the following content dn: uid=mickael,ou=People,dc=soca,dc=local changetype: modify add: mail mail: mickael@soca.dev Then execute the ldapadd as root user@host: ldapadd -x -D cn = admin,dc = soca,dc = local -y /root/OpenLdapAdminPassword.txt -f update_email.ldif modifying entry \"uid=mickael,ou=People,dc=soca,dc=local\" Finally re-run the ldapsearch command and validate your user now has mail attribute user@host: ldapsearch -x uid = mickael dn: uid = mickael,ou = People,dc = soca,dc = local objectClass: top objectClass: person objectClass: posixAccount objectClass: shadowAccount objectClass: inetOrgPerson objectClass: organizationalPerson uid: mickael uidNumber: 5001 gidNumber: 5001 cn: mickael sn: mickael loginShell: /bin/bash homeDirectory: /data/home/mickael mail: mickael@soca.dev","title":"Add/Update email"},{"location":"tutorials/job-start-stop-email-notification/#check-the-logs","text":"Scheduler hooks are located: /var/spool/pbs/server_logs/ for notify_job_start on the Scheduler /var/spool/pbs/mom_logs/ for notify_job_complete on the Execution Host(s)","title":"Check the logs"},{"location":"tutorials/launch-always-on-instances/","text":"Why AlwaysOn instances? \u00b6 By default, Scale-Out Computing on AWS provisions on-demand capacity when there are jobs in the normal queue. This means any job submitted will wait in the queue 5 to 8 minutes until EC2 capacity is ready. If you want to avoid this penalty, you can provision \"AlwaysOn instances\". Please note you will be charged until you manually terminate it or specify --terminate_when_idle option How to launch an AlwaysOn instances? \u00b6 On your scheduler host, sudo as root and run source /etc/environment to load Scale-Out Computing on AWS shell and then execute /apps/soca/$SOCA_CONFIGURATION/python/latest/bin/python3 /apps/soca/$SOCA_CONFIGURATION/cluster_manager/add_nodes.py [ root@ip-a-b-c-d ~ ] # /apps/soca/$SOCA_CONFIGURATION/python/latest/bin/python3 \\ /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/add_nodes.py -h usage: add_nodes.py [ -h ] --desired_capacity [ DESIRED_CAPACITY ] --instance_type [ INSTANCE_TYPE ] --job_name [ JOB_NAME ] --job_owner [ JOB_OWNER ] --queue [ QUEUE ] [ --efa_support EFA_SUPPORT ] [ --ht_support HT_SUPPORT ] [ --keep_forever KEEP_FOREVER ] [ --terminate_when_idle [ TERMINATE_WHEN_IDLE ]] [ --base_os BASE_OS ] [ --fsx_lustre FSX_LUSTRE ] [ --fsx_lustre_size FSX_LUSTRE_SIZE ] [ --fsx_lustre_per_unit_throughput FSX_LUSTRE_PER_UNIT_THROUGHPUT ] [ --fsx_lustre_deployment_type FSX_LUSTRE_DEPLOYMENT_TYPE ] --instance_ami [ INSTANCE_AMI ] [ --job_id [ JOB_ID ]] [ --job_project [ JOB_PROJECT ]] [ --placement_group PLACEMENT_GROUP ] [ --root_size [ ROOT_SIZE ]] [ --scratch_iops [ SCRATCH_IOPS ]] [ --scratch_size [ SCRATCH_SIZE ]] [ --spot_allocation_count [ SPOT_ALLOCATION_COUNT ]] [ --spot_allocation_strategy [ SPOT_ALLOCATION_STRATEGY ]] [ --spot_price [ SPOT_PRICE ]] [ --keep_ebs ] [ --subnet_id SUBNET_ID ] [ --tags [ TAGS ]] [ --weighted_capacity [ WEIGHTED_CAPACITY ]] optional arguments: -h, --help show this help message and exit --desired_capacity [ DESIRED_CAPACITY ] Number of EC2 instances to deploy --instance_type [ INSTANCE_TYPE ] Instance type you want to deploy --job_name [ JOB_NAME ] Job Name for which the capacity is being provisioned --job_owner [ JOB_OWNER ] Job Owner for which the capacity is being provisioned --queue [ QUEUE ] Queue to map the capacity --efa_support EFA_SUPPORT Support for EFA --ht_support HT_SUPPORT Enable Hyper Threading --keep_forever KEEP_FOREVER Whether or not capacity will stay forever --terminate_when_idle [ TERMINATE_WHEN_IDLE ] If instances will be terminated when idle for N minutes --base_os BASE_OS Specify custom Base OK --fsx_lustre FSX_LUSTRE Mount existing FSx by providing the DNS --fsx_lustre_size FSX_LUSTRE_SIZE Specify size of your FSx --fsx_lustre_per_unit_throughput FSX_LUSTRE_PER_UNIT_THROUGHPUT Storage baseline if FSX type is Persistent --fsx_lustre_deployment_type FSX_LUSTRE_DEPLOYMENT_TYPE Type of your FSx for Lustre --instance_ami [ INSTANCE_AMI ] AMI to use --job_id [ JOB_ID ] Job ID for which the capacity is being provisioned --job_project [ JOB_PROJECT ] Job Owner for which the capacity is being provisioned --placement_group PLACEMENT_GROUP Enable or disable placement group --root_size [ ROOT_SIZE ] Size of Root partition in GB --scratch_iops [ SCRATCH_IOPS ] Size of /scratch in GB --scratch_size [ SCRATCH_SIZE ] Size of /scratch in GB --spot_allocation_count [ SPOT_ALLOCATION_COUNT ] When using mixed OD and SPOT, choose % of SPOT --spot_allocation_strategy [ SPOT_ALLOCATION_STRATEGY ] lowest-price or capacity-optimized or diversified ( supported only for SpotFleet ) --spot_price [ SPOT_PRICE ] Spot Price --keep_ebs Do not delete EBS disk --subnet_id SUBNET_ID Launch capacity in a special subnet --tags [ TAGS ] Tags, format must be { 'Key' : 'Value' } --weighted_capacity [ WEIGHTED_CAPACITY ] Weighted capacity for EC2 instances To launch \"AlwaysOn\" instances, there are two alternative methods either using --keep_forever or --terminate_when_idle options. Using keep_forever option \u00b6 Use --keep_forever true and alwayson queue. If you do not want to use alwayson queue, make sure the queue you have created has been configured correctly to support AlwaysOn ( see instructions ) See example below (note: you can use additional parameters if needed) /apps/soca/ $SOCA_CONFIGURATION /python/latest/bin/python3 \\ /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/add_nodes.py \\ --instance_type = c5.large \\ --desired_capacity = 1 \\ --keep_forever true \\ --job_owner mickael \\ --job_name always_on_capacity \\ --queue alwayson When the capacity is available, simply run a job and specify alwayson as queue name Terminate an AlwaysOn instance launched with keep_forever \u00b6 Simply go to your CloudFormation console, locate the stack following the naming convention: soca-<cluster_name>-keepforever-<queue_name>-uniqueid and terminate it. Using terminate_when_idle option \u00b6 Use --terminate_when_idle N where N represents the number of minutes when the instance(s) where be terminated after all running jobs on the instances exit, Use --keep_forever false , and Use alwayson queue. If you do not want to use alwayson queue, make sure the queue you have created has been configured correctly to support AlwaysOn ( see instructions ) See example below (note: you can use additional parameters if needed) /apps/soca/ $SOCA_CONFIGURATION /python/latest/bin/python3 \\ /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/add_nodes.py \\ --instance_type = c5.large \\ --desired_capacity = 1 \\ --terminate_when_idle 5 \\ --keep_forever false \\ --job_owner mickael \\ --job_name always_on_capacity \\ --queue alwayson When the instances become available, simply submit a job and specify -q alwayson . The instance(s) launched with --terminate_when_idle will be terminated automatically once all jobs running on the instance complete then the instance is detected as idle (no jobs running) for the specified number of minutes (5 in the example above). How to launch capacity based on vCPUs/cores instead of instances? \u00b6 You can launch capacity based on vCPUs or cores using --weighted_capacity option and specify a corresponding weight for each instance type specified in the instance_type option. This will pass WeightedCapacity to the corresponding Auto Scaling Group or Spot Fleet. The example below will launch capacity where the total number of vCPUs is at least 24 vCPUs depending on availability of c5.large, c5.xlarge, and c5.2xlarge instances. In this example, ht_support is set to true to utilize vCPUs on each instance, and the weighted_capacity option has 3 weights corresponding to the desired weight of c5.large (2 vCPUs), c5.xlarge (4 vCPUs), and c5.2xlarge (8 vCPUs) /apps/soca/ $SOCA_CONFIGURATION /python/latest/bin/python3 \\ /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/add_nodes.py \\ --instance_type c5.large+c5.xlarge+c5.2xlarge \\ --desired_capacity 24 \\ --weighted_capacity 2 +4+8 \\ --ht_support true \\ --terminate_when_idle 5 \\ --keep_forever false \\ --job_owner mickael \\ --job_name vcpus_capacity \\ --queue alwayson When the instances become available, simply submit the jobs and specify -q alwayson . The instance(s) launched with --terminate_when_idle will be terminated automatically once all jobs running on the instances complete then each instance is detected as idle (no jobs running) for the specified number of minutes (5 in the example above).","title":"Launch AlwaysOn nodes"},{"location":"tutorials/launch-always-on-instances/#why-alwayson-instances","text":"By default, Scale-Out Computing on AWS provisions on-demand capacity when there are jobs in the normal queue. This means any job submitted will wait in the queue 5 to 8 minutes until EC2 capacity is ready. If you want to avoid this penalty, you can provision \"AlwaysOn instances\". Please note you will be charged until you manually terminate it or specify --terminate_when_idle option","title":"Why AlwaysOn instances?"},{"location":"tutorials/launch-always-on-instances/#how-to-launch-an-alwayson-instances","text":"On your scheduler host, sudo as root and run source /etc/environment to load Scale-Out Computing on AWS shell and then execute /apps/soca/$SOCA_CONFIGURATION/python/latest/bin/python3 /apps/soca/$SOCA_CONFIGURATION/cluster_manager/add_nodes.py [ root@ip-a-b-c-d ~ ] # /apps/soca/$SOCA_CONFIGURATION/python/latest/bin/python3 \\ /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/add_nodes.py -h usage: add_nodes.py [ -h ] --desired_capacity [ DESIRED_CAPACITY ] --instance_type [ INSTANCE_TYPE ] --job_name [ JOB_NAME ] --job_owner [ JOB_OWNER ] --queue [ QUEUE ] [ --efa_support EFA_SUPPORT ] [ --ht_support HT_SUPPORT ] [ --keep_forever KEEP_FOREVER ] [ --terminate_when_idle [ TERMINATE_WHEN_IDLE ]] [ --base_os BASE_OS ] [ --fsx_lustre FSX_LUSTRE ] [ --fsx_lustre_size FSX_LUSTRE_SIZE ] [ --fsx_lustre_per_unit_throughput FSX_LUSTRE_PER_UNIT_THROUGHPUT ] [ --fsx_lustre_deployment_type FSX_LUSTRE_DEPLOYMENT_TYPE ] --instance_ami [ INSTANCE_AMI ] [ --job_id [ JOB_ID ]] [ --job_project [ JOB_PROJECT ]] [ --placement_group PLACEMENT_GROUP ] [ --root_size [ ROOT_SIZE ]] [ --scratch_iops [ SCRATCH_IOPS ]] [ --scratch_size [ SCRATCH_SIZE ]] [ --spot_allocation_count [ SPOT_ALLOCATION_COUNT ]] [ --spot_allocation_strategy [ SPOT_ALLOCATION_STRATEGY ]] [ --spot_price [ SPOT_PRICE ]] [ --keep_ebs ] [ --subnet_id SUBNET_ID ] [ --tags [ TAGS ]] [ --weighted_capacity [ WEIGHTED_CAPACITY ]] optional arguments: -h, --help show this help message and exit --desired_capacity [ DESIRED_CAPACITY ] Number of EC2 instances to deploy --instance_type [ INSTANCE_TYPE ] Instance type you want to deploy --job_name [ JOB_NAME ] Job Name for which the capacity is being provisioned --job_owner [ JOB_OWNER ] Job Owner for which the capacity is being provisioned --queue [ QUEUE ] Queue to map the capacity --efa_support EFA_SUPPORT Support for EFA --ht_support HT_SUPPORT Enable Hyper Threading --keep_forever KEEP_FOREVER Whether or not capacity will stay forever --terminate_when_idle [ TERMINATE_WHEN_IDLE ] If instances will be terminated when idle for N minutes --base_os BASE_OS Specify custom Base OK --fsx_lustre FSX_LUSTRE Mount existing FSx by providing the DNS --fsx_lustre_size FSX_LUSTRE_SIZE Specify size of your FSx --fsx_lustre_per_unit_throughput FSX_LUSTRE_PER_UNIT_THROUGHPUT Storage baseline if FSX type is Persistent --fsx_lustre_deployment_type FSX_LUSTRE_DEPLOYMENT_TYPE Type of your FSx for Lustre --instance_ami [ INSTANCE_AMI ] AMI to use --job_id [ JOB_ID ] Job ID for which the capacity is being provisioned --job_project [ JOB_PROJECT ] Job Owner for which the capacity is being provisioned --placement_group PLACEMENT_GROUP Enable or disable placement group --root_size [ ROOT_SIZE ] Size of Root partition in GB --scratch_iops [ SCRATCH_IOPS ] Size of /scratch in GB --scratch_size [ SCRATCH_SIZE ] Size of /scratch in GB --spot_allocation_count [ SPOT_ALLOCATION_COUNT ] When using mixed OD and SPOT, choose % of SPOT --spot_allocation_strategy [ SPOT_ALLOCATION_STRATEGY ] lowest-price or capacity-optimized or diversified ( supported only for SpotFleet ) --spot_price [ SPOT_PRICE ] Spot Price --keep_ebs Do not delete EBS disk --subnet_id SUBNET_ID Launch capacity in a special subnet --tags [ TAGS ] Tags, format must be { 'Key' : 'Value' } --weighted_capacity [ WEIGHTED_CAPACITY ] Weighted capacity for EC2 instances To launch \"AlwaysOn\" instances, there are two alternative methods either using --keep_forever or --terminate_when_idle options.","title":"How to launch an AlwaysOn instances?"},{"location":"tutorials/launch-always-on-instances/#using-keep_forever-option","text":"Use --keep_forever true and alwayson queue. If you do not want to use alwayson queue, make sure the queue you have created has been configured correctly to support AlwaysOn ( see instructions ) See example below (note: you can use additional parameters if needed) /apps/soca/ $SOCA_CONFIGURATION /python/latest/bin/python3 \\ /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/add_nodes.py \\ --instance_type = c5.large \\ --desired_capacity = 1 \\ --keep_forever true \\ --job_owner mickael \\ --job_name always_on_capacity \\ --queue alwayson When the capacity is available, simply run a job and specify alwayson as queue name","title":"Using keep_forever option"},{"location":"tutorials/launch-always-on-instances/#terminate-an-alwayson-instance-launched-with-keep_forever","text":"Simply go to your CloudFormation console, locate the stack following the naming convention: soca-<cluster_name>-keepforever-<queue_name>-uniqueid and terminate it.","title":"Terminate an AlwaysOn instance launched with keep_forever"},{"location":"tutorials/launch-always-on-instances/#using-terminate_when_idle-option","text":"Use --terminate_when_idle N where N represents the number of minutes when the instance(s) where be terminated after all running jobs on the instances exit, Use --keep_forever false , and Use alwayson queue. If you do not want to use alwayson queue, make sure the queue you have created has been configured correctly to support AlwaysOn ( see instructions ) See example below (note: you can use additional parameters if needed) /apps/soca/ $SOCA_CONFIGURATION /python/latest/bin/python3 \\ /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/add_nodes.py \\ --instance_type = c5.large \\ --desired_capacity = 1 \\ --terminate_when_idle 5 \\ --keep_forever false \\ --job_owner mickael \\ --job_name always_on_capacity \\ --queue alwayson When the instances become available, simply submit a job and specify -q alwayson . The instance(s) launched with --terminate_when_idle will be terminated automatically once all jobs running on the instance complete then the instance is detected as idle (no jobs running) for the specified number of minutes (5 in the example above).","title":"Using terminate_when_idle option"},{"location":"tutorials/launch-always-on-instances/#how-to-launch-capacity-based-on-vcpuscores-instead-of-instances","text":"You can launch capacity based on vCPUs or cores using --weighted_capacity option and specify a corresponding weight for each instance type specified in the instance_type option. This will pass WeightedCapacity to the corresponding Auto Scaling Group or Spot Fleet. The example below will launch capacity where the total number of vCPUs is at least 24 vCPUs depending on availability of c5.large, c5.xlarge, and c5.2xlarge instances. In this example, ht_support is set to true to utilize vCPUs on each instance, and the weighted_capacity option has 3 weights corresponding to the desired weight of c5.large (2 vCPUs), c5.xlarge (4 vCPUs), and c5.2xlarge (8 vCPUs) /apps/soca/ $SOCA_CONFIGURATION /python/latest/bin/python3 \\ /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/add_nodes.py \\ --instance_type c5.large+c5.xlarge+c5.2xlarge \\ --desired_capacity 24 \\ --weighted_capacity 2 +4+8 \\ --ht_support true \\ --terminate_when_idle 5 \\ --keep_forever false \\ --job_owner mickael \\ --job_name vcpus_capacity \\ --queue alwayson When the instances become available, simply submit the jobs and specify -q alwayson . The instance(s) launched with --terminate_when_idle will be terminated automatically once all jobs running on the instances complete then each instance is detected as idle (no jobs running) for the specified number of minutes (5 in the example above).","title":"How to launch capacity based on vCPUs/cores instead of instances?"},{"location":"tutorials/launch-your-first-job/","text":"Submit your job \u00b6 Things to know before you start Jobs start on average 5 minutes after submission (this value may differ depending on the number and type of compute resource you need to be provisioned). You can reduce this cold-start by pre-configuring your AMI Nodes are ephemeral and tie to a given job id. If needed, you can launch 'AlwaysOn' instances that will be running 24/7. If your simulation requires a lot of disk I/O, it's recommended to use high performance SSD-NVMe disks (using /scratch location) and not default $HOME path Use the web-based simulator to generate your qsub/script command. Web Based Job Submission In addition of regular qsub, SOCA supports web based job submission as well as via HTTP REST API To get started, create a simple text file and name it \"job_submit.que\". See below for a simple template (you will be required to edit whatever is between **) # !/bin/bash # # BEGIN PBS SETTINGS: Note PBS lines MUST start with # # PBS -N **your_job_name** # PBS -V -j oe -o **your_job_name**.qlog # PBS -P **your_project** # PBS -q **your_queue** # PBS -l nodes = **number_of_nodes_for_this_job** # # END PBS SETTINGS # # BEGIN ACTUAL CODE ** your code goes here ** # # END ACTUAL CODE Run your job \u00b6 Run qsub job_submit.que to submit your job to the queue. user@host:~$ qsub job_submit.que 3323.ip-10-10-10-28 If your qsub command succeed, you will receive an id for your job (3323 in this example). To get more information about this job, run qstat -f 3323 (or qstat -f 3323 -x is the job is already terminated). Your job will start as soon as resources are available (usually within 5 minutes after job submission) Delete a job from the queue \u00b6 Run qdel <job_id> to remove a job from the queue. If the job was running, associated capacity will be terminated within 4 minutes. Custom AWS scheduler resources (optional) \u00b6 Here is a list of scheduler resources specially designed for workloads running on AWS. The line starting with -l (lowercase L) is meant to define scheduler resources which will be used by this job. Syntax is as follow: In a script: #PBS -l parameter_name=parameter_value,parameter_name_2=parameter_value_2 Using qsub: qsub -l parameter_name=parameter_value -l parameter_name_2=parameter_value_2 myscript.sh Info If you don't specify them, your job will use the default values configured for your queue (see /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml ) Specify an EC2 Instance Type (optional) \u00b6 Scale-Out Computing on AWS supports all type of EC2 instance. If you don't specify it, job will use a default type which may not be optimal (eg: simulation is memory intensive but default EC2 is compute optimized) If you are not familiar with EC2 instances, take some time to review https://aws.amazon.com/ec2/instance-types/ If you want to force utilization of a specific instance type (and not use the default one), simply change the line and modify instance_type value #PBS -l [existing_parameters...],instance_type=**instance_type_value** Specify a license restriction (optional) \u00b6 License Mapping Please refer to /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/licenses_mapping.yml for a list of licenses you can restrict. Contact your Administrator if your license is not available yet. If your job needs to check-out a specific license to run, you want to make sure enough licenses are available before provisioning capacity for the job. To do so, you can add a new resource which will be your license name and the number of license you need. Example: Your job will only start if we have at least 2 Synopsys VCSRuntime_Net licenses available. #PBS -l [existing_parameters...],synopsys_lic_vcsruntimenet=2 Manage your application logs \u00b6 PBS will automatically generate a .qlog file once the job is complete as shown below. #PBS -V -j oe -o **your_job_name**.qlog If you need more verbose log, we recommend you using STDERR/STDOUT redirection on your code My job is queued. What next? (AWS orchestration) \u00b6 First, let's make sure your jobs have been sent to the queue. You can run default qstat or use aligoqstat which is a custom wrapper developed for Scale-Out Computing on AWS. Web Based CLI As soon as jobs are sent to the queue, our in-house dispatcher script which will decide if the job can start based on hardware availabilities, priorities or license requirements. Run qstat -f **job_id** | grep Resource . Web Based CLI If you see stack_id or compute_node resource (under select), that means all requirements are met and capacity is being provisioned (aka: CloudFormation stack is created and capacity is being provisioned). Look at your EC2 console. This is what you will see (syntax is **cluster_id**-compute-node-**job_id** ): Instances are being provisioned successfully, now let's make sure they are correctly being added to the scheduler by running pbsnodes -a Note: PBS is updated as soon as the host are being added to EC2. You will need to wait a couple of minutes before the state change from \"down\" to \"free\" as Scale-Out Computing on AWS has to configure each node (install libraries, scheduler ...) user@host:~$ pbsnodes -a #Host Ready ip-90-0-118-49 Mom = ip-90-0-118-49.us-west-2.compute.internal ntype = PBS state = free pcpus = 16 jobs = 1.ip-90-0-24-214/0 resources_available.arch = linux resources_available.availability_zone = us-west-2a resources_available.compute_node = job1 resources_available.host = ip-90-0-118-49 resources_available.instance_type = c5.4xlarge resources_available.mem = 31890060kb resources_available.ncpus = 16 resources_available.subnet_id = subnet-055c0dcdd6ddbb020 resources_available.vnode = ip-90-0-118-49 resources_assigned.accelerator_memory = 0kb resources_assigned.hbmem = 0kb resources_assigned.mem = 0kb resources_assigned.naccelerators = 0 resources_assigned.ncpus = 1 resources_assigned.vmem = 0kb queue = normal resv_enable = True sharing = default_shared last_state_change_time = Sun Sep 29 23:30:05 2019 # Host not ready yet ip-90-0-188-37 Mom = ip-90-0-188-37.us-west-2.compute.internal ntype = PBS state = state-unknown,down resources_available.availability_zone = us-west-2c resources_available.compute_node = job2 resources_available.host = ip-90-0-188-37 resources_available.instance_type = r5.xlarge resources_available.subnet_id = subnet-0d046c8668ccfdcb0 resources_available.vnode = ip-90-0-188-37 resources_assigned.accelerator_memory = 0kb resources_assigned.hbmem = 0kb resources_assigned.mem = 0kb resources_assigned.naccelerators = 0 resources_assigned.ncpus = 0 resources_assigned.vmem = 0kb queue = normal comment = node down: communication closed resv_enable = True sharing = default_shared last_state_change_time = Sun Sep 29 23:28:05 2019` Simply wait a couple of minutes. Your jobs will start as soon as the PBS nodes are configured. The web ui will also reflect this change. How to submit and run multiple jobs on the same EC2 instance \u00b6 SOCA 2.7.0 includes a new queue named job-shared which allows multiple jobs to run on the same EC2 instance. To allow multiple jobs to run on the same instance, the jobs need to have the same values for the following four parameters: instance_ami instance_type ht_support spot_price If the jobs have the same values, then the jobs can run on the same EC2 instance. If some of the jobs have different values for any of these parameters, then one or more instances will be provisioned for these jobs. EC2 instance capacity for job-shared queue is dynamically provisioned and de-provisioned automatically similar to the normal queue. The provisioning is based on the total number of vCPUs (when ht_support=true) or the total number of cores (when ht_support=false ) for all queued jobs. Instances are deprovisioned after all jobs running on the instance complate and the instance(s) become idle for terminate_when_idle minutes. terminate_when_idle is defined in /apps/soca/$SOCA_CONFIGURATION/cluster_manager/queue_mapping.yml for the job-shared queue. See Examples job submissions for job-shared queue Examples \u00b6 Job Submission Simulator Use the web-based simulator to generate your qsub/script command. How to set a parameter In a script: #PBS -l parameter_name=parameter_value,parameter_name_2=parameter_value_2 Using qsub: qsub -l parameter_name=parameter_value -l parameter_name_2=parameter_value_2 myscript.sh Refer to this page to get a list of all supported parameters For the rest of the examples below, I will run a simple script named \"script.sh\" with the following content: #!/bin/bash # Will output the hostname of the host where the script is executed # If using MPI (more than 1 node), you will get the hostname of all the hosts allocated for your job echo ` hostname ` Run a simple script on 1 node using default settings on 'normal' queue \u00b6 #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=1 ## END PBS SETTINGS cd $HOME ./script.sh >> my_output.log 2 > & 1 Run a simple script on 1 node using default settings on 'normal' queue \u00b6 #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=1 ## END PBS SETTINGS cd $HOME ./script.sh >> my_output.log 2 > & 1 Run a simple MPI script on 3 nodes using custom EC2 instance type \u00b6 This job will use a 3 c5.18xlarge instances #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=3,instance_type=c5.18xlarge ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/soca/ $SOCA_CONFIGURATION /openmpi/4.1.1/lib/ export PATH = $PATH :/apps/soca/ $SOCA_CONFIGURATION /openmpi/4.1.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 3 hosts /apps/soca/ $SOCA_CONFIGURATION /openmpi/4.1.1/bin/mpirun --hostfile mpi_nodes -np 108 script.sh > my_output.log Run a simple script on 3 nodes using custom License Restriction \u00b6 This job will only start if we have at least 4 Comsol Acoustic licenses available #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=3,instance_type=c5.18xlarge,comsol_lic_acoustic=4 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 3 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 108 script.sh > my_output.log Run a simple script on 5 nodes using custom AMI \u00b6 This job will use a user-specified AMI ID #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5.18xlarge,instance_ami=ami-123abcde ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log Run a simple script on 5 nodes using custom AMI using a different OS \u00b6 This job will use a user-specified AMI ID which use a operating system different than the scheduler #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5.18xlarge,instance_ami=ami-123abcde,base_os=centos7 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log Run a simple script on 5 m5.24xlarge SPOT instances as long as instance price is lower than $2.5 per hour \u00b6 This job will use SPOT instances. Instances will be automatically terminated if BID price is higher than $2.5 / per hour per instance #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=m5.24xlarge,spot_price=2.5 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # m5.24xlarge is 48 cores so -np is 48 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 240 script.sh > my_output.log Run a simple script on 5 m5.24xlarge SPOT instances as long as instance price is lower than OD price \u00b6 #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=m5.24xlarge,spot_price=auto ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # m5.24xlarge is 48 cores so -np is 48 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 240 script.sh > my_output.log Submit a job with EFA \u00b6 Make sure to use an instance type supported by EFA https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html#efa-instance-types #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5n.18xlarge,efa_support=true ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/openmpi/4.0.1/lib/ export PATH=$PATH:/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log Use 50 c5.xlarge for your job and fallback to m5.xlarge and r5.xlarge if capacity is not available \u00b6 AWS honors the instance order, so it will try to provision 50 c5.large first and fallback to m5.xlarge/r5.xlarge if needed (in case your account has instance limitation or AWS can't allocate more than X instance type on a given AZ/region). Ultimately, you may end up with the following configuration (but not limited to): 50 c5.xlarge 30 c5.xlarge, 20 m5.xlarge 20 c5.xlarge, 20 m5.xlarge, 10 r5.xlarge Or any other combination. The only certain know is that you will get 50 instances #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=50,instance_type=c5.xlarge+m5.xlarge+r5.xlarge ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/openmpi/4.0.1/lib/ export PATH=$PATH:/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log Use multiple SPOT instance type \u00b6 #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5.xlarge+m5.xlarge+r5.xlarge, spot_price=auto ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/openmpi/4.0.1/lib/ export PATH=$PATH:/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log Provision 50 instances (10 On-Demand and 40 SPOT) \u00b6 #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=50,instance_type=c5.large,spot_allocation_count=40 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/openmpi/4.0.1/lib/ export PATH=$PATH:/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log Multi-lines parameters \u00b6 Custom AMI running on a different distribution than the scheduler, with EFA enable, without placement group and within a specific subnet_id #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal ## Resources can be specified on multiple lines #PBS -l nodes=5,instance_type=c5n.18xlarge,efa_support=yes #PBS -l placement_group=false,base_os=rhel7,ami_id=ami-12345,subnet_id=sub-abcde ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log Examples for job-shared queue \u00b6 Run a simple script on 96 vCPUs using on-demand c5.4xlarge instances on 'job-shared' queue \u00b6 #!/bin/bash for i in { 1 ..96 } ; do qsub -q job-shared -l instance_type = c5.4xlarge -l ht_support = true -- /path/to/script.sh done Since we specified instance_type=c5.4xlarge and ht_support=true , the number of instances required would be calculated as 96 vCPUs (total number of queued jobs is 96 and each job requires 1 vCPU) divided by 16 vCPUs provided by each c5.4xlarge instance. So, these queued jobs would lead to provisioning of 6 on-demand c5.4xlarge instances Run a simple script on 96 cores using on-demand c5.4xlarge instances on 'job-shared' queue \u00b6 #!/bin/bash for i in { 1 ..96 } ; do qsub -q job-shared -l instance_type = c5.4xlarge -l ht_support = false -- /path/to/script.sh done Since we specified instance_type=c5.4xlarge and ht_support=false , the number of instances required would be calculated as 96 cores (total number of queued jobs is 96 and each job requires 1 core) divided by 8 cores provided by each c5.4xlarge instance. So, these queued jobs would lead to provisioning of 12 on-demand c5.4xlarge instances Run a simple script on 96 vCPUs using Spot Fleet with c5.4xlarge or c5.9xlarge on 'job-shared' queue \u00b6 #!/bin/bash for i in { 1 ..96 } ; do qsub -q job-shared -l instance_type = c5.4xlarge+c5.9xlarge -l ht_support = true -l spot_price = auto -- /path/to/script.sh done Since we specified instance_type=c5.4xlarge+c5.9xlarge , and spot_price=auto , this will create a spot fleet request with two instance types c5.4xlarge and c5.9xlarge and the total required capacity would be 96. Weighted Capacity for each instance type would be automatically calculated for c5.4xlarge and c5.9xlarge based on the value of ht_support . In this case the weighted capacity for c5.4xlarge would be 16 and the weighted capacity for c5.9xlarge would be 36. The Spot fleet would then create a corresponding number of instances depending on instance availability. Run a script that requires 4 cores 24 times using Spot Fleet with c5.4xlarge or c5.9xlarge on 'job-shared' queue \u00b6 #!/bin/bash for i in { 1 ..24 } ; do qsub -q job-shared -l instance_type = c5.4xlarge+c5.9xlarge -l ht_support = false -l select = 1 :ncpus = 4 -l spot_price = auto -- /path/to/four_core_script.sh done Since we specified instance_type=c5.4xlarge+c5.9xlarge , and spot_price=auto , this will create a spot fleet request with two instance types c5.4xlarge and c5.9xlarge and the total required capacity would be 96 (24 jobs each requires 4 cores as ht_support is false). Weighted Capacity for each instance type would be automatically calculated for c5.4xlarge and c5.9xlarge based on the value of ht_support . In this case the weighted capacity for c5.4xlarge would be 8 and the weighted capacity for c5.9xlarge would be 18. The Spot fleet would then create a corresponding number of instances depending on instance availability.","title":"Launch your first job"},{"location":"tutorials/launch-your-first-job/#submit-your-job","text":"Things to know before you start Jobs start on average 5 minutes after submission (this value may differ depending on the number and type of compute resource you need to be provisioned). You can reduce this cold-start by pre-configuring your AMI Nodes are ephemeral and tie to a given job id. If needed, you can launch 'AlwaysOn' instances that will be running 24/7. If your simulation requires a lot of disk I/O, it's recommended to use high performance SSD-NVMe disks (using /scratch location) and not default $HOME path Use the web-based simulator to generate your qsub/script command. Web Based Job Submission In addition of regular qsub, SOCA supports web based job submission as well as via HTTP REST API To get started, create a simple text file and name it \"job_submit.que\". See below for a simple template (you will be required to edit whatever is between **) # !/bin/bash # # BEGIN PBS SETTINGS: Note PBS lines MUST start with # # PBS -N **your_job_name** # PBS -V -j oe -o **your_job_name**.qlog # PBS -P **your_project** # PBS -q **your_queue** # PBS -l nodes = **number_of_nodes_for_this_job** # # END PBS SETTINGS # # BEGIN ACTUAL CODE ** your code goes here ** # # END ACTUAL CODE","title":"Submit your job"},{"location":"tutorials/launch-your-first-job/#run-your-job","text":"Run qsub job_submit.que to submit your job to the queue. user@host:~$ qsub job_submit.que 3323.ip-10-10-10-28 If your qsub command succeed, you will receive an id for your job (3323 in this example). To get more information about this job, run qstat -f 3323 (or qstat -f 3323 -x is the job is already terminated). Your job will start as soon as resources are available (usually within 5 minutes after job submission)","title":"Run your job"},{"location":"tutorials/launch-your-first-job/#delete-a-job-from-the-queue","text":"Run qdel <job_id> to remove a job from the queue. If the job was running, associated capacity will be terminated within 4 minutes.","title":"Delete a job from the queue"},{"location":"tutorials/launch-your-first-job/#custom-aws-scheduler-resources-optional","text":"Here is a list of scheduler resources specially designed for workloads running on AWS. The line starting with -l (lowercase L) is meant to define scheduler resources which will be used by this job. Syntax is as follow: In a script: #PBS -l parameter_name=parameter_value,parameter_name_2=parameter_value_2 Using qsub: qsub -l parameter_name=parameter_value -l parameter_name_2=parameter_value_2 myscript.sh Info If you don't specify them, your job will use the default values configured for your queue (see /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml )","title":"Custom AWS scheduler resources (optional)"},{"location":"tutorials/launch-your-first-job/#specify-an-ec2-instance-type-optional","text":"Scale-Out Computing on AWS supports all type of EC2 instance. If you don't specify it, job will use a default type which may not be optimal (eg: simulation is memory intensive but default EC2 is compute optimized) If you are not familiar with EC2 instances, take some time to review https://aws.amazon.com/ec2/instance-types/ If you want to force utilization of a specific instance type (and not use the default one), simply change the line and modify instance_type value #PBS -l [existing_parameters...],instance_type=**instance_type_value**","title":"Specify an EC2 Instance Type (optional)"},{"location":"tutorials/launch-your-first-job/#specify-a-license-restriction-optional","text":"License Mapping Please refer to /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/licenses_mapping.yml for a list of licenses you can restrict. Contact your Administrator if your license is not available yet. If your job needs to check-out a specific license to run, you want to make sure enough licenses are available before provisioning capacity for the job. To do so, you can add a new resource which will be your license name and the number of license you need. Example: Your job will only start if we have at least 2 Synopsys VCSRuntime_Net licenses available. #PBS -l [existing_parameters...],synopsys_lic_vcsruntimenet=2","title":"Specify a license restriction (optional)"},{"location":"tutorials/launch-your-first-job/#manage-your-application-logs","text":"PBS will automatically generate a .qlog file once the job is complete as shown below. #PBS -V -j oe -o **your_job_name**.qlog If you need more verbose log, we recommend you using STDERR/STDOUT redirection on your code","title":"Manage your application logs"},{"location":"tutorials/launch-your-first-job/#my-job-is-queued-what-next-aws-orchestration","text":"First, let's make sure your jobs have been sent to the queue. You can run default qstat or use aligoqstat which is a custom wrapper developed for Scale-Out Computing on AWS. Web Based CLI As soon as jobs are sent to the queue, our in-house dispatcher script which will decide if the job can start based on hardware availabilities, priorities or license requirements. Run qstat -f **job_id** | grep Resource . Web Based CLI If you see stack_id or compute_node resource (under select), that means all requirements are met and capacity is being provisioned (aka: CloudFormation stack is created and capacity is being provisioned). Look at your EC2 console. This is what you will see (syntax is **cluster_id**-compute-node-**job_id** ): Instances are being provisioned successfully, now let's make sure they are correctly being added to the scheduler by running pbsnodes -a Note: PBS is updated as soon as the host are being added to EC2. You will need to wait a couple of minutes before the state change from \"down\" to \"free\" as Scale-Out Computing on AWS has to configure each node (install libraries, scheduler ...) user@host:~$ pbsnodes -a #Host Ready ip-90-0-118-49 Mom = ip-90-0-118-49.us-west-2.compute.internal ntype = PBS state = free pcpus = 16 jobs = 1.ip-90-0-24-214/0 resources_available.arch = linux resources_available.availability_zone = us-west-2a resources_available.compute_node = job1 resources_available.host = ip-90-0-118-49 resources_available.instance_type = c5.4xlarge resources_available.mem = 31890060kb resources_available.ncpus = 16 resources_available.subnet_id = subnet-055c0dcdd6ddbb020 resources_available.vnode = ip-90-0-118-49 resources_assigned.accelerator_memory = 0kb resources_assigned.hbmem = 0kb resources_assigned.mem = 0kb resources_assigned.naccelerators = 0 resources_assigned.ncpus = 1 resources_assigned.vmem = 0kb queue = normal resv_enable = True sharing = default_shared last_state_change_time = Sun Sep 29 23:30:05 2019 # Host not ready yet ip-90-0-188-37 Mom = ip-90-0-188-37.us-west-2.compute.internal ntype = PBS state = state-unknown,down resources_available.availability_zone = us-west-2c resources_available.compute_node = job2 resources_available.host = ip-90-0-188-37 resources_available.instance_type = r5.xlarge resources_available.subnet_id = subnet-0d046c8668ccfdcb0 resources_available.vnode = ip-90-0-188-37 resources_assigned.accelerator_memory = 0kb resources_assigned.hbmem = 0kb resources_assigned.mem = 0kb resources_assigned.naccelerators = 0 resources_assigned.ncpus = 0 resources_assigned.vmem = 0kb queue = normal comment = node down: communication closed resv_enable = True sharing = default_shared last_state_change_time = Sun Sep 29 23:28:05 2019` Simply wait a couple of minutes. Your jobs will start as soon as the PBS nodes are configured. The web ui will also reflect this change.","title":"My job is queued. What next? (AWS orchestration)"},{"location":"tutorials/launch-your-first-job/#how-to-submit-and-run-multiple-jobs-on-the-same-ec2-instance","text":"SOCA 2.7.0 includes a new queue named job-shared which allows multiple jobs to run on the same EC2 instance. To allow multiple jobs to run on the same instance, the jobs need to have the same values for the following four parameters: instance_ami instance_type ht_support spot_price If the jobs have the same values, then the jobs can run on the same EC2 instance. If some of the jobs have different values for any of these parameters, then one or more instances will be provisioned for these jobs. EC2 instance capacity for job-shared queue is dynamically provisioned and de-provisioned automatically similar to the normal queue. The provisioning is based on the total number of vCPUs (when ht_support=true) or the total number of cores (when ht_support=false ) for all queued jobs. Instances are deprovisioned after all jobs running on the instance complate and the instance(s) become idle for terminate_when_idle minutes. terminate_when_idle is defined in /apps/soca/$SOCA_CONFIGURATION/cluster_manager/queue_mapping.yml for the job-shared queue. See Examples job submissions for job-shared queue","title":"How to submit and run multiple jobs on the same EC2 instance"},{"location":"tutorials/launch-your-first-job/#examples","text":"Job Submission Simulator Use the web-based simulator to generate your qsub/script command. How to set a parameter In a script: #PBS -l parameter_name=parameter_value,parameter_name_2=parameter_value_2 Using qsub: qsub -l parameter_name=parameter_value -l parameter_name_2=parameter_value_2 myscript.sh Refer to this page to get a list of all supported parameters For the rest of the examples below, I will run a simple script named \"script.sh\" with the following content: #!/bin/bash # Will output the hostname of the host where the script is executed # If using MPI (more than 1 node), you will get the hostname of all the hosts allocated for your job echo ` hostname `","title":"Examples"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-1-node-using-default-settings-on-normal-queue","text":"#!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=1 ## END PBS SETTINGS cd $HOME ./script.sh >> my_output.log 2 > & 1","title":"Run a simple script on 1 node using default settings on 'normal' queue"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-1-node-using-default-settings-on-normal-queue_1","text":"#!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=1 ## END PBS SETTINGS cd $HOME ./script.sh >> my_output.log 2 > & 1","title":"Run a simple script on 1 node using default settings on 'normal' queue"},{"location":"tutorials/launch-your-first-job/#run-a-simple-mpi-script-on-3-nodes-using-custom-ec2-instance-type","text":"This job will use a 3 c5.18xlarge instances #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=3,instance_type=c5.18xlarge ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/soca/ $SOCA_CONFIGURATION /openmpi/4.1.1/lib/ export PATH = $PATH :/apps/soca/ $SOCA_CONFIGURATION /openmpi/4.1.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 3 hosts /apps/soca/ $SOCA_CONFIGURATION /openmpi/4.1.1/bin/mpirun --hostfile mpi_nodes -np 108 script.sh > my_output.log","title":"Run a simple MPI script on 3 nodes using custom EC2 instance type"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-3-nodes-using-custom-license-restriction","text":"This job will only start if we have at least 4 Comsol Acoustic licenses available #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=3,instance_type=c5.18xlarge,comsol_lic_acoustic=4 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 3 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 108 script.sh > my_output.log","title":"Run a simple script on 3 nodes using custom License Restriction"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-5-nodes-using-custom-ami","text":"This job will use a user-specified AMI ID #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5.18xlarge,instance_ami=ami-123abcde ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log","title":"Run a simple script on 5 nodes using custom AMI"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-5-nodes-using-custom-ami-using-a-different-os","text":"This job will use a user-specified AMI ID which use a operating system different than the scheduler #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5.18xlarge,instance_ami=ami-123abcde,base_os=centos7 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5.18xlarge is 36 cores so -np is 36 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log","title":"Run a simple script on 5 nodes using custom AMI using a different OS"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-5-m524xlarge-spot-instances-as-long-as-instance-price-is-lower-than-25-per-hour","text":"This job will use SPOT instances. Instances will be automatically terminated if BID price is higher than $2.5 / per hour per instance #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=m5.24xlarge,spot_price=2.5 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # m5.24xlarge is 48 cores so -np is 48 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 240 script.sh > my_output.log","title":"Run a simple script on 5 m5.24xlarge SPOT instances as long as instance price is lower than $2.5 per hour"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-5-m524xlarge-spot-instances-as-long-as-instance-price-is-lower-than-od-price","text":"#!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=m5.24xlarge,spot_price=auto ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # m5.24xlarge is 48 cores so -np is 48 * 5 hosts /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 240 script.sh > my_output.log","title":"Run a simple script on 5 m5.24xlarge SPOT instances as long as instance price is lower than OD price"},{"location":"tutorials/launch-your-first-job/#submit-a-job-with-efa","text":"Make sure to use an instance type supported by EFA https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/efa.html#efa-instance-types #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5n.18xlarge,efa_support=true ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/openmpi/4.0.1/lib/ export PATH=$PATH:/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log","title":"Submit a job with EFA"},{"location":"tutorials/launch-your-first-job/#use-50-c5xlarge-for-your-job-and-fallback-to-m5xlarge-and-r5xlarge-if-capacity-is-not-available","text":"AWS honors the instance order, so it will try to provision 50 c5.large first and fallback to m5.xlarge/r5.xlarge if needed (in case your account has instance limitation or AWS can't allocate more than X instance type on a given AZ/region). Ultimately, you may end up with the following configuration (but not limited to): 50 c5.xlarge 30 c5.xlarge, 20 m5.xlarge 20 c5.xlarge, 20 m5.xlarge, 10 r5.xlarge Or any other combination. The only certain know is that you will get 50 instances #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=50,instance_type=c5.xlarge+m5.xlarge+r5.xlarge ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/openmpi/4.0.1/lib/ export PATH=$PATH:/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log","title":"Use 50 c5.xlarge for your job and fallback to m5.xlarge and r5.xlarge if capacity is not available"},{"location":"tutorials/launch-your-first-job/#use-multiple-spot-instance-type","text":"#!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=5,instance_type=c5.xlarge+m5.xlarge+r5.xlarge, spot_price=auto ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/openmpi/4.0.1/lib/ export PATH=$PATH:/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log","title":"Use multiple SPOT instance type"},{"location":"tutorials/launch-your-first-job/#provision-50-instances-10-on-demand-and-40-spot","text":"#!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal #PBS -l nodes=50,instance_type=c5.large,spot_allocation_count=40 ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/apps/openmpi/4.0.1/lib/ export PATH=$PATH:/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log","title":"Provision 50 instances (10 On-Demand and 40 SPOT)"},{"location":"tutorials/launch-your-first-job/#multi-lines-parameters","text":"Custom AMI running on a different distribution than the scheduler, with EFA enable, without placement group and within a specific subnet_id #!/bin/bash #PBS -N my_job_name #PBS -V -j oe -o my_job_name.qlog #PBS -P project_a #PBS -q normal ## Resources can be specified on multiple lines #PBS -l nodes=5,instance_type=c5n.18xlarge,efa_support=yes #PBS -l placement_group=false,base_os=rhel7,ami_id=ami-12345,subnet_id=sub-abcde ## END PBS SETTINGS cd $PBS_O_WORKDIR cat $PBS_NODEFILE | sort | uniq > mpi_nodes export LD_LIBRARY_PATH = $LD_LIBRARY_PATH :/apps/openmpi/4.0.1/lib/ export PATH = $PATH :/apps/openmpi/4.0.1/bin/ # c5n.18xlarge is 36 cores so -np is 36 * 5 /apps/openmpi/4.0.1/bin/mpirun --hostfile mpi_nodes -np 180 script.sh > my_output.log","title":"Multi-lines parameters"},{"location":"tutorials/launch-your-first-job/#examples-for-job-shared-queue","text":"","title":"Examples for job-shared queue"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-96-vcpus-using-on-demand-c54xlarge-instances-on-job-shared-queue","text":"#!/bin/bash for i in { 1 ..96 } ; do qsub -q job-shared -l instance_type = c5.4xlarge -l ht_support = true -- /path/to/script.sh done Since we specified instance_type=c5.4xlarge and ht_support=true , the number of instances required would be calculated as 96 vCPUs (total number of queued jobs is 96 and each job requires 1 vCPU) divided by 16 vCPUs provided by each c5.4xlarge instance. So, these queued jobs would lead to provisioning of 6 on-demand c5.4xlarge instances","title":"Run a simple script on 96 vCPUs using on-demand c5.4xlarge instances on 'job-shared' queue"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-96-cores-using-on-demand-c54xlarge-instances-on-job-shared-queue","text":"#!/bin/bash for i in { 1 ..96 } ; do qsub -q job-shared -l instance_type = c5.4xlarge -l ht_support = false -- /path/to/script.sh done Since we specified instance_type=c5.4xlarge and ht_support=false , the number of instances required would be calculated as 96 cores (total number of queued jobs is 96 and each job requires 1 core) divided by 8 cores provided by each c5.4xlarge instance. So, these queued jobs would lead to provisioning of 12 on-demand c5.4xlarge instances","title":"Run a simple script on 96 cores using on-demand c5.4xlarge instances on 'job-shared' queue"},{"location":"tutorials/launch-your-first-job/#run-a-simple-script-on-96-vcpus-using-spot-fleet-with-c54xlarge-or-c59xlarge-on-job-shared-queue","text":"#!/bin/bash for i in { 1 ..96 } ; do qsub -q job-shared -l instance_type = c5.4xlarge+c5.9xlarge -l ht_support = true -l spot_price = auto -- /path/to/script.sh done Since we specified instance_type=c5.4xlarge+c5.9xlarge , and spot_price=auto , this will create a spot fleet request with two instance types c5.4xlarge and c5.9xlarge and the total required capacity would be 96. Weighted Capacity for each instance type would be automatically calculated for c5.4xlarge and c5.9xlarge based on the value of ht_support . In this case the weighted capacity for c5.4xlarge would be 16 and the weighted capacity for c5.9xlarge would be 36. The Spot fleet would then create a corresponding number of instances depending on instance availability.","title":"Run a simple script on 96 vCPUs using Spot Fleet with c5.4xlarge or c5.9xlarge on 'job-shared' queue"},{"location":"tutorials/launch-your-first-job/#run-a-script-that-requires-4-cores-24-times-using-spot-fleet-with-c54xlarge-or-c59xlarge-on-job-shared-queue","text":"#!/bin/bash for i in { 1 ..24 } ; do qsub -q job-shared -l instance_type = c5.4xlarge+c5.9xlarge -l ht_support = false -l select = 1 :ncpus = 4 -l spot_price = auto -- /path/to/four_core_script.sh done Since we specified instance_type=c5.4xlarge+c5.9xlarge , and spot_price=auto , this will create a spot fleet request with two instance types c5.4xlarge and c5.9xlarge and the total required capacity would be 96 (24 jobs each requires 4 cores as ht_support is false). Weighted Capacity for each instance type would be automatically calculated for c5.4xlarge and c5.9xlarge based on the value of ht_support . In this case the weighted capacity for c5.4xlarge would be 8 and the weighted capacity for c5.9xlarge would be 18. The Spot fleet would then create a corresponding number of instances depending on instance availability.","title":"Run a script that requires 4 cores 24 times using Spot Fleet with c5.4xlarge or c5.9xlarge on 'job-shared' queue"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/","text":"By default, SOCA provision a vanilla AMI and install all required packages in ~3 to 5 minutes. If this cold time is not acceptable for your workload, you can launch AlwaysOn instance or pre-bake your AMI with all required libraries. Step 1: Locate your base AMI \u00b6 Run cat /etc/environment | grep SOCA_INSTALL_AMI on your scheduler host $ ssh -i <key> ec2-user@<ip> Last login: Wed Oct 2 20 :06:47 2019 from <ip> _____ ____ ______ ___ / ___/ / __ \\ / ____// | \\_ _ \\ / / / // / / / | | ___/ // /_/ // /___ / ___ | /____/ \\_ ___/ \\_ ___//_/ | _ | Cluster: soca-uiupdates > source /etc/environment to SOCA paths [ ec2-user@ip-30-0-1-28 ~ ] $ cat /etc/environment | grep SOCA_INSTALL_AMI export SOCA_INSTALL_AMI = ami-082b5a644766e0e6f [ ec2-user@ip-30-0-1-28 ~ ] $ Step 2: Launch a temporary EC2 instance \u00b6 Launch a new EC2 instance using the SOCA_INSTALL_AMI image Step 3: Pre-configure your AMI \u00b6 Important Step 3 is only required if you want to reduce the time required for your compute node to boot. You can skip this section if you just want to install your customization on your AMI and let SOCA handles PBS/Gnome/System packages installation. 3.1 Pre-Install system packages \u00b6 You can pre-install the packages listed on https://github.com/awslabs/scale-out-computing-on-aws/blob/main/source/scripts/config.cfg . You will need to run yum install for: SYSTEM_PKGS SCHEDULER_PKGS OPENLDAP_SERVER_PKGS SSSD_PKGS Easy Install Copy the content of the config.cfg on your filesystem (say /root/config.cfg ) Run source /root/config.cfg Run the following commands: yum install -y $(echo ${SYSTEM_PKGS[*]}) yum install -y $(echo ${SCHEDULER_PKGS[*]}) yum install -y $(echo ${OPENLDAP_SERVER_PKGS[*]}) yum install -y $(echo ${SSSD_PKGS[*]}) Here is an example of how you can install packages listed in an array in bash. 3.2: Pre-Install the scheduler \u00b6 To reduce the launch time of your EC2 instance, it's recommended to pre-install OpenPBS. First, refer to https://github.com/awslabs/scale-out-computing-on-aws/blob/main/source/scripts/config.cfg and note all OpenPBS related variables as you will need to use them below (see highlighted lines): # Sudo as Root sudo su - # Define OpenPBS variable export OPENPBS_URL = <variable_from_config.txt> # ex https://github.com/openpbs/openpbs/archive/v20.0.1.tar.gz export OPENPBS_TGZ = <variable_from_config.txt> # ex v20.0.1.tar.gz export OPENPBS_VERSION = <variable_from_config.txt> # ex 20.0.1 # Run the following command to install OpenPBS cd ~ wget $OPENPBS_URL tar zxvf $OPENPBS_TGZ cd openpbs- $OPENPBS_VERSION ./autogen.sh ./configure --prefix = /opt/pbs make -j6 make install -j6 /opt/pbs/libexec/pbs_postinstall chmod 4755 /opt/pbs/sbin/pbs_iff /opt/pbs/sbin/pbs_rcp systemctl disable pbs Installation Path Make sure to install OpenPBS under /opt/pbs 3.3: (Optional) Pre-Install Gnome \u00b6 If you are using RHEL , run yum groupinstall \"Server with GUI\" -y If you are using Centos , run yum groupinstall \"GNOME Desktop\" -y If you are using Amazon Linux , run yum install -y $(echo ${DCV_AMAZONLINUX_PKGS[*]}) 3.4: Reboot your EC2 machine \u00b6 3.5: Make sure you do not have any libvirt of firewalld/iptables \u00b6 Post reboot, some distribution may automatically start libvirt or firewall. If that's the case you must delete them otherwise PBS won't be able to contact the scheduler. To find if you have a running libvirt, run ifconfig and check if you have virbr0 interface such as: ifconfig ens5: flags = 4163 <UP,BROADCAST,RUNNING,MULTICAST> mtu 9001 inet 10 .10.2.19 netmask 255 .255.255.0 broadcast 10 .10.2.255 inet6 fe80::8b1:6aff:fe8a:5ad8 prefixlen 64 scopeid 0x20<link> ether 0a:b1:6a:8a:5a:d8 txqueuelen 1000 ( Ethernet ) RX packets 81 bytes 11842 ( 11 .5 KiB ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 92 bytes 12853 ( 12 .5 KiB ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags = 73 <UP,LOOPBACK,RUNNING> mtu 65536 inet 127 .0.0.1 netmask 255 .0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10<host> loop txqueuelen 1000 ( Local Loopback ) RX packets 8 bytes 601 ( 601 .0 B ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 8 bytes 601 ( 601 .0 B ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 virbr0: flags = 4099 <UP,BROADCAST,MULTICAST> mtu 1500 inet 192 .168.122.1 netmask 255 .255.255.0 broadcast 192 .168.122.255 ether 52 :54:00:ea:5a:b9 txqueuelen 1000 ( Ethernet ) RX packets 0 bytes 0 ( 0 .0 B ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 ( 0 .0 B ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 If that's the case, disable libvirt by running /bin/systemctl disable libvirtd.service ip link set virbr0 down brctl delbr virbr0 Then, make sure you do not have iptables ( iptables -L ) running. If needed, disable firewalld by running /bin/systemctl disable firewalld 3.6: Create soca_preinstalled_packages.log \u00b6 Create an empty file located at /root/soca_preinstalled_packages.log for example, run touch /root/soca_preinstalled_packages.log . This would enable compute node initialization scripts to skip the installation of all the required packages since these are now pre-installed in the AMI. Step 4: Create your AMI \u00b6 Once you are done, go back to EC2 console, locate your instance and click \"Actions > Image > Create Image\" Choose an AMI name and click 'Create Image'. Your AMI is now being created. Please note it may take a couple of minutes for the AMI to be ready. To check the status, go to EC2 Console and then click \"AMIs\" on the left sidebar Stop your temporary EC2 instance Once your AMI has been created, you can safely terminate the EC2 instance you just launched as you won't need it anymore. Step 5: Test your new AMI \u00b6 # Test 1: Submit a job with a vanilla AMI $ qsub -l instance_type = c5.9xlarge -- /bin/date # Test 2: Submit a job with a pre-configured AMI $ qsub -l instance_type = c5.9xlarge -l instance_ami = ami-0e05219e578020c64 -- /bin/date Results: Test1 (Vanilla): 3 minutes 45 seconds to provision EC2 capacity, register node on SOCA and start the job Test2 (Pre-Configured): 1 minute 44 seconds to provision EC2 capacity, register host on SOCA and start the job Step 6: Update default AMI (Optional) \u00b6 Single job \u00b6 As you are planning to use a custom AMI, you will be required to specify -l instance_ami=<IMAGE_ID> at job submission. It's recommended to go with the \"Entire Queue\" option below if you do not want to manually specify this resource each time you submit a job Entire queue \u00b6 Edit /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml and update the default AMI queue_type : compute : queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] instance_ami : \"<YOUR_AMI_ID>\" # <- Add your new AMI instance_type : ... Any jobs running in the queue configured on the queue_mapping will now use your pre-configured AMI by default. You do not need to specify -l instance_ami at job submission anymore. Entire cluster \u00b6 If you want to change the default AMI to use regardless of queue/job, open your Secret Manager console and select your Scale-Out Computing on AWS cluster configuration. Click \u201cRetrieve Secret Value\u201d and then \u201cEdit\u201d. Find the entry \u201cCustomAMI\u201d and update the value with your new AMI ID then click Save","title":"Import custom AMI to provision capacity faster"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#step-1-locate-your-base-ami","text":"Run cat /etc/environment | grep SOCA_INSTALL_AMI on your scheduler host $ ssh -i <key> ec2-user@<ip> Last login: Wed Oct 2 20 :06:47 2019 from <ip> _____ ____ ______ ___ / ___/ / __ \\ / ____// | \\_ _ \\ / / / // / / / | | ___/ // /_/ // /___ / ___ | /____/ \\_ ___/ \\_ ___//_/ | _ | Cluster: soca-uiupdates > source /etc/environment to SOCA paths [ ec2-user@ip-30-0-1-28 ~ ] $ cat /etc/environment | grep SOCA_INSTALL_AMI export SOCA_INSTALL_AMI = ami-082b5a644766e0e6f [ ec2-user@ip-30-0-1-28 ~ ] $","title":"Step 1: Locate your base AMI"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#step-2-launch-a-temporary-ec2-instance","text":"Launch a new EC2 instance using the SOCA_INSTALL_AMI image","title":"Step 2: Launch a temporary EC2 instance"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#step-3-pre-configure-your-ami","text":"Important Step 3 is only required if you want to reduce the time required for your compute node to boot. You can skip this section if you just want to install your customization on your AMI and let SOCA handles PBS/Gnome/System packages installation.","title":"Step 3: Pre-configure your AMI"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#31-pre-install-system-packages","text":"You can pre-install the packages listed on https://github.com/awslabs/scale-out-computing-on-aws/blob/main/source/scripts/config.cfg . You will need to run yum install for: SYSTEM_PKGS SCHEDULER_PKGS OPENLDAP_SERVER_PKGS SSSD_PKGS Easy Install Copy the content of the config.cfg on your filesystem (say /root/config.cfg ) Run source /root/config.cfg Run the following commands: yum install -y $(echo ${SYSTEM_PKGS[*]}) yum install -y $(echo ${SCHEDULER_PKGS[*]}) yum install -y $(echo ${OPENLDAP_SERVER_PKGS[*]}) yum install -y $(echo ${SSSD_PKGS[*]}) Here is an example of how you can install packages listed in an array in bash.","title":"3.1 Pre-Install system packages"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#32-pre-install-the-scheduler","text":"To reduce the launch time of your EC2 instance, it's recommended to pre-install OpenPBS. First, refer to https://github.com/awslabs/scale-out-computing-on-aws/blob/main/source/scripts/config.cfg and note all OpenPBS related variables as you will need to use them below (see highlighted lines): # Sudo as Root sudo su - # Define OpenPBS variable export OPENPBS_URL = <variable_from_config.txt> # ex https://github.com/openpbs/openpbs/archive/v20.0.1.tar.gz export OPENPBS_TGZ = <variable_from_config.txt> # ex v20.0.1.tar.gz export OPENPBS_VERSION = <variable_from_config.txt> # ex 20.0.1 # Run the following command to install OpenPBS cd ~ wget $OPENPBS_URL tar zxvf $OPENPBS_TGZ cd openpbs- $OPENPBS_VERSION ./autogen.sh ./configure --prefix = /opt/pbs make -j6 make install -j6 /opt/pbs/libexec/pbs_postinstall chmod 4755 /opt/pbs/sbin/pbs_iff /opt/pbs/sbin/pbs_rcp systemctl disable pbs Installation Path Make sure to install OpenPBS under /opt/pbs","title":"3.2: Pre-Install the scheduler"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#33-optional-pre-install-gnome","text":"If you are using RHEL , run yum groupinstall \"Server with GUI\" -y If you are using Centos , run yum groupinstall \"GNOME Desktop\" -y If you are using Amazon Linux , run yum install -y $(echo ${DCV_AMAZONLINUX_PKGS[*]})","title":"3.3: (Optional) Pre-Install Gnome"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#34-reboot-your-ec2-machine","text":"","title":"3.4: Reboot your EC2 machine"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#35-make-sure-you-do-not-have-any-libvirt-of-firewalldiptables","text":"Post reboot, some distribution may automatically start libvirt or firewall. If that's the case you must delete them otherwise PBS won't be able to contact the scheduler. To find if you have a running libvirt, run ifconfig and check if you have virbr0 interface such as: ifconfig ens5: flags = 4163 <UP,BROADCAST,RUNNING,MULTICAST> mtu 9001 inet 10 .10.2.19 netmask 255 .255.255.0 broadcast 10 .10.2.255 inet6 fe80::8b1:6aff:fe8a:5ad8 prefixlen 64 scopeid 0x20<link> ether 0a:b1:6a:8a:5a:d8 txqueuelen 1000 ( Ethernet ) RX packets 81 bytes 11842 ( 11 .5 KiB ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 92 bytes 12853 ( 12 .5 KiB ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 lo: flags = 73 <UP,LOOPBACK,RUNNING> mtu 65536 inet 127 .0.0.1 netmask 255 .0.0.0 inet6 ::1 prefixlen 128 scopeid 0x10<host> loop txqueuelen 1000 ( Local Loopback ) RX packets 8 bytes 601 ( 601 .0 B ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 8 bytes 601 ( 601 .0 B ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 virbr0: flags = 4099 <UP,BROADCAST,MULTICAST> mtu 1500 inet 192 .168.122.1 netmask 255 .255.255.0 broadcast 192 .168.122.255 ether 52 :54:00:ea:5a:b9 txqueuelen 1000 ( Ethernet ) RX packets 0 bytes 0 ( 0 .0 B ) RX errors 0 dropped 0 overruns 0 frame 0 TX packets 0 bytes 0 ( 0 .0 B ) TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 If that's the case, disable libvirt by running /bin/systemctl disable libvirtd.service ip link set virbr0 down brctl delbr virbr0 Then, make sure you do not have iptables ( iptables -L ) running. If needed, disable firewalld by running /bin/systemctl disable firewalld","title":"3.5: Make sure you do not have any libvirt of firewalld/iptables"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#36-create-soca_preinstalled_packageslog","text":"Create an empty file located at /root/soca_preinstalled_packages.log for example, run touch /root/soca_preinstalled_packages.log . This would enable compute node initialization scripts to skip the installation of all the required packages since these are now pre-installed in the AMI.","title":"3.6: Create soca_preinstalled_packages.log"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#step-4-create-your-ami","text":"Once you are done, go back to EC2 console, locate your instance and click \"Actions > Image > Create Image\" Choose an AMI name and click 'Create Image'. Your AMI is now being created. Please note it may take a couple of minutes for the AMI to be ready. To check the status, go to EC2 Console and then click \"AMIs\" on the left sidebar Stop your temporary EC2 instance Once your AMI has been created, you can safely terminate the EC2 instance you just launched as you won't need it anymore.","title":"Step 4: Create your AMI"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#step-5-test-your-new-ami","text":"# Test 1: Submit a job with a vanilla AMI $ qsub -l instance_type = c5.9xlarge -- /bin/date # Test 2: Submit a job with a pre-configured AMI $ qsub -l instance_type = c5.9xlarge -l instance_ami = ami-0e05219e578020c64 -- /bin/date Results: Test1 (Vanilla): 3 minutes 45 seconds to provision EC2 capacity, register node on SOCA and start the job Test2 (Pre-Configured): 1 minute 44 seconds to provision EC2 capacity, register host on SOCA and start the job","title":"Step 5: Test your new AMI"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#step-6-update-default-ami-optional","text":"","title":"Step 6: Update default AMI (Optional)"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#single-job","text":"As you are planning to use a custom AMI, you will be required to specify -l instance_ami=<IMAGE_ID> at job submission. It's recommended to go with the \"Entire Queue\" option below if you do not want to manually specify this resource each time you submit a job","title":"Single job"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#entire-queue","text":"Edit /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml and update the default AMI queue_type : compute : queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] instance_ami : \"<YOUR_AMI_ID>\" # <- Add your new AMI instance_type : ... Any jobs running in the queue configured on the queue_mapping will now use your pre-configured AMI by default. You do not need to specify -l instance_ami at job submission anymore.","title":"Entire queue"},{"location":"tutorials/reduce-compute-node-launch-time-with-custom-ami/#entire-cluster","text":"If you want to change the default AMI to use regardless of queue/job, open your Secret Manager console and select your Scale-Out Computing on AWS cluster configuration. Click \u201cRetrieve Secret Value\u201d and then \u201cEdit\u201d. Find the entry \u201cCustomAMI\u201d and update the value with your new AMI ID then click Save","title":"Entire cluster"},{"location":"web-interface/","text":"About \u00b6 Web User Interface \u00b6 Scale-Out Computing on AWS includes a simple web ui designed to simplify user interactions such as: Start/Stop virtual desktops (Windows/Linux) sessions in 1 click Download private key in both PEM or PPK format Check the queue and job status in real-time Add/Remove LDAP users Access the analytic dashboard Access your filesystem Create Application profiles and let your users submit job directly via the web interface Understand why your jobs are stuck in the queue And more .. (refer to the left sidebar for additional resources) HTTP Rest API \u00b6 Users can submit/retrieve/delete jobs remotely via an HTTP REST API","title":"About"},{"location":"web-interface/#about","text":"","title":"About"},{"location":"web-interface/#web-user-interface","text":"Scale-Out Computing on AWS includes a simple web ui designed to simplify user interactions such as: Start/Stop virtual desktops (Windows/Linux) sessions in 1 click Download private key in both PEM or PPK format Check the queue and job status in real-time Add/Remove LDAP users Access the analytic dashboard Access your filesystem Create Application profiles and let your users submit job directly via the web interface Understand why your jobs are stuck in the queue And more .. (refer to the left sidebar for additional resources)","title":"Web User Interface"},{"location":"web-interface/#http-rest-api","text":"Users can submit/retrieve/delete jobs remotely via an HTTP REST API","title":"HTTP Rest API"},{"location":"web-interface/control-hpc-job-with-http-web-rest-api/","text":"API Documentation You can interact with your SOCA cluster (create users, groups, queue, submit jobs, view jobs etc ..) via a simple REST interface. Documentation and examples can be found on https://your_soca_url/api/doc (note: Your SOCA must have a valid SSL certificate Submit a job via web In addition of REST API, SOCA also support job management via a web based interface. Click here to learn more Step1: Retrieve your API key \u00b6 To retrieve your API key, navigate to \"My API Key\" section on the left sidebar. Important Your API key is unique and linked to your account. Do not share it with anyone. If you think your key has been compromised, reset it immediately via the link on the same page Step2: Prepare your job input \u00b6 For this example, let's assume we want to submit a simple \"Hello World\" job as shown below: #!/bin/bash #PBS -N testjob #PBS -V -j oe -o testjob_output.qlog #PBS -P myproject #PBS -q normal #PBS -l nodes=1,instance_type=c5.large /bin/echo \"Hello World\" To be able to send this job to SOCA via HTTP, you must first encode this file using base64. There are multiple ways to create a base64 hash, on Linux/Mac, the easiest method is to use base64 encode function: $ base64 job_submit.sh IyEvYmluL2Jhc2gKI1BCUyAtTiB0ZXN0am9iCiNQQlMgLVYgLWogb2UgLW8gdGVzdGpvYl9vdXRwdXQucWxvZwojUEJTIC1QIG15cHJvamVjdAojUEJTIC1xIG5vcm1hbAojUEJTIC1sIG5vZGVzPTEsaW5zdGFuY2VfdHlwZT1jNS5sYXJnZQovYmluL2VjaG8gIkhlbGxvIFdvcmxkIgo= You can verify the hash is correct by running the base64 decode function (this should return your original input file) $ echo \"IyEvYmluL2Jhc2gKI1BCUyAtTiB0ZXN0am9iCiNQQlMgLVYgLWogb2UgLW8gdGVzdGpvYl9vdXRwdXQucWxvZwojUEJTIC1QIG15cHJvamVjdAojUEJTIC1xIG5vcm1hbAojUEJTIC1sIG5vZGVzPTEsaW5zdGFuY2VfdHlwZT1jNS5sYXJnZQovYmluL2VjaG8gIkhlbGxvIFdvcmxkIgo\" | base64 --decode #!/bin/bash #PBS -N testjob #PBS -V -j oe -o testjob_output.qlog #PBS -P myproject #PBS -q normal #PBS -l nodes=1,instance_type=c5.large /bin/echo \"Hello World\" Step3: Send the job to SOCA \u00b6 To be able to submit the POST request, you will need to specify three headers: X-SOCA-USER and set the value to <YOUR_SOCA_USER> X-SOCA-TOKEN and set the value to <YOUR_SOCA_TOKEN> Content-Type and set the value to multipart/form-data Once you have your headers configured, submit a HTTP/POST request and pass your hash as form data via payload parameter: curl -X POST \\ https://<YOUR_SOCA_URL>/api/scheduler/job \\ -H 'X-SOCA-TOKEN: <YOUR_SOCA_TOKEN>' \\ -H 'X-SOCA-USER: <YOUR_SOCA_USER>' \\ -F payload = IyEvYmluL2Jhc2gKI1BCUyAtTiB0ZXN0am9iCiNQQlMgLVYgLWogb2UgLW8gdGVzdGpvYl9vdXRwdXQucWxvZwojUEJTIC1QIG15cHJvamVjdAojUEJTIC1xIG5vcm1hbAojUEJTIC1sIG5vZGVzPTEsaW5zdGFuY2VfdHlwZT1jNS5sYXJnZQovYmluL2VjaG8gIkhlbGxvIFdvcmxkIgo = If your hash is a valid PBS job file, SOCA will return the job id associated to your request (11313 in this example) { \"success\" : true , \"message\" : \"11313\" } Step4: Get job info \u00b6 To retrieve information about a given job (assuming you are the job owner), simply submit a HTTP/GET request and pass job_id as parameter curl -X GET \\ https://<YOUR_SOCA_URL>/api/scheduler/job?job_id = 11313 \\ -H 'X-SOCA-TOKEN: <YOUR_SOCA_TOKEN>' \\ -H 'X-SOCA-USER: <YOUR_SOCA_USER>' This command will return a JSON object with all information regarding your job. { \"success\" : true , \"message\" : { \"Job_Name\" : \"testjob\" , \"Job_Owner\" : \"<YOUR_SOCA_USER>@ip-10-10-0-75.us-west-2.compute.internal\" , \"job_state\" : \"Q\" , \"queue\" : \"normal\" , \"server\" : \"ip-10-10-0-75\" , \"Checkpoint\" : \"u\" , \"ctime\" : \"Thu May 21 02:47:05 2020\" , \"Error_Path\" : \"ip-10-10-0-75.us-west-2.compute.internal:/data/home/<YOUR_SOCA_USER>/soca_job_output/testjob_pGep6UiWpK/testjob.e11313\" , \"Hold_Types\" : \"n\" .... } } Step4: Delete job \u00b6 To delete a job, simply submit a HTTP/DELETE request and specify job_id parameter: curl -X DELETE \\ https://<YOUR_SOCA_URL>/api/scheduler/job?job_id = 11313 \\ -H 'X-SOCA-TOKEN: <YOUR_SOCA_TOKEN>' \\ -H 'X-SOCA-USER: <YOUR_SOCA_USER>' If the command is valid, you will receive a validation message: { \"success\" : true , \"message\" : \"Job deleted\" } You can verify the Job has been removed from the queue using the same HTTP/GET request: curl -X GET \\ https://<YOUR_SOCA_URL>/api/scheduler/job?job_id = 11313 \\ -H 'X-SOCA-TOKEN: <YOUR_SOCA_TOKEN>' \\ -H 'X-SOCA-USER: <YOUR_SOCA_USER>' This time the output will return an error: { \"success\" : false , \"message\" : \"Unable to retrieve Job ID (job may have terminated and is no longer in the queue)\" }","title":"Control your job with HTTP/REST API"},{"location":"web-interface/control-hpc-job-with-http-web-rest-api/#step1-retrieve-your-api-key","text":"To retrieve your API key, navigate to \"My API Key\" section on the left sidebar. Important Your API key is unique and linked to your account. Do not share it with anyone. If you think your key has been compromised, reset it immediately via the link on the same page","title":"Step1: Retrieve your API key"},{"location":"web-interface/control-hpc-job-with-http-web-rest-api/#step2-prepare-your-job-input","text":"For this example, let's assume we want to submit a simple \"Hello World\" job as shown below: #!/bin/bash #PBS -N testjob #PBS -V -j oe -o testjob_output.qlog #PBS -P myproject #PBS -q normal #PBS -l nodes=1,instance_type=c5.large /bin/echo \"Hello World\" To be able to send this job to SOCA via HTTP, you must first encode this file using base64. There are multiple ways to create a base64 hash, on Linux/Mac, the easiest method is to use base64 encode function: $ base64 job_submit.sh IyEvYmluL2Jhc2gKI1BCUyAtTiB0ZXN0am9iCiNQQlMgLVYgLWogb2UgLW8gdGVzdGpvYl9vdXRwdXQucWxvZwojUEJTIC1QIG15cHJvamVjdAojUEJTIC1xIG5vcm1hbAojUEJTIC1sIG5vZGVzPTEsaW5zdGFuY2VfdHlwZT1jNS5sYXJnZQovYmluL2VjaG8gIkhlbGxvIFdvcmxkIgo= You can verify the hash is correct by running the base64 decode function (this should return your original input file) $ echo \"IyEvYmluL2Jhc2gKI1BCUyAtTiB0ZXN0am9iCiNQQlMgLVYgLWogb2UgLW8gdGVzdGpvYl9vdXRwdXQucWxvZwojUEJTIC1QIG15cHJvamVjdAojUEJTIC1xIG5vcm1hbAojUEJTIC1sIG5vZGVzPTEsaW5zdGFuY2VfdHlwZT1jNS5sYXJnZQovYmluL2VjaG8gIkhlbGxvIFdvcmxkIgo\" | base64 --decode #!/bin/bash #PBS -N testjob #PBS -V -j oe -o testjob_output.qlog #PBS -P myproject #PBS -q normal #PBS -l nodes=1,instance_type=c5.large /bin/echo \"Hello World\"","title":"Step2: Prepare your job input"},{"location":"web-interface/control-hpc-job-with-http-web-rest-api/#step3-send-the-job-to-soca","text":"To be able to submit the POST request, you will need to specify three headers: X-SOCA-USER and set the value to <YOUR_SOCA_USER> X-SOCA-TOKEN and set the value to <YOUR_SOCA_TOKEN> Content-Type and set the value to multipart/form-data Once you have your headers configured, submit a HTTP/POST request and pass your hash as form data via payload parameter: curl -X POST \\ https://<YOUR_SOCA_URL>/api/scheduler/job \\ -H 'X-SOCA-TOKEN: <YOUR_SOCA_TOKEN>' \\ -H 'X-SOCA-USER: <YOUR_SOCA_USER>' \\ -F payload = IyEvYmluL2Jhc2gKI1BCUyAtTiB0ZXN0am9iCiNQQlMgLVYgLWogb2UgLW8gdGVzdGpvYl9vdXRwdXQucWxvZwojUEJTIC1QIG15cHJvamVjdAojUEJTIC1xIG5vcm1hbAojUEJTIC1sIG5vZGVzPTEsaW5zdGFuY2VfdHlwZT1jNS5sYXJnZQovYmluL2VjaG8gIkhlbGxvIFdvcmxkIgo = If your hash is a valid PBS job file, SOCA will return the job id associated to your request (11313 in this example) { \"success\" : true , \"message\" : \"11313\" }","title":"Step3: Send the job to SOCA"},{"location":"web-interface/control-hpc-job-with-http-web-rest-api/#step4-get-job-info","text":"To retrieve information about a given job (assuming you are the job owner), simply submit a HTTP/GET request and pass job_id as parameter curl -X GET \\ https://<YOUR_SOCA_URL>/api/scheduler/job?job_id = 11313 \\ -H 'X-SOCA-TOKEN: <YOUR_SOCA_TOKEN>' \\ -H 'X-SOCA-USER: <YOUR_SOCA_USER>' This command will return a JSON object with all information regarding your job. { \"success\" : true , \"message\" : { \"Job_Name\" : \"testjob\" , \"Job_Owner\" : \"<YOUR_SOCA_USER>@ip-10-10-0-75.us-west-2.compute.internal\" , \"job_state\" : \"Q\" , \"queue\" : \"normal\" , \"server\" : \"ip-10-10-0-75\" , \"Checkpoint\" : \"u\" , \"ctime\" : \"Thu May 21 02:47:05 2020\" , \"Error_Path\" : \"ip-10-10-0-75.us-west-2.compute.internal:/data/home/<YOUR_SOCA_USER>/soca_job_output/testjob_pGep6UiWpK/testjob.e11313\" , \"Hold_Types\" : \"n\" .... } }","title":"Step4: Get job info"},{"location":"web-interface/control-hpc-job-with-http-web-rest-api/#step4-delete-job","text":"To delete a job, simply submit a HTTP/DELETE request and specify job_id parameter: curl -X DELETE \\ https://<YOUR_SOCA_URL>/api/scheduler/job?job_id = 11313 \\ -H 'X-SOCA-TOKEN: <YOUR_SOCA_TOKEN>' \\ -H 'X-SOCA-USER: <YOUR_SOCA_USER>' If the command is valid, you will receive a validation message: { \"success\" : true , \"message\" : \"Job deleted\" } You can verify the Job has been removed from the queue using the same HTTP/GET request: curl -X GET \\ https://<YOUR_SOCA_URL>/api/scheduler/job?job_id = 11313 \\ -H 'X-SOCA-TOKEN: <YOUR_SOCA_TOKEN>' \\ -H 'X-SOCA-USER: <YOUR_SOCA_USER>' This time the output will return an error: { \"success\" : false , \"message\" : \"Unable to retrieve Job ID (job may have terminated and is no longer in the queue)\" }","title":"Step4: Delete job"},{"location":"web-interface/create-virtual-desktops-images/","text":"Feature in preview This feature is only available on beta builds By default, SOCA only provide base DCV image for Windows, meaning no applications are pre-installed. Customers can create their own bundle using Amazon Images (AMI) and let their user choose what type of software they want to see pre-installed on their graphical sessions Windows \u00b6 Important An image is a complete snapshot of your EC2 host. Make sure you do not have any confidential data hosted on it before creating the image First, launch a simple Windows graphical session and install some applications. In this example, I have installed Creo To prepare your image, you first need to configure your system. Click the Start button and search for Powershell On the Powershell terminal, execute C:\\ProgramData\\Amazon\\EC2-Windows\\Launch\\Scripts\\InitializeInstance.ps1 -Schedule Go back to the Windows session list and retrieve the instance ID associated to your session (mouse hover the question mark icon) On your EC2 console and select your instance Click \"Actions\" -> \"Image\" -> \"Create Image\" Choose a name and a description, make sure to check \"No Reboot\" then click Create Image Navigate to the AMI tab and verify if your image status is \"available\" My Image is taking forever to be created To check the progress of your image, navigate to \"Snapshots\" section and refer to the Progress column for all EBS volumes created by your image Info You will not be able to use your image until the status is available. Creating an AMI may take a couple of hours depending the size of the image AMI is region specific AMI has 3 syllabes and is pronounced \"Ay-Em-I\" . Once your AMI is ready, login to your SOCA web interface and go to \"AMI management\" under \"Admin\" section Fill out the form (specify the AMI ID, operating system, minimum storage requirement as well as pick a friendly label) What is root disk size? Root disk size is the minimum storage required for your AMI to boot up. In other words, if you created your AMI with 150GB root disk, then you won't be able to launch any instance based on this AMI unless the associated EBS disk is greater or equal than the base storage. SOCA will automatically honor the minimum storage required by the image if users choose a lower value. Once done, click 'Register AMI in SOCA', you will get a success message if everything is configured correctly Now go back to the \"Windows Session\" section, this time you should be able to see your newly create image under Software Stack You can now launch your session with a pre-configured image. Linux \u00b6 Refer to this article to learn how to create Linux images","title":"Create Virtual Desktop Images for Windows/Linux"},{"location":"web-interface/create-virtual-desktops-images/#windows","text":"Important An image is a complete snapshot of your EC2 host. Make sure you do not have any confidential data hosted on it before creating the image First, launch a simple Windows graphical session and install some applications. In this example, I have installed Creo To prepare your image, you first need to configure your system. Click the Start button and search for Powershell On the Powershell terminal, execute C:\\ProgramData\\Amazon\\EC2-Windows\\Launch\\Scripts\\InitializeInstance.ps1 -Schedule Go back to the Windows session list and retrieve the instance ID associated to your session (mouse hover the question mark icon) On your EC2 console and select your instance Click \"Actions\" -> \"Image\" -> \"Create Image\" Choose a name and a description, make sure to check \"No Reboot\" then click Create Image Navigate to the AMI tab and verify if your image status is \"available\" My Image is taking forever to be created To check the progress of your image, navigate to \"Snapshots\" section and refer to the Progress column for all EBS volumes created by your image Info You will not be able to use your image until the status is available. Creating an AMI may take a couple of hours depending the size of the image AMI is region specific AMI has 3 syllabes and is pronounced \"Ay-Em-I\" . Once your AMI is ready, login to your SOCA web interface and go to \"AMI management\" under \"Admin\" section Fill out the form (specify the AMI ID, operating system, minimum storage requirement as well as pick a friendly label) What is root disk size? Root disk size is the minimum storage required for your AMI to boot up. In other words, if you created your AMI with 150GB root disk, then you won't be able to launch any instance based on this AMI unless the associated EBS disk is greater or equal than the base storage. SOCA will automatically honor the minimum storage required by the image if users choose a lower value. Once done, click 'Register AMI in SOCA', you will get a success message if everything is configured correctly Now go back to the \"Windows Session\" section, this time you should be able to see your newly create image under Software Stack You can now launch your session with a pre-configured image.","title":"Windows"},{"location":"web-interface/create-virtual-desktops-images/#linux","text":"Refer to this article to learn how to create Linux images","title":"Linux"},{"location":"web-interface/create-virtual-desktops/","text":"Feature in preview This feature is only available on beta builds SOCA lets you deploy Windows and/or Linux desktops using NICE DCV technology. To get started, select either \"Linux Desktop\" or \"Windows Desktop\" in the left sidebar. Create your Windows/Linux desktop \u00b6 To launch your Windows/Linux desktop, you first have to pick a name, choose how much storage you want to allocate as well as specify other options such as hibernation support, compute type, software stack or even subnet id. Need help? Hover your mouse on the parameter name to display help section. Once you are ready, click \"Launch My Session\" button. Your session interface will be changed to pending state. Your virtual desktop will be ready within 10-15 minutes. Startup time is based on the image selected, the operating system as well as the instance type. Windows tends to boot faster as some components such as the scheduler or EFS are not installed. Nvidia drivers for GPU instances SOCA automatically install Nvidia GRID (G3/G4) drivers. Tesla drivers (P3) have to be installed separately by the customers. Once your virtual desktop is active, your session interface will display two ways to connect to it: You can either access your desktop directly via your browser (Option1) or use the NICE DCV native application (Option2). A link to download the application is provided on the website. For better performance, we recommend using the Option 2. Example of Linux desktop \u00b6 Example of Windows desktop \u00b6 Important Unlike Linux desktops, Windows desktops are not connected to the scheduler and you cannot submit jobs directly from your Windows system. To submit a job via Windows, you first need to access the web ui then submit a job via HTTP endpoint or via the HTTP REST API Stop/Hibernate your desktop \u00b6 What does hibernate means? When you hibernate an instance, your desktop state is saved in memory. When you restart it, all your applications will automatically resume. On the other hand, stopping a virtual desktop is the same as powering off your laptop. Note not all EC2 instances support hibernation. To stop/hibernate your session, click the \"Stop\" (or \"Hibernate\") button in the top bar. A popup will appear, asking you to confirm your action. Click \"Stop/Hibernate my session\" button to temporarily turn off your virtual desktop. Your desktop is now stopped (you are not charged for compute price while your instance is stopped). To restart your session simply click \"Restart your session\" button. Configure your desktop auto start/stop time \u00b6 To change the schedule of your desktop, click the \"Schedule\" button in the top bar. SOCA will honors your session schedule and automatically start/stop your desktop based on your own schedule. Desktop will be stopped outside of scheduled hours only if they are idle. You can configure three type of schedule for your desktop: Run all day (SOCA will ensures your desktop is up and running from 12 AM to 12 PM) Stopped all day (SOCA will ensures your desktop is stopped from 12 AM to 12 PM) Custom schedule (SOCA will honors your own schedule) Info If you manually start your desktop during off hours, your session will be up and running until it becomes idle. Idle time can be customized by cluster admins. Schedule and Timezone By default, SOCA is configured to use UTC timezone and this may be a problem if you are currently using a different timezone. To fix that, you can adjust your local timezone on config.py (then restart the web ui) Update your hardware requirements on the fly \u00b6 You can upgrade/downgrade the hardware of your desktop only when your session is stopped and if you have disabled hibernation. To do so, click Choose the new type of instance you want to resize your desktop to then click \"Change Instance Type\". Hardware upgrade/downgrade are instant. Your desktop will use the updated config as soon as you restart it. Terminate your desktop \u00b6 To terminate your desktop, click the \"Kill\" button from the menu bar. You will be prompted for a confirmation message. Terminate a session may cause data loss if you are using ephemeral storage, so make sure to have uploaded all your data back to SOCA filesystem first. (Windows only) Retrieve local admin password \u00b6 As of today, Windows desktops are not configured to OpenLDAP, meaning you will use a local Windows account unique to each session. To retrieve the password for the local Administrator account, click \"Get Password\" icon. This will open a new window where you can retrieve the password. Create custom Windows/Linux images \u00b6 SOCA images are Windows or Linux desktops with pre-installed applications. Refer to this page to learn more about images management How idle time is calculated? \u00b6 When you have enabled DCV_LINUX_HIBERNATE_IDLE_SESSION , DCV_LINUX_STOP_IDLE_SESSION , DCV_WINDOWS_HIBERNATE_IDLE_SESSION or DCV_WINDOWS_STOP_IDLE_SESSION configured, SOCA will automatically try to hibernate/stop your idle instance if they are outside of regular schedule hours. Idle time is calculated based on: Last time the user accessed the virtual desktop Current CPUs usage. Session will only be stopped/hibernated if current CPUs is below a threshold configured via DCV_IDLE_CPU_THRESHOLD (default to 15%). This setting avoid desktop being stopped/hibernated while compute intensive tasks are still running on, even if the user did not access the desktop for a while. Common configuration \u00b6 # General TIMEZONE = \"UTC\" # Change to match your local timezone if needed. See https://en.wikipedia.org/wiki/List_of_tz_database_time_zones for all TZ DCV_FORCE_INSTANCE_HIBERNATE_SUPPORT = False # If True, users can only provision instances that support hibernation DCV_TOKEN_SYMMETRIC_KEY = os.environ[\"SOCA_DCV_TOKEN_SYMMETRIC_KEY\"] # used to encrypt/decrypt and validate DCV session auth DCV_RESTRICTED_INSTANCE_TYPE = ['metal', 'nano', 'micro', 'p3', 'p2'] # This instance type won't be visible on the dropdown menu DCV_IDLE_CPU_THRESHOLD = 15 # SOCA will NOT hibernate/stop an instance if current CPU usage % is over this value ALLOW_DOWNLOAD_FROM_PORTAL = True # Give user ability to download files from the web portal # DCV Linux DCV_LINUX_SESSION_COUNT = 4 DCV_LINUX_ALLOW_INSTANCE_CHANGE = True # Allow user to change their instance type if their DCV session is stopped DCV_LINUX_HIBERNATE_IDLE_SESSION = 1 # In hours. Windows DCV sessions will be hibernated to save cost if there is no active connection within the time specified. 0 to disable DCV_LINUX_STOP_IDLE_SESSION = 1 # In hours. Windows DCV sessions will be stopped to save cost if there is no active connection within the time specified. 0 to disable DCV_LINUX_TERMINATE_STOPPED_SESSION = 0 # In hours. Stopped Windows DCV will be permanently terminated if user won't restart it within the time specified. 0 to disable # DCV Windows DCV_WINDOWS_SESSION_COUNT = 4 DCV_WINDOWS_ALLOW_INSTANCE_CHANGE = True # Allow user to change their instance type if their DCV session is stopped DCV_WINDOWS_HIBERNATE_IDLE_SESSION = 1 # In hours. Windows DCV sessions will be hibernated to save cost if there is no active connection within the time specified. 0 to disable DCV_WINDOWS_STOP_IDLE_SESSION = 1 # In hours. Windows DCV sessions will be stopped to save cost if there is no active connection within the time specified. 0 to disable DCV_WINDOWS_TERMINATE_STOPPED_SESSION = 0 # In hours. Stopped Windows DCV will be permanently terminated if user won't restart it within the time specified. 0 to disable DCV_WINDOWS_AUTOLOGON = True # enable or disable autologon. If disabled user will have to manually input Windows password DCV_WINDOWS_AMI = {\"graphics\": {\"us-east-1\": \"ami-035a352d4d53371dc\", .... \"ap-south-1\": \"ami-09c1d03de366041a4\"}, \"non-graphics\": {\"us-east-1\": \"ami-021660b17250fbc9b\", .... \"ap-south-1\": \"ami-08e852f6df553818a\"}}","title":"Create Virtual Desktops (Windows/Linux)"},{"location":"web-interface/create-virtual-desktops/#create-your-windowslinux-desktop","text":"To launch your Windows/Linux desktop, you first have to pick a name, choose how much storage you want to allocate as well as specify other options such as hibernation support, compute type, software stack or even subnet id. Need help? Hover your mouse on the parameter name to display help section. Once you are ready, click \"Launch My Session\" button. Your session interface will be changed to pending state. Your virtual desktop will be ready within 10-15 minutes. Startup time is based on the image selected, the operating system as well as the instance type. Windows tends to boot faster as some components such as the scheduler or EFS are not installed. Nvidia drivers for GPU instances SOCA automatically install Nvidia GRID (G3/G4) drivers. Tesla drivers (P3) have to be installed separately by the customers. Once your virtual desktop is active, your session interface will display two ways to connect to it: You can either access your desktop directly via your browser (Option1) or use the NICE DCV native application (Option2). A link to download the application is provided on the website. For better performance, we recommend using the Option 2.","title":"Create your Windows/Linux desktop"},{"location":"web-interface/create-virtual-desktops/#example-of-linux-desktop","text":"","title":"Example of Linux desktop"},{"location":"web-interface/create-virtual-desktops/#example-of-windows-desktop","text":"Important Unlike Linux desktops, Windows desktops are not connected to the scheduler and you cannot submit jobs directly from your Windows system. To submit a job via Windows, you first need to access the web ui then submit a job via HTTP endpoint or via the HTTP REST API","title":"Example of Windows desktop"},{"location":"web-interface/create-virtual-desktops/#stophibernate-your-desktop","text":"What does hibernate means? When you hibernate an instance, your desktop state is saved in memory. When you restart it, all your applications will automatically resume. On the other hand, stopping a virtual desktop is the same as powering off your laptop. Note not all EC2 instances support hibernation. To stop/hibernate your session, click the \"Stop\" (or \"Hibernate\") button in the top bar. A popup will appear, asking you to confirm your action. Click \"Stop/Hibernate my session\" button to temporarily turn off your virtual desktop. Your desktop is now stopped (you are not charged for compute price while your instance is stopped). To restart your session simply click \"Restart your session\" button.","title":"Stop/Hibernate your desktop"},{"location":"web-interface/create-virtual-desktops/#configure-your-desktop-auto-startstop-time","text":"To change the schedule of your desktop, click the \"Schedule\" button in the top bar. SOCA will honors your session schedule and automatically start/stop your desktop based on your own schedule. Desktop will be stopped outside of scheduled hours only if they are idle. You can configure three type of schedule for your desktop: Run all day (SOCA will ensures your desktop is up and running from 12 AM to 12 PM) Stopped all day (SOCA will ensures your desktop is stopped from 12 AM to 12 PM) Custom schedule (SOCA will honors your own schedule) Info If you manually start your desktop during off hours, your session will be up and running until it becomes idle. Idle time can be customized by cluster admins. Schedule and Timezone By default, SOCA is configured to use UTC timezone and this may be a problem if you are currently using a different timezone. To fix that, you can adjust your local timezone on config.py (then restart the web ui)","title":"Configure your desktop auto start/stop time"},{"location":"web-interface/create-virtual-desktops/#update-your-hardware-requirements-on-the-fly","text":"You can upgrade/downgrade the hardware of your desktop only when your session is stopped and if you have disabled hibernation. To do so, click Choose the new type of instance you want to resize your desktop to then click \"Change Instance Type\". Hardware upgrade/downgrade are instant. Your desktop will use the updated config as soon as you restart it.","title":"Update your hardware requirements on the fly"},{"location":"web-interface/create-virtual-desktops/#terminate-your-desktop","text":"To terminate your desktop, click the \"Kill\" button from the menu bar. You will be prompted for a confirmation message. Terminate a session may cause data loss if you are using ephemeral storage, so make sure to have uploaded all your data back to SOCA filesystem first.","title":"Terminate your desktop"},{"location":"web-interface/create-virtual-desktops/#windows-only-retrieve-local-admin-password","text":"As of today, Windows desktops are not configured to OpenLDAP, meaning you will use a local Windows account unique to each session. To retrieve the password for the local Administrator account, click \"Get Password\" icon. This will open a new window where you can retrieve the password.","title":"(Windows only) Retrieve local admin password"},{"location":"web-interface/create-virtual-desktops/#create-custom-windowslinux-images","text":"SOCA images are Windows or Linux desktops with pre-installed applications. Refer to this page to learn more about images management","title":"Create custom Windows/Linux images"},{"location":"web-interface/create-virtual-desktops/#how-idle-time-is-calculated","text":"When you have enabled DCV_LINUX_HIBERNATE_IDLE_SESSION , DCV_LINUX_STOP_IDLE_SESSION , DCV_WINDOWS_HIBERNATE_IDLE_SESSION or DCV_WINDOWS_STOP_IDLE_SESSION configured, SOCA will automatically try to hibernate/stop your idle instance if they are outside of regular schedule hours. Idle time is calculated based on: Last time the user accessed the virtual desktop Current CPUs usage. Session will only be stopped/hibernated if current CPUs is below a threshold configured via DCV_IDLE_CPU_THRESHOLD (default to 15%). This setting avoid desktop being stopped/hibernated while compute intensive tasks are still running on, even if the user did not access the desktop for a while.","title":"How idle time is calculated?"},{"location":"web-interface/create-virtual-desktops/#common-configuration","text":"# General TIMEZONE = \"UTC\" # Change to match your local timezone if needed. See https://en.wikipedia.org/wiki/List_of_tz_database_time_zones for all TZ DCV_FORCE_INSTANCE_HIBERNATE_SUPPORT = False # If True, users can only provision instances that support hibernation DCV_TOKEN_SYMMETRIC_KEY = os.environ[\"SOCA_DCV_TOKEN_SYMMETRIC_KEY\"] # used to encrypt/decrypt and validate DCV session auth DCV_RESTRICTED_INSTANCE_TYPE = ['metal', 'nano', 'micro', 'p3', 'p2'] # This instance type won't be visible on the dropdown menu DCV_IDLE_CPU_THRESHOLD = 15 # SOCA will NOT hibernate/stop an instance if current CPU usage % is over this value ALLOW_DOWNLOAD_FROM_PORTAL = True # Give user ability to download files from the web portal # DCV Linux DCV_LINUX_SESSION_COUNT = 4 DCV_LINUX_ALLOW_INSTANCE_CHANGE = True # Allow user to change their instance type if their DCV session is stopped DCV_LINUX_HIBERNATE_IDLE_SESSION = 1 # In hours. Windows DCV sessions will be hibernated to save cost if there is no active connection within the time specified. 0 to disable DCV_LINUX_STOP_IDLE_SESSION = 1 # In hours. Windows DCV sessions will be stopped to save cost if there is no active connection within the time specified. 0 to disable DCV_LINUX_TERMINATE_STOPPED_SESSION = 0 # In hours. Stopped Windows DCV will be permanently terminated if user won't restart it within the time specified. 0 to disable # DCV Windows DCV_WINDOWS_SESSION_COUNT = 4 DCV_WINDOWS_ALLOW_INSTANCE_CHANGE = True # Allow user to change their instance type if their DCV session is stopped DCV_WINDOWS_HIBERNATE_IDLE_SESSION = 1 # In hours. Windows DCV sessions will be hibernated to save cost if there is no active connection within the time specified. 0 to disable DCV_WINDOWS_STOP_IDLE_SESSION = 1 # In hours. Windows DCV sessions will be stopped to save cost if there is no active connection within the time specified. 0 to disable DCV_WINDOWS_TERMINATE_STOPPED_SESSION = 0 # In hours. Stopped Windows DCV will be permanently terminated if user won't restart it within the time specified. 0 to disable DCV_WINDOWS_AUTOLOGON = True # enable or disable autologon. If disabled user will have to manually input Windows password DCV_WINDOWS_AMI = {\"graphics\": {\"us-east-1\": \"ami-035a352d4d53371dc\", .... \"ap-south-1\": \"ami-09c1d03de366041a4\"}, \"non-graphics\": {\"us-east-1\": \"ami-021660b17250fbc9b\", .... \"ap-south-1\": \"ami-08e852f6df553818a\"}}","title":"Common configuration"},{"location":"web-interface/create-your-own-queue/","text":"Things to know before you start By default, Scale-Out Computing on AWS creates 4 queues: high, normal (default), low and alwayson. Queue with automatic instance provisioning \u00b6 Create the queue \u00b6 Via the web UI \u00b6 As an admin, click \"Queue Management\" section on the left sidebar. Select a queue name then choose \"Automatic Provisioning\" Via command-line \u00b6 On your scheduler host, run qmgr as root and enter the following commands: # Create queue name. Note: can't start with numerical character and it's recommended to use lowercase only Qmgr:create queue <queue_name> # Set the queue to execution Qmgr:set queue <queue_name> queue_type = Execution # Set default compute node - See below for more information Qmgr:set queue <queue_name> default_chunk.compute_node = tbd # Enable / Start the queue Qmgr:set queue <queue_name> enabled = True Qmgr:set queue <queue_name> started = True # Exit Qmgr:exit What is compute_node=tbd On Scale-Out Computing on AWS, unless you configure queue with AlwaysOn instances, nodes will be provisioned based on queue status. When you submit a job, Scale-Out Computing on AWS will automatically provision capacity for this job and compute_node is the scheduler making sure only one job can run on this instance. compute_node=tbd is the default value, making sure any new jobs won't run on existing (if any) nodes Configure automatic host provisioning \u00b6 If you want to enable automatic host provisioning, edit this file: /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml Option1: I want to use the same settings as an existing queue \u00b6 In this case, simply update the array with your new queue queue_type : compute : queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] # <- Add your queue to the array instance_ami : \"ami-1234567\" instance_type : \"c5.large\" ... Option2: I want to configure specific settings \u00b6 In this case, you will first need to create a new section on the YAML file (see example with memory) queue_type : compute : queues : [ \"queue1\" ] instance_ami : \"ami-1234567\" instance_type : \"c5.large\" scratch_size : \"100\" memory : # <- Add new section queues : [ \"queue2\" ] instance_ami : \"ami-9876543\" instance_type : \"r5.24xlarge\" scratch_size : \"600\" Finally, add a new crontab on the scheduler machine (as root). Use -c to path to the YAML file and -t to the YAML section you just created */3 * * * * source /etc/environment ; /apps/soca/ $SOCA_CONFIGURATION /python/latest/bin/python3 /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/dispatcher.py -c /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/settings/queue_mapping.yml -t memory Automatic Host provisioning logs \u00b6 All logs queues are stored under /apps/soca/$SOCA_CONFIGURATION/cluster_manager/logs/<queue_name> Queue with AlwaysOn instances \u00b6 Important Scale-Out Computing on AWS automatically created one AlwaysOn queue for you called \"alwayson\" during the first installation In this mode, instances will never be stopped programmatically. You are responsible to terminate the capacity manually by deleting the associated CloudFormation stack Create the queue \u00b6 Via the web UI \u00b6 As an admin, click \"Queue Management\" section on the left sidebar. Select a queue name then choose \"Always On\" Via command-line \u00b6 On your scheduler host, run qmgr as root and enter the following commands: # Create queue name. Note: can't start with numerical character and it's recommended to use lowercase only Qmgr:create queue <queue_name> # Set the queue to execution Qmgr:set queue <queue_name> queue_type = Execution # Enable / Start the queue Qmgr:set queue <queue_name> enabled = True Qmgr:set queue <queue_name> started = True # Exit Qmgr:exit Start provisioning some capacity \u00b6 Run python3 apps/soca/cluster_manager/add_nodes.py and enable --keep_forever True flag # Launch 1 c5.large always on python3 /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/add_nodes.py --instance_type c5.large \\ --desired_capacity 1 \\ --queue <queue_name> \\ --job_name instancealwayson \\ --job_owner mcrozes \\ --keep_forever True IMPORTANT: You specified --keep-forever flag. This instance will be running 24 /7 until you MANUALLY terminate the Cloudformation Stack If you need help with this script, run python3 add_nodes.py -h Delete AlwaysOn capacity \u00b6 Simply go to your CloudFormation console, locate the stack following the naming convention: soca- cluster-name -keepforever- queue_name -uniqueid and terminate it. Delete a queue \u00b6 Via the web ui, go to \"Queue Management\" then navigate to the \"Delete Queue\" tab","title":"Create your own queue"},{"location":"web-interface/create-your-own-queue/#queue-with-automatic-instance-provisioning","text":"","title":"Queue with automatic instance provisioning"},{"location":"web-interface/create-your-own-queue/#create-the-queue","text":"","title":"Create the queue"},{"location":"web-interface/create-your-own-queue/#via-the-web-ui","text":"As an admin, click \"Queue Management\" section on the left sidebar. Select a queue name then choose \"Automatic Provisioning\"","title":"Via the web UI"},{"location":"web-interface/create-your-own-queue/#via-command-line","text":"On your scheduler host, run qmgr as root and enter the following commands: # Create queue name. Note: can't start with numerical character and it's recommended to use lowercase only Qmgr:create queue <queue_name> # Set the queue to execution Qmgr:set queue <queue_name> queue_type = Execution # Set default compute node - See below for more information Qmgr:set queue <queue_name> default_chunk.compute_node = tbd # Enable / Start the queue Qmgr:set queue <queue_name> enabled = True Qmgr:set queue <queue_name> started = True # Exit Qmgr:exit What is compute_node=tbd On Scale-Out Computing on AWS, unless you configure queue with AlwaysOn instances, nodes will be provisioned based on queue status. When you submit a job, Scale-Out Computing on AWS will automatically provision capacity for this job and compute_node is the scheduler making sure only one job can run on this instance. compute_node=tbd is the default value, making sure any new jobs won't run on existing (if any) nodes","title":"Via command-line"},{"location":"web-interface/create-your-own-queue/#configure-automatic-host-provisioning","text":"If you want to enable automatic host provisioning, edit this file: /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml","title":"Configure automatic host provisioning"},{"location":"web-interface/create-your-own-queue/#option1-i-want-to-use-the-same-settings-as-an-existing-queue","text":"In this case, simply update the array with your new queue queue_type : compute : queues : [ \"queue1\" , \"queue2\" , \"queue3\" ] # <- Add your queue to the array instance_ami : \"ami-1234567\" instance_type : \"c5.large\" ...","title":"Option1: I want to use the same settings as an existing queue"},{"location":"web-interface/create-your-own-queue/#option2-i-want-to-configure-specific-settings","text":"In this case, you will first need to create a new section on the YAML file (see example with memory) queue_type : compute : queues : [ \"queue1\" ] instance_ami : \"ami-1234567\" instance_type : \"c5.large\" scratch_size : \"100\" memory : # <- Add new section queues : [ \"queue2\" ] instance_ami : \"ami-9876543\" instance_type : \"r5.24xlarge\" scratch_size : \"600\" Finally, add a new crontab on the scheduler machine (as root). Use -c to path to the YAML file and -t to the YAML section you just created */3 * * * * source /etc/environment ; /apps/soca/ $SOCA_CONFIGURATION /python/latest/bin/python3 /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/dispatcher.py -c /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/settings/queue_mapping.yml -t memory","title":"Option2: I want to configure specific settings"},{"location":"web-interface/create-your-own-queue/#automatic-host-provisioning-logs","text":"All logs queues are stored under /apps/soca/$SOCA_CONFIGURATION/cluster_manager/logs/<queue_name>","title":"Automatic Host provisioning logs"},{"location":"web-interface/create-your-own-queue/#queue-with-alwayson-instances","text":"Important Scale-Out Computing on AWS automatically created one AlwaysOn queue for you called \"alwayson\" during the first installation In this mode, instances will never be stopped programmatically. You are responsible to terminate the capacity manually by deleting the associated CloudFormation stack","title":"Queue with AlwaysOn instances"},{"location":"web-interface/create-your-own-queue/#create-the-queue_1","text":"","title":"Create the queue"},{"location":"web-interface/create-your-own-queue/#via-the-web-ui_1","text":"As an admin, click \"Queue Management\" section on the left sidebar. Select a queue name then choose \"Always On\"","title":"Via the web UI"},{"location":"web-interface/create-your-own-queue/#via-command-line_1","text":"On your scheduler host, run qmgr as root and enter the following commands: # Create queue name. Note: can't start with numerical character and it's recommended to use lowercase only Qmgr:create queue <queue_name> # Set the queue to execution Qmgr:set queue <queue_name> queue_type = Execution # Enable / Start the queue Qmgr:set queue <queue_name> enabled = True Qmgr:set queue <queue_name> started = True # Exit Qmgr:exit","title":"Via command-line"},{"location":"web-interface/create-your-own-queue/#start-provisioning-some-capacity","text":"Run python3 apps/soca/cluster_manager/add_nodes.py and enable --keep_forever True flag # Launch 1 c5.large always on python3 /apps/soca/ $SOCA_CONFIGURATION /cluster_manager/add_nodes.py --instance_type c5.large \\ --desired_capacity 1 \\ --queue <queue_name> \\ --job_name instancealwayson \\ --job_owner mcrozes \\ --keep_forever True IMPORTANT: You specified --keep-forever flag. This instance will be running 24 /7 until you MANUALLY terminate the Cloudformation Stack If you need help with this script, run python3 add_nodes.py -h","title":"Start provisioning some capacity"},{"location":"web-interface/create-your-own-queue/#delete-alwayson-capacity","text":"Simply go to your CloudFormation console, locate the stack following the naming convention: soca- cluster-name -keepforever- queue_name -uniqueid and terminate it.","title":"Delete AlwaysOn capacity"},{"location":"web-interface/create-your-own-queue/#delete-a-queue","text":"Via the web ui, go to \"Queue Management\" then navigate to the \"Delete Queue\" tab","title":"Delete a queue"},{"location":"web-interface/disable-api/","text":"If required, SOCA administrators can disable web API or views by using @disabled decorator Disable an API \u00b6 First, let's confirm a user can submit a job via the /api/scheduler/job endpoint: curl -k -X POST \\ > -H \"X-SOCA-TOKEN: xxx\" \\ > -H \"X-SOCA-USER: mickael\" \\ > -F payload = \"IyEvYmluL2Jhc2gKI1BCUyAtTiB0ZXN0am9iCiNQQlMgLVYgLWogb2UgLW8gdGVzdGpvYl9vdXRwdXQucWxvZwojUEJTIC1QIG15cHJvamVjdAojUEJTIC1xIG5vcm1hbAojUEJTIC1sIG5vZGVzPTEsaW5zdGFuY2VfdHlwZT1jNS5sYXJnZQovYmluL2VjaG8gIkhlbGxvIFdvcmxkIgo=\" \\ > https://xxx.us-west-2.elb.amazonaws.com/api/scheduler/job { \"success\" : true, \"message\" : \"0\" } Edit /apps/soca/$SOCA_CONFIGURATION/cluter_web_ui/api/v1/scheduler/pbspro/job.py and import the new decorator from decorators import disabled Locate the API you want to disable and replace the current decorator with @disabled Before: @private_api def post ( self ): // code After: @disabled def post ( self ): // code Restart SOCA web interface via socawebui.sh stop/start and validate you cannot use the API anymore curl -k -X POST \\ > -H \"X-SOCA-TOKEN: xxx\" \\ > -H \"X-SOCA-USER: mickael\" \\ > -F payload = \"IyEvYmluL2Jhc2gKI1BCUyAtTiB0ZXN0am9iCiNQQlMgLVYgLWogb2UgLW8gdGVzdGpvYl9vdXRwdXQucWxvZwojUEJTIC1QIG15cHJvamVjdAojUEJTIC1xIG5vcm1hbAojUEJTIC1sIG5vZGVzPTEsaW5zdGFuY2VfdHlwZT1jNS5sYXJnZQovYmluL2VjaG8gIkhlbGxvIFdvcmxkIgo=\" \\ > https://xxx.us-west-2.elb.amazonaws.com/api/scheduler/job { \"success\" : false, \"message\" : \"This API has been disabled by your Administrator\" } If you want to re-enable the API, simply configure the decorator back to its previous version ( @private_api ). Restart the web interface again and verify the API is now enabled: curl -k -X POST \\ > -H \"X-SOCA-TOKEN: xxx\" \\ > -H \"X-SOCA-USER: mickael\" \\ > -F payload = \"IyEvYmluL2Jhc2gKI1BCUyAtTiB0ZXN0am9iCiNQQlMgLVYgLWogb2UgLW8gdGVzdGpvYl9vdXRwdXQucWxvZwojUEJTIC1QIG15cHJvamVjdAojUEJTIC1xIG5vcm1hbAojUEJTIC1sIG5vZGVzPTEsaW5zdGFuY2VfdHlwZT1jNS5sYXJnZQovYmluL2VjaG8gIkhlbGxvIFdvcmxkIgo=\" \\ > https://xxx.us-west-2.elb.amazonaws.com/api/scheduler/job { \"success\" : true, \"message\" : \"1\" } Disable a view \u00b6 Process is very similar, locate the HTTP view you want to restrict. For example edit /apps/soca/$SOCA_CONFIGURATION/cluster_web_ui/views/remote_desktop.py Import the new decorator from decorators import login_required , disabled Then replace the current decorator of the view you want to restrict with @disabled @remote_desktop . route ( '/remote_desktop' , methods = [ 'GET' ]) @disabled def index (): // code Restart the Web UI. Accessing the view will now redirect you back to your homepage","title":"Disable API"},{"location":"web-interface/disable-api/#disable-an-api","text":"First, let's confirm a user can submit a job via the /api/scheduler/job endpoint: curl -k -X POST \\ > -H \"X-SOCA-TOKEN: xxx\" \\ > -H \"X-SOCA-USER: mickael\" \\ > -F payload = \"IyEvYmluL2Jhc2gKI1BCUyAtTiB0ZXN0am9iCiNQQlMgLVYgLWogb2UgLW8gdGVzdGpvYl9vdXRwdXQucWxvZwojUEJTIC1QIG15cHJvamVjdAojUEJTIC1xIG5vcm1hbAojUEJTIC1sIG5vZGVzPTEsaW5zdGFuY2VfdHlwZT1jNS5sYXJnZQovYmluL2VjaG8gIkhlbGxvIFdvcmxkIgo=\" \\ > https://xxx.us-west-2.elb.amazonaws.com/api/scheduler/job { \"success\" : true, \"message\" : \"0\" } Edit /apps/soca/$SOCA_CONFIGURATION/cluter_web_ui/api/v1/scheduler/pbspro/job.py and import the new decorator from decorators import disabled Locate the API you want to disable and replace the current decorator with @disabled Before: @private_api def post ( self ): // code After: @disabled def post ( self ): // code Restart SOCA web interface via socawebui.sh stop/start and validate you cannot use the API anymore curl -k -X POST \\ > -H \"X-SOCA-TOKEN: xxx\" \\ > -H \"X-SOCA-USER: mickael\" \\ > -F payload = \"IyEvYmluL2Jhc2gKI1BCUyAtTiB0ZXN0am9iCiNQQlMgLVYgLWogb2UgLW8gdGVzdGpvYl9vdXRwdXQucWxvZwojUEJTIC1QIG15cHJvamVjdAojUEJTIC1xIG5vcm1hbAojUEJTIC1sIG5vZGVzPTEsaW5zdGFuY2VfdHlwZT1jNS5sYXJnZQovYmluL2VjaG8gIkhlbGxvIFdvcmxkIgo=\" \\ > https://xxx.us-west-2.elb.amazonaws.com/api/scheduler/job { \"success\" : false, \"message\" : \"This API has been disabled by your Administrator\" } If you want to re-enable the API, simply configure the decorator back to its previous version ( @private_api ). Restart the web interface again and verify the API is now enabled: curl -k -X POST \\ > -H \"X-SOCA-TOKEN: xxx\" \\ > -H \"X-SOCA-USER: mickael\" \\ > -F payload = \"IyEvYmluL2Jhc2gKI1BCUyAtTiB0ZXN0am9iCiNQQlMgLVYgLWogb2UgLW8gdGVzdGpvYl9vdXRwdXQucWxvZwojUEJTIC1QIG15cHJvamVjdAojUEJTIC1xIG5vcm1hbAojUEJTIC1sIG5vZGVzPTEsaW5zdGFuY2VfdHlwZT1jNS5sYXJnZQovYmluL2VjaG8gIkhlbGxvIFdvcmxkIgo=\" \\ > https://xxx.us-west-2.elb.amazonaws.com/api/scheduler/job { \"success\" : true, \"message\" : \"1\" }","title":"Disable an API"},{"location":"web-interface/disable-api/#disable-a-view","text":"Process is very similar, locate the HTTP view you want to restrict. For example edit /apps/soca/$SOCA_CONFIGURATION/cluster_web_ui/views/remote_desktop.py Import the new decorator from decorators import login_required , disabled Then replace the current decorator of the view you want to restrict with @disabled @remote_desktop . route ( '/remote_desktop' , methods = [ 'GET' ]) @disabled def index (): // code Restart the Web UI. Accessing the view will now redirect you back to your homepage","title":"Disable a view"},{"location":"web-interface/import-export-application-profiles/","text":"Feature in preview This feature is only available on beta builds Refer to this page to learn how you can easily share your application profiles between multiple SOCA clusters. Export an existing application \u00b6 To share your application profile, go to \"Application Management\" section and navigate to \"Import/Export\" tab Select the application you want to export from the dropdown menu then click \"Export\" This will download a json file. Share this json file with whoever want to be able to use your application profile on their SOCA environment Import an existing application \u00b6 To import an application profile, go to \"Application Management\" section and navigate to \"Import/Export\" tab Specify an application name, upload a valid json then click Import Your application will be imported successfully assuming the json provided is a valid SOCA application profile Your application is now available on SOCA. You can edit it to make any change based on your own environment or start using it the way it is.","title":"Import/Export application profiles"},{"location":"web-interface/import-export-application-profiles/#export-an-existing-application","text":"To share your application profile, go to \"Application Management\" section and navigate to \"Import/Export\" tab Select the application you want to export from the dropdown menu then click \"Export\" This will download a json file. Share this json file with whoever want to be able to use your application profile on their SOCA environment","title":"Export an existing application"},{"location":"web-interface/import-export-application-profiles/#import-an-existing-application","text":"To import an application profile, go to \"Application Management\" section and navigate to \"Import/Export\" tab Specify an application name, upload a valid json then click Import Your application will be imported successfully assuming the json provided is a valid SOCA application profile Your application is now available on SOCA. You can edit it to make any change based on your own environment or start using it the way it is.","title":"Import an existing application"},{"location":"web-interface/manage-ldap-users/","text":"Using Web UI \u00b6 Log in to the Web UI with an admin account and locate \"Users Management\" or \"Group Management\" sections on the left sidebar. Info Users and Group management are limited to admins users Users \u00b6 Add users \u00b6 To create a new user, simply fill out the \"Create New User\" form. Select whether or not the user will be an admin by checking \"Enable Sudo Access\" checkbox. If needed, you can also manually force UID/GID or choose a shell different than /bin/bash . You will see a success message if the user is created correctly What is a SUDO user? Users will SUDO permissions will be admin on the cluster and authorized to run any sudo command. Make sure to limit this ability to HPC/AWS/Linux admins and other power users. Custom shell SOCA uses /bin/bash by default but admins can specify any available shells installed on the system ( list available on /etc/shells ) Delete users \u00b6 To delete a user, navigate to 'Delete Users' section then select the user you want to delete and check the checkbox. You will see a success message if the user is deleted correctly. Info Deleting a user will only delete the LDAP user. Associated $HOME directory is still preserved on /data/home Reset password for a given user \u00b6 Users can change their own password via the web ui. If needed, admins can also temporarily unlock a user by resetting the password on his/her behalf. Manage SUDO (admin permission) \u00b6 Admins can grant/revoke SUDO permissions for any user: Groups \u00b6 Create a new group \u00b6 To create a new group, simply select \"Create a Group\" and select the user(s) you want to add to this group. Check group membership \u00b6 You can check group membership by going to \"Check group membership\" tab. Change group membership \u00b6 If needed, you can add/remove users from a given groups. Delete group \u00b6 Lastly, to delete a group, simply navigate to \"Delete Group\" tab. Other LDAP operations \u00b6 Attention It's recommended to interact with OpenLDAP via the web ui interface. Scale-Out Computing on AWS uses OpenLDAP and you can interact with your directory using LDIF directly. Scale-Out Computing on AWS LDAP Schema People: OU=People,DC=soca,DC=local Groups: OU=Group,DC=soca,DC=local Sudoers: OU=Sudoers,DC=soca,DC=local (This OU manages sudo permission on the cluster) Admin LDAP account credentials Bind DN (-D): cn=admin,dc=soca,dc=local Password (-y) /root/OpenLdapAdminPassword.txt For example, if you want to create a new group, create a new LDIF file (mynewgroup.ldif) and add the following content: dn: cn=mynewgroup,ou=Group,dc=soca,dc=local objectClass: top objectClass: posixGroup cn: mynewgroup gidNumber: 6000 memberUid: mytestuser Run the following ldapadd command to add your new group: ldapadd -x -D cn = admin,dc = soca,dc = local -y /root/OpenLdapAdminPassword.txt -f mynewgroup.ldif adding new entry \"cn=mynewgroup,ou=Group,dc=soca,dc=local\" Finally valid your group has been created correctly using ldapsearch # Validate with Ldapsearch ~ ldapsearch -x cn = mynewgroup #Extended LDIF # # LDAPv3 # base DC=soca,DC=local (default) with scope subtree # filter: cn=mynewgroup # requesting: ALL # # mynewgroup, Group, soca.local dn: cn = mynewgroup,ou = Group,dc = soca,dc = local objectClass: top objectClass: posixGroup cn: mynewgroup gidNumber: 6000 memberUid: mytestuser Example for LDIF modify operation dn: cn=mynewgroup,ou=Group,dc=soca,dc=local changetype: modify add: memberUid memberUid: anotheruser Example for LDIF delete operation dn: cn=mynewgroup,ou=Group,dc=soca,dc=local changetype: modify delete: memberUid memberUid:: anotheruser # you get the memberUid by running a simple ldapsearch first Give users permissions to submit job \u00b6 By default, users can submit job to any queue, however you can set up ACL at queue level if needed","title":"Centralized user/group management"},{"location":"web-interface/manage-ldap-users/#using-web-ui","text":"Log in to the Web UI with an admin account and locate \"Users Management\" or \"Group Management\" sections on the left sidebar. Info Users and Group management are limited to admins users","title":"Using Web UI"},{"location":"web-interface/manage-ldap-users/#users","text":"","title":"Users"},{"location":"web-interface/manage-ldap-users/#add-users","text":"To create a new user, simply fill out the \"Create New User\" form. Select whether or not the user will be an admin by checking \"Enable Sudo Access\" checkbox. If needed, you can also manually force UID/GID or choose a shell different than /bin/bash . You will see a success message if the user is created correctly What is a SUDO user? Users will SUDO permissions will be admin on the cluster and authorized to run any sudo command. Make sure to limit this ability to HPC/AWS/Linux admins and other power users. Custom shell SOCA uses /bin/bash by default but admins can specify any available shells installed on the system ( list available on /etc/shells )","title":"Add users"},{"location":"web-interface/manage-ldap-users/#delete-users","text":"To delete a user, navigate to 'Delete Users' section then select the user you want to delete and check the checkbox. You will see a success message if the user is deleted correctly. Info Deleting a user will only delete the LDAP user. Associated $HOME directory is still preserved on /data/home","title":"Delete users"},{"location":"web-interface/manage-ldap-users/#reset-password-for-a-given-user","text":"Users can change their own password via the web ui. If needed, admins can also temporarily unlock a user by resetting the password on his/her behalf.","title":"Reset password for a given user"},{"location":"web-interface/manage-ldap-users/#manage-sudo-admin-permission","text":"Admins can grant/revoke SUDO permissions for any user:","title":"Manage SUDO (admin permission)"},{"location":"web-interface/manage-ldap-users/#groups","text":"","title":"Groups"},{"location":"web-interface/manage-ldap-users/#create-a-new-group","text":"To create a new group, simply select \"Create a Group\" and select the user(s) you want to add to this group.","title":"Create a new group"},{"location":"web-interface/manage-ldap-users/#check-group-membership","text":"You can check group membership by going to \"Check group membership\" tab.","title":"Check group membership"},{"location":"web-interface/manage-ldap-users/#change-group-membership","text":"If needed, you can add/remove users from a given groups.","title":"Change group membership"},{"location":"web-interface/manage-ldap-users/#delete-group","text":"Lastly, to delete a group, simply navigate to \"Delete Group\" tab.","title":"Delete group"},{"location":"web-interface/manage-ldap-users/#other-ldap-operations","text":"Attention It's recommended to interact with OpenLDAP via the web ui interface. Scale-Out Computing on AWS uses OpenLDAP and you can interact with your directory using LDIF directly. Scale-Out Computing on AWS LDAP Schema People: OU=People,DC=soca,DC=local Groups: OU=Group,DC=soca,DC=local Sudoers: OU=Sudoers,DC=soca,DC=local (This OU manages sudo permission on the cluster) Admin LDAP account credentials Bind DN (-D): cn=admin,dc=soca,dc=local Password (-y) /root/OpenLdapAdminPassword.txt For example, if you want to create a new group, create a new LDIF file (mynewgroup.ldif) and add the following content: dn: cn=mynewgroup,ou=Group,dc=soca,dc=local objectClass: top objectClass: posixGroup cn: mynewgroup gidNumber: 6000 memberUid: mytestuser Run the following ldapadd command to add your new group: ldapadd -x -D cn = admin,dc = soca,dc = local -y /root/OpenLdapAdminPassword.txt -f mynewgroup.ldif adding new entry \"cn=mynewgroup,ou=Group,dc=soca,dc=local\" Finally valid your group has been created correctly using ldapsearch # Validate with Ldapsearch ~ ldapsearch -x cn = mynewgroup #Extended LDIF # # LDAPv3 # base DC=soca,DC=local (default) with scope subtree # filter: cn=mynewgroup # requesting: ALL # # mynewgroup, Group, soca.local dn: cn = mynewgroup,ou = Group,dc = soca,dc = local objectClass: top objectClass: posixGroup cn: mynewgroup gidNumber: 6000 memberUid: mytestuser Example for LDIF modify operation dn: cn=mynewgroup,ou=Group,dc=soca,dc=local changetype: modify add: memberUid memberUid: anotheruser Example for LDIF delete operation dn: cn=mynewgroup,ou=Group,dc=soca,dc=local changetype: modify delete: memberUid memberUid:: anotheruser # you get the memberUid by running a simple ldapsearch first","title":"Other LDAP operations"},{"location":"web-interface/manage-ldap-users/#give-users-permissions-to-submit-job","text":"By default, users can submit job to any queue, however you can set up ACL at queue level if needed","title":"Give users permissions to submit job"},{"location":"web-interface/my-activity/","text":"\"My Activity\" section let each user access their Kibana dashboard pre-configured with user and date filters. Important This page embed the Amazon OpenSearch (formerly Elasticsearch) instance configured with your SOCA cluster. You must have created the job index first","title":"My Activity"},{"location":"web-interface/my-files/","text":"\"My Files\" section let each user access their filesystem via a web browser. Create a new folder \u00b6 Click \"Create Folder\" to create a new folder un your current working directory. Your folder will be created instantly and visible under the File Explorer section. Upload files \u00b6 Click \"Upload Files\" button then drag and drop (or select) the files you want to upload. Info Maximum upload size (default 5GB) and upload timeout (default 30 minutes) can be configure via config.py Download files \u00b6 All files in directory \u00b6 Click \"Download All\" button to download all files from the current working directory (SOCA will create a zip archive) Multiple files \u00b6 Select the files you want to download by checking the assigned checkboxes them click \"Download Selected\" button. Single file \u00b6 To download a single file, click on the file name or use the first icon. Delete files \u00b6 To delete a file, simply click the last icon located on the right of the file name. This will open a confirmation window. Edit files \u00b6 To edit a file, click the 3 rd icon located on the right of the file name. This will open a text editor. The editor includes syntax highlights and auto-completion. Once you are done with your changes, check the checkbox and click \"Save\" button. Use file as simulation input \u00b6 Refer to this page for more information Files created via the filesystem are not visible \u00b6 For better performance, SOCA cache the content of a directory by default for 2 minutes (this settings can be changed in config.py via DEFAULT_CACHE_TIME). If you have created a file via SSH/DCV and this file is not yet visible, simply force a cache refresh by clicking the grey button. Permissions & Settings \u00b6 The web interface rely on POSIX permissions. In other words, creating a file/folder is the same as running \"touch\" or \"mkdir\". Web configuration parameters are listed on /apps/soca/<YOUR_CLUSTER>/cluster_web_ui/config.py . A restart is required if you change any of these settings. APPS_LOCATION = \"/apps/\" USER_HOME = \"/data/home\" # Adjust if you use a different location CHROOT_USER = False # if True, user can only access their $HOME directory (aka: USER_HOME/<user>) PATH_TO_RESTRICT = [] # eg: /apps/folder1 -> users can't access anything under /apps/folder1 DEFAULT_CACHE_TIME = 120 # 2 minutes. Change this value to optimize performance in case you have a large number of concurrent user MAX_UPLOAD_FILE = 5120 # 5 GB MAX_UPLOAD_TIMEOUT = 1800000 # 30 minutes MAX_SIZE_ONLINE_PREVIEW = 150000000 # in bytes (150mb by default), maximum size of file that can be visualized via the web editor MAX_ARCHIVE_SIZE = 150000000 # in bytes (150mb by default), maximum size of archive generated when downloading multiple files at once DAILY_BACKUP_COUNT = 15 # Keep 15 latest daily backups","title":"My Files"},{"location":"web-interface/my-files/#create-a-new-folder","text":"Click \"Create Folder\" to create a new folder un your current working directory. Your folder will be created instantly and visible under the File Explorer section.","title":"Create a new folder"},{"location":"web-interface/my-files/#upload-files","text":"Click \"Upload Files\" button then drag and drop (or select) the files you want to upload. Info Maximum upload size (default 5GB) and upload timeout (default 30 minutes) can be configure via config.py","title":"Upload files"},{"location":"web-interface/my-files/#download-files","text":"","title":"Download files"},{"location":"web-interface/my-files/#all-files-in-directory","text":"Click \"Download All\" button to download all files from the current working directory (SOCA will create a zip archive)","title":"All files in directory"},{"location":"web-interface/my-files/#multiple-files","text":"Select the files you want to download by checking the assigned checkboxes them click \"Download Selected\" button.","title":"Multiple files"},{"location":"web-interface/my-files/#single-file","text":"To download a single file, click on the file name or use the first icon.","title":"Single file"},{"location":"web-interface/my-files/#delete-files","text":"To delete a file, simply click the last icon located on the right of the file name. This will open a confirmation window.","title":"Delete files"},{"location":"web-interface/my-files/#edit-files","text":"To edit a file, click the 3 rd icon located on the right of the file name. This will open a text editor. The editor includes syntax highlights and auto-completion. Once you are done with your changes, check the checkbox and click \"Save\" button.","title":"Edit files"},{"location":"web-interface/my-files/#use-file-as-simulation-input","text":"Refer to this page for more information","title":"Use file as simulation input"},{"location":"web-interface/my-files/#files-created-via-the-filesystem-are-not-visible","text":"For better performance, SOCA cache the content of a directory by default for 2 minutes (this settings can be changed in config.py via DEFAULT_CACHE_TIME). If you have created a file via SSH/DCV and this file is not yet visible, simply force a cache refresh by clicking the grey button.","title":"Files created via the filesystem are not visible"},{"location":"web-interface/my-files/#permissions-settings","text":"The web interface rely on POSIX permissions. In other words, creating a file/folder is the same as running \"touch\" or \"mkdir\". Web configuration parameters are listed on /apps/soca/<YOUR_CLUSTER>/cluster_web_ui/config.py . A restart is required if you change any of these settings. APPS_LOCATION = \"/apps/\" USER_HOME = \"/data/home\" # Adjust if you use a different location CHROOT_USER = False # if True, user can only access their $HOME directory (aka: USER_HOME/<user>) PATH_TO_RESTRICT = [] # eg: /apps/folder1 -> users can't access anything under /apps/folder1 DEFAULT_CACHE_TIME = 120 # 2 minutes. Change this value to optimize performance in case you have a large number of concurrent user MAX_UPLOAD_FILE = 5120 # 5 GB MAX_UPLOAD_TIMEOUT = 1800000 # 30 minutes MAX_SIZE_ONLINE_PREVIEW = 150000000 # in bytes (150mb by default), maximum size of file that can be visualized via the web editor MAX_ARCHIVE_SIZE = 150000000 # in bytes (150mb by default), maximum size of archive generated when downloading multiple files at once DAILY_BACKUP_COUNT = 15 # Keep 15 latest daily backups","title":"Permissions &amp; Settings"},{"location":"web-interface/my-job-queue/","text":"\"My Job Queue\" section let each user access their jobs information via the web interface. SOCA automatically add a tag if capacity is being provisioned for a job in \"queued\" state. Retrieve job information \u00b6 Click \"Job Info\" button to get information about your job. Access your job output \u00b6 Click \"Job Directory\" button to access the job output location. Delete a job \u00b6 Click \"Kill\" button to remove a job from the queue. Understand why your job cannot start \u00b6 There are multiple reasons why you job cannot start. To display useful information to the users, SOCA automatically add a visual tag to any jobs that won't be able to start due to misconfiguration. Users can understand why their jobs are blocked by clicking the \"Job cannot start\" button. Some examples includes: Not enough software licenses \u00b6 Typo in the instance type \u00b6 Invalid configuration (request EFA but use an instance without EFA support) \u00b6 And more ... \u00b6 Limit of running jobs exceeded Limit of provisioning instance exceeded AWS Service limit errors (eg: cannot provision more EBS volume) Can't start job when \"force_ri=True\" if you do not have enough Reserved Instance availables ...","title":"My Job Queue"},{"location":"web-interface/my-job-queue/#retrieve-job-information","text":"Click \"Job Info\" button to get information about your job.","title":"Retrieve job information"},{"location":"web-interface/my-job-queue/#access-your-job-output","text":"Click \"Job Directory\" button to access the job output location.","title":"Access your job output"},{"location":"web-interface/my-job-queue/#delete-a-job","text":"Click \"Kill\" button to remove a job from the queue.","title":"Delete a job"},{"location":"web-interface/my-job-queue/#understand-why-your-job-cannot-start","text":"There are multiple reasons why you job cannot start. To display useful information to the users, SOCA automatically add a visual tag to any jobs that won't be able to start due to misconfiguration. Users can understand why their jobs are blocked by clicking the \"Job cannot start\" button. Some examples includes:","title":"Understand why your job cannot start"},{"location":"web-interface/my-job-queue/#not-enough-software-licenses","text":"","title":"Not enough software licenses"},{"location":"web-interface/my-job-queue/#typo-in-the-instance-type","text":"","title":"Typo in the instance type"},{"location":"web-interface/my-job-queue/#invalid-configuration-request-efa-but-use-an-instance-without-efa-support","text":"","title":"Invalid configuration (request EFA but use an instance without EFA support)"},{"location":"web-interface/my-job-queue/#and-more","text":"Limit of running jobs exceeded Limit of provisioning instance exceeded AWS Service limit errors (eg: cannot provision more EBS volume) Can't start job when \"force_ri=True\" if you do not have enough Reserved Instance availables ...","title":"And more ..."},{"location":"web-interface/share-data-windows-sessions/","text":"Feature in preview This feature is only available on beta builds Using DCV session storage (Windows and Linux) \u00b6 In addition of existing SCP/SFTP, you can now take advantage of DCV session-storage feature . Session Storage is a folder from where you can upload/download files directly from DCV. Using the Web Browser \u00b6 Click the cloud icon (1) and select to \"Upload Files\" (2). Choose the file you want to upload from the explorer list. To check the progress of your upload/download, click on the \"Notification bar\" Once your upload is finish, locate the file on your filesystem On Linux, your files will be uploaded to $HOME/session-storage On Windows, your files will be uploaded to C:\\session-storage Download Session Storage also let you download files directly from your DCV session. Using the native application \u00b6 If you use the native application, Click \"Connection > File Storage\" Important Although session storage is very handy, it's recommended to use traditional SFTP/SCP or the Web UI to upload large file to upload large files. Share data between Windows sessions \u00b6 Unlike Linux desktop, Windows desktops do not share a common filesystem, meaning data hosted on your Windows Session #1 are not accessible to your Windows Session #2 out of the box. On the machine you want to share \u00b6 On the machine you want to share, first open a terminal, type \"ipconfig\" and note the IP of your session (130.0.157.1 in this example) Right click on the folder you want to share. In my example I want to share the entire C: drive so I right click on C: and click Properties Navigate to \"Sharing\" tab and click \"Advanced Sharing\" Check \"Share this folder\" box and specify a name (my_first_session in this example) You can also click \"Permissions\" to manage who can access your files (default to read-only). In this example I simply give \"Everyone\" Read/Write access. Please note \"Everyone\" still require users to be able to successfully authenticate to your machine. Finally, you can verify if you disk is correctly shared On the machine you want to access the share \u00b6 On the file explorer, right click \"Network\" tab and click \"Map Network Drive\" If it's your first time, you will need to enable \"Network Sharing\", simply click \"Ok\" This will open a new ribbon, click on it and click \"Turn on Discovery and File Sharing\" You will be prompted for your sharing settings. You can use both settings but we usually recommend limiting to Private Network only Now you have enabled file sharing, right click \"Network\" tab and click \"Map Network Drive\" again. This time you will be prompted with a new window asking you the location of your share Specify \\ \\ , then click \"Connect using different credentials\" Go back to SOCA and retrieve your Windows session you want to share the folder from. Click \"Get Password\" and note the password. Please note each Windows sessions have a unique password. Go back to your Windows and then enter your SOCA username (or Administrator) and the password your just retrieved from SOCA. You are done. You can now access your share from the file explorer","title":"How to easily share data with your virtual desktops"},{"location":"web-interface/share-data-windows-sessions/#using-dcv-session-storage-windows-and-linux","text":"In addition of existing SCP/SFTP, you can now take advantage of DCV session-storage feature . Session Storage is a folder from where you can upload/download files directly from DCV.","title":"Using DCV session storage (Windows and Linux)"},{"location":"web-interface/share-data-windows-sessions/#using-the-web-browser","text":"Click the cloud icon (1) and select to \"Upload Files\" (2). Choose the file you want to upload from the explorer list. To check the progress of your upload/download, click on the \"Notification bar\" Once your upload is finish, locate the file on your filesystem On Linux, your files will be uploaded to $HOME/session-storage On Windows, your files will be uploaded to C:\\session-storage Download Session Storage also let you download files directly from your DCV session.","title":"Using the Web Browser"},{"location":"web-interface/share-data-windows-sessions/#using-the-native-application","text":"If you use the native application, Click \"Connection > File Storage\" Important Although session storage is very handy, it's recommended to use traditional SFTP/SCP or the Web UI to upload large file to upload large files.","title":"Using the native application"},{"location":"web-interface/share-data-windows-sessions/#share-data-between-windows-sessions","text":"Unlike Linux desktop, Windows desktops do not share a common filesystem, meaning data hosted on your Windows Session #1 are not accessible to your Windows Session #2 out of the box.","title":"Share data between Windows sessions"},{"location":"web-interface/share-data-windows-sessions/#on-the-machine-you-want-to-share","text":"On the machine you want to share, first open a terminal, type \"ipconfig\" and note the IP of your session (130.0.157.1 in this example) Right click on the folder you want to share. In my example I want to share the entire C: drive so I right click on C: and click Properties Navigate to \"Sharing\" tab and click \"Advanced Sharing\" Check \"Share this folder\" box and specify a name (my_first_session in this example) You can also click \"Permissions\" to manage who can access your files (default to read-only). In this example I simply give \"Everyone\" Read/Write access. Please note \"Everyone\" still require users to be able to successfully authenticate to your machine. Finally, you can verify if you disk is correctly shared","title":"On the machine you want to share"},{"location":"web-interface/share-data-windows-sessions/#on-the-machine-you-want-to-access-the-share","text":"On the file explorer, right click \"Network\" tab and click \"Map Network Drive\" If it's your first time, you will need to enable \"Network Sharing\", simply click \"Ok\" This will open a new ribbon, click on it and click \"Turn on Discovery and File Sharing\" You will be prompted for your sharing settings. You can use both settings but we usually recommend limiting to Private Network only Now you have enabled file sharing, right click \"Network\" tab and click \"Map Network Drive\" again. This time you will be prompted with a new window asking you the location of your share Specify \\ \\ , then click \"Connect using different credentials\" Go back to SOCA and retrieve your Windows session you want to share the folder from. Click \"Get Password\" and note the password. Please note each Windows sessions have a unique password. Go back to your Windows and then enter your SOCA username (or Administrator) and the password your just retrieved from SOCA. You are done. You can now access your share from the file explorer","title":"On the machine you want to access the share"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/","text":"On this page, you will learn how to create an application profile and give your users the ability to submit HPC jobs via a simple web interface. Submit a job via REST API In addition of web based job submission, SOCA also supports job management via REST API. Click here to learn more No coding experience = No Problem \u00b6 SOCA features a complete visual form builder experience with simple drag & drop capabilities. HPC admins can build their own forms without any coding/HTML experience via an intuitive wysiwyg (What You See Is What You Get) solution. Build the job script \u00b6 The first step is to identify the variables you want your users to configure. Let's take this simple job file for reference: #PBS -N MyJobName #PBS -P MyJobProject #PBS -q queue_1 #PBS -l instance_type=c5.18xlarge # CD into current working directory cd $PBS_O_WORKDIR # Prepare the job environment, edit the current PATH, License Server etc export PATH = /apps/softwarename/v2020/ export LICENSE_SERVER = 1234 @licenseserver.internal # Run the solver /apps/softwarename/v2020/bin/solver --cpus 36 \\ --input-file myfile.input \\ --parameter1 value1 # Once job is complete, archive output to S3 BACKUP = 1 if [[ \" $BACKUP \" -eq 1 ]] ; then aws s3 sync . s3://mybucketname/ fi Replace the values/parameters you want your users to configure with %VARIABLE_NAME% such as: #PBS -N %job_name% #PBS -P %job_project% # I do not want my user to be able to change the queue #PBS -q myqueue #PBS -l instance_type=%instance_type% # CD into current working directory cd $PBS_O_WORKDIR # Prepare the job environment, edit the current PATH, License Server etc export PATH = /apps/softwarename/%version%/ export LICENSE_SERVER = 1234 @licenseserver.internal # Run the solver /apps/softwarename/%version%/bin/solver --cpus %cpus% \\ --input-file %input_file% \\ --parameter1 %parameter1% # Once job is complete, archive output to S3 BACKUP = %backup_enabled% if [[ \" $BACKUP \" -eq 1 ]] ; then aws s3 sync . s3://%bucket_to_archive%/ fi In this example: %job_name% will be replaced by the actual job name specified by the user %job_project% will be replaced by the project associated to the job %version% will let the user decide what software version to use %cpus% , %input_file%, and %parameter1% are application specific parameters %backup_enabled% will determine if we want to archive the job output to S3 %bucket_to_archive% will point to the user's personal S3 bucket Create the HTML form \u00b6 Now that you have identified all variables, you must create their associated HTML components. As a HPC admin, navigate to \"Application Management\" tab and start to build the HTML form. %job_name% , %job_project%, %bucket_to_archive%, %input_file% and %parameter1% \u00b6 Drag \"Text Field\" component from the left section to add it to the form. Configure the widget and configure the Name settings (red) with the variable name associated (job_name in our example) In the example below, the value entered by the user for job_name will be sent to the job script and retrieved via %job_name% Repeat the same operation for %job_project%, %bucket_to_archive%, %parameter1% and %input_file% Note %input_file% will automatically be configured with the path of the input file selected by the user %instance_type% \u00b6 We want to enforce the instance type to be c5.18xlarge. To do that, you can simply hardcode the information on the job script or create a \"Hidden Input\" parameter. The red section references to the variable name and the green section is the variable value. %cpus% \u00b6 For %cpus% variable, we recommend using the \"Number\" component Specify a name which match your variable name (red), pick the default value (green) then choose the Min/Max/Step values allowed (blue) %version% \u00b6 Assuming your application hierarchy is as follow: \u2514\u2500\u2500 /apps \u2514\u2500\u2500 /softwarename \u251c\u2500\u2500 v2020 \u251c\u2500\u2500 v2019 \u2514\u2500\u2500 v2018 This time, we recommend you using the \"Select\" component: Similarly to the previous examples, check the \"Required\" checkbox, map the \"Name\" to your variable name (%version%) and add labels (green) and their associated values (blue) Note Use autocomplete if you have a large number of entry %backup_enabled% \u00b6 %backup_enabled% is a boolean which enable (1) or disable (0) archival of the job output data to S3. This time use \"Radio Group\" component and configure the different values: Configure the job script \u00b6 Once your HTML form is done, simply click \"Step2\" and copy/paste your job script Select your interpreter Since this script is expected to be triggered by PBS, keep the default \"will use qsub\" option. Save your profile \u00b6 Finally, navigate to step3, choose an name, upload a thumbnail if needed (optional) and click \"Create this application\" Submit a test job \u00b6 To submit a job, first navigate to \"Submit Job (Web)\" on the left sidebar. Choose your input file and click \"Use as Simulation input\" icon Select the application you want to run Fill out the HTML form generated during the previous step with your own inputs. Real time cost estimate The cost of your simulation is calculated in real-time based on the resources you are specifying Nodes Count SOCA also determine the number of nodes to provision automatically based on the instance type and cpus requested. In this example, the instance is c5.18xlarge (36 cores) and the number of CPUs requested by the user is 72. SOCA automatically detect these values and determine the number of instances to provision is 2 Once done, click \"Submit Job\" and you job will be submitted to the queue. Inputs sanitization SOCA automatically sanitize your inputs when required (remove space, special characters etc ...). In this example, the job name specified was \"My Super Job\" and was corrected to \"MySuperJob\" due some scheduler limitations. You can verify the input file generated by clicking \"Job Directory\": Then select \"View or Edit this File\" icon (3 rd from the left) This will open the submit file created by the web form. You can verify the output generated by your form is correct and/or troubleshoot any potential issue Delete job \u00b6 To delete a job, simply navigate to \"My Job Queue\" section and click on the \"Kill\" button What if I want to run a Linux script/command \u00b6 If you want your job script to use regular bash interpreter (and not qsub), simple select \"This is a Linux script\". In other words, the output generated by your HTML world will be a simple bash script and SOCA will run /bin/bash job_submit.sh command.","title":"Submit your HPC job via a custom web interface"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#no-coding-experience-no-problem","text":"SOCA features a complete visual form builder experience with simple drag & drop capabilities. HPC admins can build their own forms without any coding/HTML experience via an intuitive wysiwyg (What You See Is What You Get) solution.","title":"No coding experience = No Problem"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#build-the-job-script","text":"The first step is to identify the variables you want your users to configure. Let's take this simple job file for reference: #PBS -N MyJobName #PBS -P MyJobProject #PBS -q queue_1 #PBS -l instance_type=c5.18xlarge # CD into current working directory cd $PBS_O_WORKDIR # Prepare the job environment, edit the current PATH, License Server etc export PATH = /apps/softwarename/v2020/ export LICENSE_SERVER = 1234 @licenseserver.internal # Run the solver /apps/softwarename/v2020/bin/solver --cpus 36 \\ --input-file myfile.input \\ --parameter1 value1 # Once job is complete, archive output to S3 BACKUP = 1 if [[ \" $BACKUP \" -eq 1 ]] ; then aws s3 sync . s3://mybucketname/ fi Replace the values/parameters you want your users to configure with %VARIABLE_NAME% such as: #PBS -N %job_name% #PBS -P %job_project% # I do not want my user to be able to change the queue #PBS -q myqueue #PBS -l instance_type=%instance_type% # CD into current working directory cd $PBS_O_WORKDIR # Prepare the job environment, edit the current PATH, License Server etc export PATH = /apps/softwarename/%version%/ export LICENSE_SERVER = 1234 @licenseserver.internal # Run the solver /apps/softwarename/%version%/bin/solver --cpus %cpus% \\ --input-file %input_file% \\ --parameter1 %parameter1% # Once job is complete, archive output to S3 BACKUP = %backup_enabled% if [[ \" $BACKUP \" -eq 1 ]] ; then aws s3 sync . s3://%bucket_to_archive%/ fi In this example: %job_name% will be replaced by the actual job name specified by the user %job_project% will be replaced by the project associated to the job %version% will let the user decide what software version to use %cpus% , %input_file%, and %parameter1% are application specific parameters %backup_enabled% will determine if we want to archive the job output to S3 %bucket_to_archive% will point to the user's personal S3 bucket","title":"Build the job script"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#create-the-html-form","text":"Now that you have identified all variables, you must create their associated HTML components. As a HPC admin, navigate to \"Application Management\" tab and start to build the HTML form.","title":"Create the HTML form"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#job_name-job_project-bucket_to_archive-input_file-and-parameter1","text":"Drag \"Text Field\" component from the left section to add it to the form. Configure the widget and configure the Name settings (red) with the variable name associated (job_name in our example) In the example below, the value entered by the user for job_name will be sent to the job script and retrieved via %job_name% Repeat the same operation for %job_project%, %bucket_to_archive%, %parameter1% and %input_file% Note %input_file% will automatically be configured with the path of the input file selected by the user","title":"%job_name% , %job_project%, %bucket_to_archive%, %input_file% and %parameter1%"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#instance_type","text":"We want to enforce the instance type to be c5.18xlarge. To do that, you can simply hardcode the information on the job script or create a \"Hidden Input\" parameter. The red section references to the variable name and the green section is the variable value.","title":"%instance_type%"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#cpus","text":"For %cpus% variable, we recommend using the \"Number\" component Specify a name which match your variable name (red), pick the default value (green) then choose the Min/Max/Step values allowed (blue)","title":"%cpus%"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#version","text":"Assuming your application hierarchy is as follow: \u2514\u2500\u2500 /apps \u2514\u2500\u2500 /softwarename \u251c\u2500\u2500 v2020 \u251c\u2500\u2500 v2019 \u2514\u2500\u2500 v2018 This time, we recommend you using the \"Select\" component: Similarly to the previous examples, check the \"Required\" checkbox, map the \"Name\" to your variable name (%version%) and add labels (green) and their associated values (blue) Note Use autocomplete if you have a large number of entry","title":"%version%"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#backup_enabled","text":"%backup_enabled% is a boolean which enable (1) or disable (0) archival of the job output data to S3. This time use \"Radio Group\" component and configure the different values:","title":"%backup_enabled%"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#configure-the-job-script","text":"Once your HTML form is done, simply click \"Step2\" and copy/paste your job script Select your interpreter Since this script is expected to be triggered by PBS, keep the default \"will use qsub\" option.","title":"Configure the job script"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#save-your-profile","text":"Finally, navigate to step3, choose an name, upload a thumbnail if needed (optional) and click \"Create this application\"","title":"Save your profile"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#submit-a-test-job","text":"To submit a job, first navigate to \"Submit Job (Web)\" on the left sidebar. Choose your input file and click \"Use as Simulation input\" icon Select the application you want to run Fill out the HTML form generated during the previous step with your own inputs. Real time cost estimate The cost of your simulation is calculated in real-time based on the resources you are specifying Nodes Count SOCA also determine the number of nodes to provision automatically based on the instance type and cpus requested. In this example, the instance is c5.18xlarge (36 cores) and the number of CPUs requested by the user is 72. SOCA automatically detect these values and determine the number of instances to provision is 2 Once done, click \"Submit Job\" and you job will be submitted to the queue. Inputs sanitization SOCA automatically sanitize your inputs when required (remove space, special characters etc ...). In this example, the job name specified was \"My Super Job\" and was corrected to \"MySuperJob\" due some scheduler limitations. You can verify the input file generated by clicking \"Job Directory\": Then select \"View or Edit this File\" icon (3 rd from the left) This will open the submit file created by the web form. You can verify the output generated by your form is correct and/or troubleshoot any potential issue","title":"Submit a test job"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#delete-job","text":"To delete a job, simply navigate to \"My Job Queue\" section and click on the \"Kill\" button","title":"Delete job"},{"location":"web-interface/submit-hpc-jobs-web-based-interface/#what-if-i-want-to-run-a-linux-scriptcommand","text":"If you want your job script to use regular bash interpreter (and not qsub), simple select \"This is a Linux script\". In other words, the output generated by your HTML world will be a simple bash script and SOCA will run /bin/bash job_submit.sh command.","title":"What if I want to run a Linux script/command"},{"location":"workshops/","text":"About \u00b6 Welcome to the AWS workshop and lab content portal for Scale-Out Computing on AWS! Here you will find a collection of workshops and other hands-on content aimed at helping you gain an understanding of how to deploy and operate an elastic, multiuser environment for computationally intensive workflows on the AWS Cloud. The resources on this site include a collection of easy to follow instructions with examples, templates to help you get started and scripts automating tasks supporting the hands-on labs. Prior expertise with AWS and HPC workloads is helpful but not required to complete the labs.","title":"About"},{"location":"workshops/#about","text":"Welcome to the AWS workshop and lab content portal for Scale-Out Computing on AWS! Here you will find a collection of workshops and other hands-on content aimed at helping you gain an understanding of how to deploy and operate an elastic, multiuser environment for computationally intensive workflows on the AWS Cloud. The resources on this site include a collection of easy to follow instructions with examples, templates to help you get started and scripts automating tasks supporting the hands-on labs. Prior expertise with AWS and HPC workloads is helpful but not required to complete the labs.","title":"About"},{"location":"workshops/Synopsys-Physical-Verification/","text":"Synopsys Physical Verification with ICV Tutorial Overview \u00b6 Launch a turnkey scale-out compute environment in minutes on AWS \u00b6 The elasticity of the cloud puts virtually unlimited compute capacity at your fingertips, available within minutes. This can enable companies to quickly scale up in ways they couldn't before, which helps them get results faster. In this workshop, you will login to a pre-deployed cluster based on Scale-Out Computing on AWS reference implementation , a solution vetted by AWS Solutions Architects that provides a full-stack, dynamic computing environment that includes a web UI, a workload manager, remote desktops, directory services, analytics dashboards, and budget management. Note This tutorial assumes familiarity with the Linux command line. Lab environment at a glance \u00b6 At its core, this solution implements a scheduler using Amazon Elastic Compute Cloud (Amazon EC2) instance, which leverages AWS CloudFormation and Amazon EC2 Auto Scaling to automatically provision the resources necessary to execute cluster user tasks such as scale-out compute jobs and remote visualization sessions. The solution also deploys Amazon Elastic File System (Amazon EFS) for persistent storage; AWS Lambda functions to verify the required prerequisites and create a default signed certificate for an Application Load Balancer (ALB) to manage access to Desktop Cloud Visualization (DCV) workstation sessions; an Amazon Elasticsearch Service (Amazon ES) cluster to store job and host metrics; and AWS Secrets Manager to store the solution configuration files. The solution also leverages AWS Identity and Access Management (IAM) roles to enforce least privileged access. Due to time limitations for this tutorial session, you will login to a pre-deployed cluster that has been configured to have the following items: Synopsys IC Validator (ICV) software pre-installed, A license server with valid licenses, and Test case to use for ICV Let's get started. Click the Next link in the bottom right corner to move on to the next module.","title":"Synopsys Physical Verification with ICV Tutorial Overview"},{"location":"workshops/Synopsys-Physical-Verification/#synopsys-physical-verification-with-icv-tutorial-overview","text":"","title":"Synopsys Physical Verification with ICV Tutorial Overview"},{"location":"workshops/Synopsys-Physical-Verification/#launch-a-turnkey-scale-out-compute-environment-in-minutes-on-aws","text":"The elasticity of the cloud puts virtually unlimited compute capacity at your fingertips, available within minutes. This can enable companies to quickly scale up in ways they couldn't before, which helps them get results faster. In this workshop, you will login to a pre-deployed cluster based on Scale-Out Computing on AWS reference implementation , a solution vetted by AWS Solutions Architects that provides a full-stack, dynamic computing environment that includes a web UI, a workload manager, remote desktops, directory services, analytics dashboards, and budget management. Note This tutorial assumes familiarity with the Linux command line.","title":"Launch a turnkey scale-out compute environment in minutes on AWS"},{"location":"workshops/Synopsys-Physical-Verification/#lab-environment-at-a-glance","text":"At its core, this solution implements a scheduler using Amazon Elastic Compute Cloud (Amazon EC2) instance, which leverages AWS CloudFormation and Amazon EC2 Auto Scaling to automatically provision the resources necessary to execute cluster user tasks such as scale-out compute jobs and remote visualization sessions. The solution also deploys Amazon Elastic File System (Amazon EFS) for persistent storage; AWS Lambda functions to verify the required prerequisites and create a default signed certificate for an Application Load Balancer (ALB) to manage access to Desktop Cloud Visualization (DCV) workstation sessions; an Amazon Elasticsearch Service (Amazon ES) cluster to store job and host metrics; and AWS Secrets Manager to store the solution configuration files. The solution also leverages AWS Identity and Access Management (IAM) roles to enforce least privileged access. Due to time limitations for this tutorial session, you will login to a pre-deployed cluster that has been configured to have the following items: Synopsys IC Validator (ICV) software pre-installed, A license server with valid licenses, and Test case to use for ICV Let's get started. Click the Next link in the bottom right corner to move on to the next module.","title":"Lab environment at a glance"},{"location":"workshops/Synopsys-Physical-Verification/modules/01-web-login/","text":"Lab 1: Login to SOCA Web UI and Launch Remote Desktop Session \u00b6 The goal of this module is to login to SOCA web interface and start a remote desktop session from which you will run applications and submit jobs into the cluster. You will use the cluster's management portal to start and monitor the session. Step 1: Login to SOCA Web UI \u00b6 Click one of the links below depending on the session you're attending to login to corresponding SOCA web interface. Workshop sessions are not active at this time!! Note Your web browser will warn you about a certificate problem with the site. To open the webpage, you need to authorize the browser to trust the self-signed security certificate. In a production deployment, you would upload a Server Certificate to the Elastic Load Balancer endpoint. Log in to the web UI using the following credentials: username: provided during tutorial session password: provided during tutorial session Step 2: Launch remote desktop server \u00b6 Follow these instructions to start a full remote desktop experience in your new cluster: Click Linux Desktop on the left sidebar. Under Linux Session #1 group: Select CentOS 7 - x86_64 in the Software Stack dropdown menu. Choose 2D - Medium (8 vCPUs - 32GB ram) in the Session Type dropdown menu. Click Launch my Session #1 After you click Launch my session , the SOCA solution will create a new EC2 instance with 8 vCPUs and 32GB of memory and install all desktop required packages such as Gnome and Nice DCV to access a remote desktop session. You will see a message asking you to wait up to 10 minutes before being able to access your remote desktop. Warning Please wait till the desktop instance is ready before moving on to the next step. In this lab you learned how to use SOCA web portal to create a desktop cloud visualization instance so you can access the compute cluster. Click Next once the status of Linux Session #1 changes and you see green button labeled Open Session directly on a browser .","title":"Lab 1: Login to SOCA Web UI and Launch Remote Desktop Session"},{"location":"workshops/Synopsys-Physical-Verification/modules/01-web-login/#lab-1-login-to-soca-web-ui-and-launch-remote-desktop-session","text":"The goal of this module is to login to SOCA web interface and start a remote desktop session from which you will run applications and submit jobs into the cluster. You will use the cluster's management portal to start and monitor the session.","title":"Lab 1: Login to SOCA Web UI and Launch Remote Desktop Session"},{"location":"workshops/Synopsys-Physical-Verification/modules/01-web-login/#step-1-login-to-soca-web-ui","text":"Click one of the links below depending on the session you're attending to login to corresponding SOCA web interface. Workshop sessions are not active at this time!! Note Your web browser will warn you about a certificate problem with the site. To open the webpage, you need to authorize the browser to trust the self-signed security certificate. In a production deployment, you would upload a Server Certificate to the Elastic Load Balancer endpoint. Log in to the web UI using the following credentials: username: provided during tutorial session password: provided during tutorial session","title":"Step 1: Login to SOCA Web UI"},{"location":"workshops/Synopsys-Physical-Verification/modules/01-web-login/#step-2-launch-remote-desktop-server","text":"Follow these instructions to start a full remote desktop experience in your new cluster: Click Linux Desktop on the left sidebar. Under Linux Session #1 group: Select CentOS 7 - x86_64 in the Software Stack dropdown menu. Choose 2D - Medium (8 vCPUs - 32GB ram) in the Session Type dropdown menu. Click Launch my Session #1 After you click Launch my session , the SOCA solution will create a new EC2 instance with 8 vCPUs and 32GB of memory and install all desktop required packages such as Gnome and Nice DCV to access a remote desktop session. You will see a message asking you to wait up to 10 minutes before being able to access your remote desktop. Warning Please wait till the desktop instance is ready before moving on to the next step. In this lab you learned how to use SOCA web portal to create a desktop cloud visualization instance so you can access the compute cluster. Click Next once the status of Linux Session #1 changes and you see green button labeled Open Session directly on a browser .","title":"Step 2: Launch remote desktop server"},{"location":"workshops/Synopsys-Physical-Verification/modules/02-login-copy/","text":"Lab 2: Login to Remote Desktop and Copy Lab Data \u00b6 The goal with this lab is to login to the remote cloud desktop visualization and experience using it. You'll also copy the data required for the subsequent labs. Step 1: Log into your session \u00b6 By now your remote desktop session should be ready and you should see the following under Your Session #1 : Click Open Session directly on a browser to log into the remote desktop session in the cluster. Note You can also access the session with the NICE DCV native clients, which are available for Mac, Linux, and Windows from https://download.nice-dcv.com Start a new terminal session by going to Applications \u2192 System Tools \u2192 Terminal in the desktop manager. Step 2: Copy lab data \u00b6 Create a working directory for your user under /fsxl/ by typing mkdir /fsxl/`whoami` at the command prompt and hit enter. This should create a directory under /fsxl with the username assigned to you earlier in the session Copy the lab data to your working directory by typing cp -r /data/synopsys/lab_data /fsxl/`whoami` at the command prompt and hit enter Note /data is a mount point for Amazon Elastic File System which provides a simple, scalable elastic NFS file system. /fsxl is a mount point for Amazon FSx for Lustre which provides a Lustre file system suitable for high performance computing (HPC) workloads such as EDA Change directory to test case cd /fsxl/`whoami`/lab_data and hit enter Source environment settings by typing source setup.csh and hit enter In this lab you learned how to login to desktop cloud visualiztion instance, and copied the lab data. You've completed this lab. Click Next to move to the next lab.","title":"Lab 2: Login to Remote Desktop and Copy Lab Data"},{"location":"workshops/Synopsys-Physical-Verification/modules/02-login-copy/#lab-2-login-to-remote-desktop-and-copy-lab-data","text":"The goal with this lab is to login to the remote cloud desktop visualization and experience using it. You'll also copy the data required for the subsequent labs.","title":"Lab 2: Login to Remote Desktop and Copy Lab Data"},{"location":"workshops/Synopsys-Physical-Verification/modules/02-login-copy/#step-1-log-into-your-session","text":"By now your remote desktop session should be ready and you should see the following under Your Session #1 : Click Open Session directly on a browser to log into the remote desktop session in the cluster. Note You can also access the session with the NICE DCV native clients, which are available for Mac, Linux, and Windows from https://download.nice-dcv.com Start a new terminal session by going to Applications \u2192 System Tools \u2192 Terminal in the desktop manager.","title":"Step 1: Log into your session"},{"location":"workshops/Synopsys-Physical-Verification/modules/02-login-copy/#step-2-copy-lab-data","text":"Create a working directory for your user under /fsxl/ by typing mkdir /fsxl/`whoami` at the command prompt and hit enter. This should create a directory under /fsxl with the username assigned to you earlier in the session Copy the lab data to your working directory by typing cp -r /data/synopsys/lab_data /fsxl/`whoami` at the command prompt and hit enter Note /data is a mount point for Amazon Elastic File System which provides a simple, scalable elastic NFS file system. /fsxl is a mount point for Amazon FSx for Lustre which provides a Lustre file system suitable for high performance computing (HPC) workloads such as EDA Change directory to test case cd /fsxl/`whoami`/lab_data and hit enter Source environment settings by typing source setup.csh and hit enter In this lab you learned how to login to desktop cloud visualiztion instance, and copied the lab data. You've completed this lab. Click Next to move to the next lab.","title":"Step 2: Copy lab data"},{"location":"workshops/Synopsys-Physical-Verification/modules/03-submit-elasti/","text":"Lab 3: Run ELASTI Test Case \u00b6 Synopsys IC Validator (ICV) has the ability to request CPUs when it needs additional resources and can also relinquish CPUs when it no longer needs them. In this lab, we'll use ICV -host_elastic option to provide a script that ICV can use to dynamically add CPUs when it is running on AWS. Step 1: Run the ELASTI test case \u00b6 Change directory to ELASTI cd ELASTI and hit enter. Run the test case by typing ./runme then hit enter. You should see a message indicating that a job has been submitted to the cluster scheduler (OpenPBS in this case) with the corresponding job id. Step 2: Watch job status \u00b6 Run the qstat command to view the status of the jobs. You can also view job status by clicking on My Job Queue in the left side navigation bar in SOCA portal under PROFILE section as shown in the screen shot below: The result should be similar to the screen shot below which is a table format showing job id, name, status, etc...: You can run the pbsnodes -aSjL command to see the EC2 instances that have joined the cluster. Initially, the nodes will be in state-unknown,down till they boot-up and join the queue. Note SOCA automation scripts are configured to monitor the status of the queues every minute. It typically takes 5-6 minutes to launch a new EC2 instance, boot the operating system, configure it to join the cluster, and have the assigned job to start running. Step 3: Monitor ELASTI job \u00b6 Monitor the status of ELASTI job by refreshing the My Job Queue page in SOCA portal and look for the Status column for the job with ELASTI under Name column. You can also monitor the status of the jobs in the terminal by typing watch -n 10 \"qstat -u `whoami`\" command which will keep monitoring the status of the jobs every 10 seconds. You'll need to wait for the S column represeting the status of the job correspoding to the one with ELASTI under Name column. Once the job is in the running state ( S column changes to R in the terminal or Status column changes to RUNNING on My Job Queue page in SOCA portal), hit CTRL+C in the terminal to exit of the watch command. Then look inside elastic_run/run_details directory for a file that matches *.dp.log by typing ls elastic_run/run_details/*.dp.log . Wait until this log file is created before moving to the next step. Step 4: Run ICV Dashboard to monitor progress of the ELASTI test case \u00b6 Monitor the progress of the ELASTI test case by typing this command: icv_dashboard -keys hSdSVaCfhpv elastic_run/run_details/saed32nm_1p9m_drc_rules.dp.log & As the job progresses, ICV will request more CPU resources or release idle resources. In this example, it will submit a new job to the cluster so it can obtain additional resources dynamically. You an monitor the status of the jobs by typing watch -n 10 \"qstat -u `whoami`\" command in the terminal to keep monitoring the status of jobs every 10 seconds. Depending on resource availability in the cluster, SOCA might need to create additional instances for the new job. Once the resources become available and the job status changes to running, the CPU history section in the ICV dashboard would be updated to reflect the additioanl CPUs as shown below. ICV will repeat this process one more time and the final ICV dashboard should be similar to the screenshot below. Note You don't have to wait till the completion of this test case and can move to the next lab. In this lab you learned how to submit ICV ELASTI test case jobs to the cluster, how to monitor the status of these jobs, and how to visualize the job progress using ICV dashboard. Click Next to move to the next lab.","title":"Lab 3: Run ELASTI Test Case"},{"location":"workshops/Synopsys-Physical-Verification/modules/03-submit-elasti/#lab-3-run-elasti-test-case","text":"Synopsys IC Validator (ICV) has the ability to request CPUs when it needs additional resources and can also relinquish CPUs when it no longer needs them. In this lab, we'll use ICV -host_elastic option to provide a script that ICV can use to dynamically add CPUs when it is running on AWS.","title":"Lab 3: Run ELASTI Test Case"},{"location":"workshops/Synopsys-Physical-Verification/modules/03-submit-elasti/#step-1-run-the-elasti-test-case","text":"Change directory to ELASTI cd ELASTI and hit enter. Run the test case by typing ./runme then hit enter. You should see a message indicating that a job has been submitted to the cluster scheduler (OpenPBS in this case) with the corresponding job id.","title":"Step 1: Run the ELASTI test case"},{"location":"workshops/Synopsys-Physical-Verification/modules/03-submit-elasti/#step-2-watch-job-status","text":"Run the qstat command to view the status of the jobs. You can also view job status by clicking on My Job Queue in the left side navigation bar in SOCA portal under PROFILE section as shown in the screen shot below: The result should be similar to the screen shot below which is a table format showing job id, name, status, etc...: You can run the pbsnodes -aSjL command to see the EC2 instances that have joined the cluster. Initially, the nodes will be in state-unknown,down till they boot-up and join the queue. Note SOCA automation scripts are configured to monitor the status of the queues every minute. It typically takes 5-6 minutes to launch a new EC2 instance, boot the operating system, configure it to join the cluster, and have the assigned job to start running.","title":"Step 2: Watch job status"},{"location":"workshops/Synopsys-Physical-Verification/modules/03-submit-elasti/#step-3-monitor-elasti-job","text":"Monitor the status of ELASTI job by refreshing the My Job Queue page in SOCA portal and look for the Status column for the job with ELASTI under Name column. You can also monitor the status of the jobs in the terminal by typing watch -n 10 \"qstat -u `whoami`\" command which will keep monitoring the status of the jobs every 10 seconds. You'll need to wait for the S column represeting the status of the job correspoding to the one with ELASTI under Name column. Once the job is in the running state ( S column changes to R in the terminal or Status column changes to RUNNING on My Job Queue page in SOCA portal), hit CTRL+C in the terminal to exit of the watch command. Then look inside elastic_run/run_details directory for a file that matches *.dp.log by typing ls elastic_run/run_details/*.dp.log . Wait until this log file is created before moving to the next step.","title":"Step 3: Monitor ELASTI job"},{"location":"workshops/Synopsys-Physical-Verification/modules/03-submit-elasti/#step-4-run-icv-dashboard-to-monitor-progress-of-the-elasti-test-case","text":"Monitor the progress of the ELASTI test case by typing this command: icv_dashboard -keys hSdSVaCfhpv elastic_run/run_details/saed32nm_1p9m_drc_rules.dp.log & As the job progresses, ICV will request more CPU resources or release idle resources. In this example, it will submit a new job to the cluster so it can obtain additional resources dynamically. You an monitor the status of the jobs by typing watch -n 10 \"qstat -u `whoami`\" command in the terminal to keep monitoring the status of jobs every 10 seconds. Depending on resource availability in the cluster, SOCA might need to create additional instances for the new job. Once the resources become available and the job status changes to running, the CPU history section in the ICV dashboard would be updated to reflect the additioanl CPUs as shown below. ICV will repeat this process one more time and the final ICV dashboard should be similar to the screenshot below. Note You don't have to wait till the completion of this test case and can move to the next lab. In this lab you learned how to submit ICV ELASTI test case jobs to the cluster, how to monitor the status of these jobs, and how to visualize the job progress using ICV dashboard. Click Next to move to the next lab.","title":"Step 4: Run ICV Dashboard to monitor progress of the ELASTI test case"},{"location":"workshops/Synopsys-Physical-Verification/modules/04-submit-explorer/","text":"Lab 4: Run EXPLORER Test Case \u00b6 Explorer analysis gives users options to quickly check on multiple design weaknesses. ICV DRC Explorer runs on dynamically selected parts of signoff runset. This mode comes very handy while running designs that are still in maturing state. In the lab you will see DRCs caused from instance overlap, DRCs caused from re-use of old blocks that have diffrent width and spacing requirements. IC Validator offers Explorer functions both on DRC and LVS. This lab only talks about DRC Explorer. Step 1: Run the EXPLORER test case \u00b6 Change directory to EXPLORER test case by typing cd ../EXPLORER and hit enter. Run the test case by typing ./runme then hit enter. ICV should run directly on the remote desktop host (i.e, doesn't submit a job to the scheduler) and should generate a log similar to this Step 2: Launch ICV Workbench \u00b6 Launch ICV Workbench for quick and easy viewing of layout data typing icvwb EXP_TOP.oas & and hit enter. Step 3: Debug with ICV_VUE \u00b6 To launch VUE from ICVWB, under the User mode click on ICV_VUE Load the DRC explorer vue file by browsing to EXP_TOP.vue. Click on the browse icon then double click on explorer_run directory and select EXP_TOP.vue Step 4: Heat Map \u00b6 In the VUE Window, click on Heat Map to start debugging. The Error Heat Map is a graphical interface used to visualize error distributions. Once the tab is opened, heat map quickly shows where all diagnostic warnings are flagged Note The color definitions window displays the heat map color definitions ranging from red (maximum error count) to dark blue (minimum error count) You can also change the Violation Category Mode to \"Clustered Errors\" where errors are clustered together so that multiple rules can be debugged concurrently Zoom-in to an error hotspot in the heatmap, to view both the heat map and design shapes in ICVWB, overlay the heat map. Click on Overlay Heat Map icon or right click in the layout area and select Overlay Heat Map from the menu. You can change the highlight pattern from the drop-down menu. Overlaying marker to the layout (showing possibility of cell overlapping) You can highlight error marker from the heat map. Select M4.S.1 rule from the violation section. Then right click on the heat map window, then click on \"Highlight top-cell error in current window\". This option highlights errors for violations in the current zoomed violation heat map window. Overlaying to the layout You can adjust the size of the heat map grid. Slide the bar to the left to see larger grids (coarser) and to the right to see smaller grids (finer). You can toggle the info button for grid statistics. You can search and filter single or multiple violations by name or layer under the search option In this lab you learned how to run ICV DRC Explorer. Close VUE and ICV Workbench windows. Click Next to move to the next lab.","title":"Lab 4: Run EXPLORER Test Case"},{"location":"workshops/Synopsys-Physical-Verification/modules/04-submit-explorer/#lab-4-run-explorer-test-case","text":"Explorer analysis gives users options to quickly check on multiple design weaknesses. ICV DRC Explorer runs on dynamically selected parts of signoff runset. This mode comes very handy while running designs that are still in maturing state. In the lab you will see DRCs caused from instance overlap, DRCs caused from re-use of old blocks that have diffrent width and spacing requirements. IC Validator offers Explorer functions both on DRC and LVS. This lab only talks about DRC Explorer.","title":"Lab 4: Run EXPLORER Test Case"},{"location":"workshops/Synopsys-Physical-Verification/modules/04-submit-explorer/#step-1-run-the-explorer-test-case","text":"Change directory to EXPLORER test case by typing cd ../EXPLORER and hit enter. Run the test case by typing ./runme then hit enter. ICV should run directly on the remote desktop host (i.e, doesn't submit a job to the scheduler) and should generate a log similar to this","title":"Step 1: Run the EXPLORER test case"},{"location":"workshops/Synopsys-Physical-Verification/modules/04-submit-explorer/#step-2-launch-icv-workbench","text":"Launch ICV Workbench for quick and easy viewing of layout data typing icvwb EXP_TOP.oas & and hit enter.","title":"Step 2: Launch ICV Workbench"},{"location":"workshops/Synopsys-Physical-Verification/modules/04-submit-explorer/#step-3-debug-with-icv_vue","text":"To launch VUE from ICVWB, under the User mode click on ICV_VUE Load the DRC explorer vue file by browsing to EXP_TOP.vue. Click on the browse icon then double click on explorer_run directory and select EXP_TOP.vue","title":"Step 3: Debug with ICV_VUE"},{"location":"workshops/Synopsys-Physical-Verification/modules/04-submit-explorer/#step-4-heat-map","text":"In the VUE Window, click on Heat Map to start debugging. The Error Heat Map is a graphical interface used to visualize error distributions. Once the tab is opened, heat map quickly shows where all diagnostic warnings are flagged Note The color definitions window displays the heat map color definitions ranging from red (maximum error count) to dark blue (minimum error count) You can also change the Violation Category Mode to \"Clustered Errors\" where errors are clustered together so that multiple rules can be debugged concurrently Zoom-in to an error hotspot in the heatmap, to view both the heat map and design shapes in ICVWB, overlay the heat map. Click on Overlay Heat Map icon or right click in the layout area and select Overlay Heat Map from the menu. You can change the highlight pattern from the drop-down menu. Overlaying marker to the layout (showing possibility of cell overlapping) You can highlight error marker from the heat map. Select M4.S.1 rule from the violation section. Then right click on the heat map window, then click on \"Highlight top-cell error in current window\". This option highlights errors for violations in the current zoomed violation heat map window. Overlaying to the layout You can adjust the size of the heat map grid. Slide the bar to the left to see larger grids (coarser) and to the right to see smaller grids (finer). You can toggle the info button for grid statistics. You can search and filter single or multiple violations by name or layer under the search option In this lab you learned how to run ICV DRC Explorer. Close VUE and ICV Workbench windows. Click Next to move to the next lab.","title":"Step 4: Heat Map"},{"location":"workshops/Synopsys-Physical-Verification/modules/05-submit-blackbox/","text":"Lab 5: Run BBOX Test Case \u00b6 The idea for this lab is to show you how to validate designs before all the building blocks are completed which allows for parallel development. Step 1: Run the test case without black_box configuration \u00b6 Change directory to BBOX test case by typing cd ../BBOX and hit enter. Edit the runme script to uncomment the command under #cmd1 (make sure that cmd2 and cmd3 are commented out). You can use vim, gvim, or gedit to edit the file. This command runs without defining -D black_box variable which should show the original DRCs reported. Run the test case by typing ./runme then hit enter. After the run is completed, open ICV Workbench by typing icvwb TOP_BB.oas . Then click on ICV_VUE and browse to black_box_run/ORCA_TOP.vue Click on DRC Errors and notice there are 399 errors for cell SRAMLP2RW32X4 . Close VUE and ICV Worbench windows. Step 2: Run with black_box configuration \u00b6 Edit the runme script to uncomment the command under #cmd2 (comment cmd1 and cmd3). This command defines -D black_box and provides an options set in black_box.rs file passed via -runset_config option. Run the test case by typing ./runme then hit enter. After the run is completed, open ICV Workbench by typing icvwb TOP_BB.oas . Then click on ICV_VUE and browse to black_box_run/ORCA_TOP.vue Click on DRC Errors and notice there are no violations reported for SRAMLP2RW32x4 as black_box is set for the cell. Close VUE and ICV Worbench windows. Step 3: black_box with ambit \u00b6 Edit the runme script to uncomment the command under #cmd3 (comment cmd1 and cmd2). This command provides for heat map generation. VUE automatically reads and creates the heat map tab in the VUE GUI. -hm \u2192 top-cell heat map -hm_cell \u2192 subcell heat map Edit black_box.rs configuration file by uncommenting the following lines: ambit \u2192 this will enable ICV to read polygons till the value provided from the reference layer. clip_region = false \u2192 to consider the entire polygon which is interacting with the ambit region. By default, it is true i.e. to clip polygons till the exclude region. Run the test case by typing ./runme then hit enter. After the run is completed, open ICV Workbench by typing icvwb TOP_BB.oas . Then click on ICV_VUE and browse to black_box_run/ORCA_TOP.vue Click on DRC Errors and notice there are 58 violations reported for black_box cell SRAMLP2RW32x4 with ambit = 0.2 Step 4: Heat Map \u00b6 Click the Heat Map tab to start debugging. The Error Heat Map is a graphical interface used to visualize error distributions. Once the tab is opened, heat map quickly shows where all diagnostic warnings are flaged. Change the cell to \"SRAMLP2RW32x4\" Zoom-in to the upper left error hotspot in heatmap. To view both Heat Maps and design shapes in ICVWB, overlay the heat map. Select \"Overlay Heat Map\" from the options list in the Violation heat Map window. Overlay heat map to the layout Close VUE and ICV Worbench windows. In this lab you learned how to run ICV for the black box test case. Click Next to move to the wrap-up section.","title":"Lab 5: Run BBOX Test Case"},{"location":"workshops/Synopsys-Physical-Verification/modules/05-submit-blackbox/#lab-5-run-bbox-test-case","text":"The idea for this lab is to show you how to validate designs before all the building blocks are completed which allows for parallel development.","title":"Lab 5: Run BBOX Test Case"},{"location":"workshops/Synopsys-Physical-Verification/modules/05-submit-blackbox/#step-1-run-the-test-case-without-black_box-configuration","text":"Change directory to BBOX test case by typing cd ../BBOX and hit enter. Edit the runme script to uncomment the command under #cmd1 (make sure that cmd2 and cmd3 are commented out). You can use vim, gvim, or gedit to edit the file. This command runs without defining -D black_box variable which should show the original DRCs reported. Run the test case by typing ./runme then hit enter. After the run is completed, open ICV Workbench by typing icvwb TOP_BB.oas . Then click on ICV_VUE and browse to black_box_run/ORCA_TOP.vue Click on DRC Errors and notice there are 399 errors for cell SRAMLP2RW32X4 . Close VUE and ICV Worbench windows.","title":"Step 1: Run the test case without black_box configuration"},{"location":"workshops/Synopsys-Physical-Verification/modules/05-submit-blackbox/#step-2-run-with-black_box-configuration","text":"Edit the runme script to uncomment the command under #cmd2 (comment cmd1 and cmd3). This command defines -D black_box and provides an options set in black_box.rs file passed via -runset_config option. Run the test case by typing ./runme then hit enter. After the run is completed, open ICV Workbench by typing icvwb TOP_BB.oas . Then click on ICV_VUE and browse to black_box_run/ORCA_TOP.vue Click on DRC Errors and notice there are no violations reported for SRAMLP2RW32x4 as black_box is set for the cell. Close VUE and ICV Worbench windows.","title":"Step 2: Run with black_box configuration"},{"location":"workshops/Synopsys-Physical-Verification/modules/05-submit-blackbox/#step-3-black_box-with-ambit","text":"Edit the runme script to uncomment the command under #cmd3 (comment cmd1 and cmd2). This command provides for heat map generation. VUE automatically reads and creates the heat map tab in the VUE GUI. -hm \u2192 top-cell heat map -hm_cell \u2192 subcell heat map Edit black_box.rs configuration file by uncommenting the following lines: ambit \u2192 this will enable ICV to read polygons till the value provided from the reference layer. clip_region = false \u2192 to consider the entire polygon which is interacting with the ambit region. By default, it is true i.e. to clip polygons till the exclude region. Run the test case by typing ./runme then hit enter. After the run is completed, open ICV Workbench by typing icvwb TOP_BB.oas . Then click on ICV_VUE and browse to black_box_run/ORCA_TOP.vue Click on DRC Errors and notice there are 58 violations reported for black_box cell SRAMLP2RW32x4 with ambit = 0.2","title":"Step 3: black_box with ambit"},{"location":"workshops/Synopsys-Physical-Verification/modules/05-submit-blackbox/#step-4-heat-map","text":"Click the Heat Map tab to start debugging. The Error Heat Map is a graphical interface used to visualize error distributions. Once the tab is opened, heat map quickly shows where all diagnostic warnings are flaged. Change the cell to \"SRAMLP2RW32x4\" Zoom-in to the upper left error hotspot in heatmap. To view both Heat Maps and design shapes in ICVWB, overlay the heat map. Select \"Overlay Heat Map\" from the options list in the Violation heat Map window. Overlay heat map to the layout Close VUE and ICV Worbench windows. In this lab you learned how to run ICV for the black box test case. Click Next to move to the wrap-up section.","title":"Step 4: Heat Map"},{"location":"workshops/Synopsys-Physical-Verification/modules/09-wrap-up/","text":"Wrap-up \u00b6 Congratulations! You've completed this Tutorial. Clean up \u00b6 Go back to the SOCA portal and click on the Kill red button above the Linux Desktop Session. Then click on the checkbox I am sure I want to terminate this session then click on Terminate LinuxDesktop1 Next steps \u00b6 Please complete the session survey.","title":"Wrap-up"},{"location":"workshops/Synopsys-Physical-Verification/modules/09-wrap-up/#wrap-up","text":"Congratulations! You've completed this Tutorial.","title":"Wrap-up"},{"location":"workshops/Synopsys-Physical-Verification/modules/09-wrap-up/#clean-up","text":"Go back to the SOCA portal and click on the Kill red button above the Linux Desktop Session. Then click on the checkbox I am sure I want to terminate this session then click on Terminate LinuxDesktop1","title":"Clean up"},{"location":"workshops/Synopsys-Physical-Verification/modules/09-wrap-up/#next-steps","text":"Please complete the session survey.","title":"Next steps"},{"location":"workshops/Synopsys-Verification/","text":"Synopsys Verification Tutorial Overview \u00b6 Launch a turnkey scale-out compute environment in minutes on AWS \u00b6 The elasticity of the cloud puts virtually unlimited compute capacity at your fingertips, available within minutes. This can enable companies to quickly scale up in ways they couldn't before, which helps them get results faster. In this workshop, you will deploy the Scale-Out Computing on AWS reference implementation , a solution vetted by AWS Solutions Architects that provides a full-stack, dynamic computing environment that includes a web UI, a workload manager, remote desktops, directory services, analytics dashboards, and budget management. Note This tutorial assumes familiarity with the Linux command line. Lab environment at a glance \u00b6 At its core, this solution implements a scheduler Amazon Elastic Compute Cloud (Amazon EC2) instance, which leverages AWS CloudFormation and Amazon EC2 Auto Scaling to automatically provision the resources necessary to execute cluster user tasks such as scale-out compute jobs and remote visualization sessions. The solution also deploys Amazon Elastic File System (Amazon EFS) for persistent storage; AWS Lambda functions to verify the required prerequisites and create a default signed certificate for an Application Load Balancer (ALB) to manage access to Desktop Cloud Visualization (DCV) workstation sessions; an Amazon Elasticsearch Service (Amazon ES) cluster to store job and host metrics; and AWS Secrets Manager to store the solution configuration files. The solution also leverages AWS Identity and Access Management (IAM) roles to enforce least privileged access. Let's get started. Click the Next link in the bottom right corner to move on to the next module.","title":"Synopsys Verification Tutorial Overview"},{"location":"workshops/Synopsys-Verification/#synopsys-verification-tutorial-overview","text":"","title":"Synopsys Verification Tutorial Overview"},{"location":"workshops/Synopsys-Verification/#launch-a-turnkey-scale-out-compute-environment-in-minutes-on-aws","text":"The elasticity of the cloud puts virtually unlimited compute capacity at your fingertips, available within minutes. This can enable companies to quickly scale up in ways they couldn't before, which helps them get results faster. In this workshop, you will deploy the Scale-Out Computing on AWS reference implementation , a solution vetted by AWS Solutions Architects that provides a full-stack, dynamic computing environment that includes a web UI, a workload manager, remote desktops, directory services, analytics dashboards, and budget management. Note This tutorial assumes familiarity with the Linux command line.","title":"Launch a turnkey scale-out compute environment in minutes on AWS"},{"location":"workshops/Synopsys-Verification/#lab-environment-at-a-glance","text":"At its core, this solution implements a scheduler Amazon Elastic Compute Cloud (Amazon EC2) instance, which leverages AWS CloudFormation and Amazon EC2 Auto Scaling to automatically provision the resources necessary to execute cluster user tasks such as scale-out compute jobs and remote visualization sessions. The solution also deploys Amazon Elastic File System (Amazon EFS) for persistent storage; AWS Lambda functions to verify the required prerequisites and create a default signed certificate for an Application Load Balancer (ALB) to manage access to Desktop Cloud Visualization (DCV) workstation sessions; an Amazon Elasticsearch Service (Amazon ES) cluster to store job and host metrics; and AWS Secrets Manager to store the solution configuration files. The solution also leverages AWS Identity and Access Management (IAM) roles to enforce least privileged access. Let's get started. Click the Next link in the bottom right corner to move on to the next module.","title":"Lab environment at a glance"},{"location":"workshops/Synopsys-Verification/Section-1/00-getting-started/","text":"Getting Started \u00b6 To begin, you'll log into a temporary AWS account that will be provided to you for this workshop. Accessing your AWS account \u00b6 At the beginning of the workshop, you will be given a 12-character access code . This access code grants you access to a temporary AWS account that you'll use for this workshop. Step 1. Log in \u00b6 Go to Event Engine Dashboard , and enter the access code in the Team Hash field. Click Accept Terms & Login . Step 2. Sign in with... \u00b6 Click on Email One-Time Password (OTP) and provide your email address on the next screen. Then check your email and provide the one-time password to verify access Step 3. Get Credentials \u00b6 On the Team Dashboard , click SSH Key to download the SSH Keypair PEM file. You'll use this file later to SSH into an EC2 instance. If your using a Mac, change permissions of the PEM file that you just downloaded. This is an SSH security requirement. chmod 600 /path/to/file.pem Next, click AWS Console to begin the login process to the AWS account. Step 4. Open AWS Console \u00b6 Click Open AWS Console . For this workshop, you will not need the Credentials or CLI Snippets Awesome! Now that you are logged into your temporary AWS account, we can start the labs. Click Next .","title":"Getting Started"},{"location":"workshops/Synopsys-Verification/Section-1/00-getting-started/#getting-started","text":"To begin, you'll log into a temporary AWS account that will be provided to you for this workshop.","title":"Getting Started"},{"location":"workshops/Synopsys-Verification/Section-1/00-getting-started/#accessing-your-aws-account","text":"At the beginning of the workshop, you will be given a 12-character access code . This access code grants you access to a temporary AWS account that you'll use for this workshop.","title":"Accessing your AWS account"},{"location":"workshops/Synopsys-Verification/Section-1/00-getting-started/#step-1-log-in","text":"Go to Event Engine Dashboard , and enter the access code in the Team Hash field. Click Accept Terms & Login .","title":"Step 1. Log in"},{"location":"workshops/Synopsys-Verification/Section-1/00-getting-started/#step-2-sign-in-with","text":"Click on Email One-Time Password (OTP) and provide your email address on the next screen. Then check your email and provide the one-time password to verify access","title":"Step 2. Sign in with..."},{"location":"workshops/Synopsys-Verification/Section-1/00-getting-started/#step-3-get-credentials","text":"On the Team Dashboard , click SSH Key to download the SSH Keypair PEM file. You'll use this file later to SSH into an EC2 instance. If your using a Mac, change permissions of the PEM file that you just downloaded. This is an SSH security requirement. chmod 600 /path/to/file.pem Next, click AWS Console to begin the login process to the AWS account.","title":"Step 3. Get Credentials"},{"location":"workshops/Synopsys-Verification/Section-1/00-getting-started/#step-4-open-aws-console","text":"Click Open AWS Console . For this workshop, you will not need the Credentials or CLI Snippets Awesome! Now that you are logged into your temporary AWS account, we can start the labs. Click Next .","title":"Step 4. Open AWS Console"},{"location":"workshops/Synopsys-Verification/Section-1/01-deploy-env/","text":"Lab 1: Deploy Environment \u00b6 We want you to have hands-on experience deploying your own cluster, and in this module we will have you walk through the process of launching a cluster in your temporary AWS account. This cluster will be provisioned in the background. For the subsequent modules you'll access a pre-built cluster where EDA tools and licenses have been provisioned. Step 1: Launch stack \u00b6 This automated AWS CloudFormation template deploys a scale-out computing environment in the AWS Cloud. Sign in to the AWS Management Console and click the Launch Stack link in the instructions below to launch the scale-out-computing-on-aws AWS CloudFormation template. Launch Stack <--- CLICK HERE Verify the launch region is Oregon Important The template must be launched in Oregon for this workshop. On the Create stack page, you should see the template URL in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. We recommend naming it \"tutorial\". Warning The stack name must be less than 20 characters and must be lower-case only. Under Parameters , modify the the last four parameters, which are marked with REQUIRED . Leave all other fields with their default values. These are variables passed the CloudFormation automation that deploys the environment. Parameter Default Description Install Location Installer S3 Bucket solutions-reference The default AWS bucket name. Do not change this parameter. Installer Folder scale-out-computing-on-aws/v2.6.1 The default AWS folder name. Do not change this parameter. Linux Distribution Linux Distribution AmazonLinux2 The preferred Linux distribution for the scheduler and compute instances. Do not change this parameter. Custom AMI If using a customized Amazon Machine Image, enter the ID. Leave this field blank. Network and Security EC2 Instance Type for Scheduler node m5.xlarge The instance type for the scheduler. Do not change this parameter. VPC Cluster CIDR 10.0.0.0/16 Choose the CIDR (/16) block for the VPC. Do not change this parameter. IP Address See description REQUIRED The public-facing IP address that is permitted to log into the environment. We recommend you change it to your public-facing IP address. You can find your public-facing IP address at http://checkip.amazonaws.com then add the /32 suffix to the IP number. Key Pair Name ee-default-keypair REQUIRED Select the ee-default-keypair provided by the workshop. Default LDAP User User Name REQUIRED Set a username for the default cluster user. Password REQUIRED Set a password for the default cluster user. (5 characters minimum, uppercase/lowercase/digit only) Choose Next . On the Configure Stack Options page, choose Next . On the Review page, review the settings and check the two boxes acknowledging that the template will create AWS Identity and Access Management (IAM) resources and might require the CAPABILITY_AUTO_EXPAND capability. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should see a status of CREATE_COMPLETE in approximately 35 minutes. By now you've learned how to deploy Scale-Out Computing on AWS to create a compute cluster for EDA Workloads in an AWS account. For the remaining portion of the this tutorial, you'll login to a different pre-built cluster that has the following items: Synopsys VCS and Verdi software pre-installed, A license server with valid licenses, and Test case to use for VCS and Verdi You can now move on to the next lab. Click Next .","title":"Lab 1: Deploy Environment"},{"location":"workshops/Synopsys-Verification/Section-1/01-deploy-env/#lab-1-deploy-environment","text":"We want you to have hands-on experience deploying your own cluster, and in this module we will have you walk through the process of launching a cluster in your temporary AWS account. This cluster will be provisioned in the background. For the subsequent modules you'll access a pre-built cluster where EDA tools and licenses have been provisioned.","title":"Lab 1: Deploy Environment"},{"location":"workshops/Synopsys-Verification/Section-1/01-deploy-env/#step-1-launch-stack","text":"This automated AWS CloudFormation template deploys a scale-out computing environment in the AWS Cloud. Sign in to the AWS Management Console and click the Launch Stack link in the instructions below to launch the scale-out-computing-on-aws AWS CloudFormation template. Launch Stack <--- CLICK HERE Verify the launch region is Oregon Important The template must be launched in Oregon for this workshop. On the Create stack page, you should see the template URL in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. We recommend naming it \"tutorial\". Warning The stack name must be less than 20 characters and must be lower-case only. Under Parameters , modify the the last four parameters, which are marked with REQUIRED . Leave all other fields with their default values. These are variables passed the CloudFormation automation that deploys the environment. Parameter Default Description Install Location Installer S3 Bucket solutions-reference The default AWS bucket name. Do not change this parameter. Installer Folder scale-out-computing-on-aws/v2.6.1 The default AWS folder name. Do not change this parameter. Linux Distribution Linux Distribution AmazonLinux2 The preferred Linux distribution for the scheduler and compute instances. Do not change this parameter. Custom AMI If using a customized Amazon Machine Image, enter the ID. Leave this field blank. Network and Security EC2 Instance Type for Scheduler node m5.xlarge The instance type for the scheduler. Do not change this parameter. VPC Cluster CIDR 10.0.0.0/16 Choose the CIDR (/16) block for the VPC. Do not change this parameter. IP Address See description REQUIRED The public-facing IP address that is permitted to log into the environment. We recommend you change it to your public-facing IP address. You can find your public-facing IP address at http://checkip.amazonaws.com then add the /32 suffix to the IP number. Key Pair Name ee-default-keypair REQUIRED Select the ee-default-keypair provided by the workshop. Default LDAP User User Name REQUIRED Set a username for the default cluster user. Password REQUIRED Set a password for the default cluster user. (5 characters minimum, uppercase/lowercase/digit only) Choose Next . On the Configure Stack Options page, choose Next . On the Review page, review the settings and check the two boxes acknowledging that the template will create AWS Identity and Access Management (IAM) resources and might require the CAPABILITY_AUTO_EXPAND capability. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should see a status of CREATE_COMPLETE in approximately 35 minutes. By now you've learned how to deploy Scale-Out Computing on AWS to create a compute cluster for EDA Workloads in an AWS account. For the remaining portion of the this tutorial, you'll login to a different pre-built cluster that has the following items: Synopsys VCS and Verdi software pre-installed, A license server with valid licenses, and Test case to use for VCS and Verdi You can now move on to the next lab. Click Next .","title":"Step 1: Launch stack"},{"location":"workshops/Synopsys-Verification/Section-2/02-web-login/","text":"Lab 2: Login to SOCA Web UI and Launch Remote Desktop Session \u00b6 The goal of this module is to login to SOCA web interface and start a remote desktop session from which you will run applications and submit jobs into the cluster. You will use the cluster's management portal to start and monitor the session. Step 1: Login to SOCA Web UI \u00b6 Click one of the links below depending on the session you're attending to login to corresponding SOCA web interface Workshop sessions are not active at this time!! Note Your web browser will warn you about a certificate problem with the site. To open the webpage, you need to authorize the browser to trust the self-signed security certificate. In a production deployment, you would upload a Server Certificate to the Elastic Load Balancer endpoint. Log in to the web UI using the following credentials: username: provided during tutorial session password: provided during tutorial session Step 2: Launch remote desktop server \u00b6 Follow these instructions to start a full remote desktop experience in your new cluster: Click Linux Desktop on the left sidebar. Under Linux Session #1 group: Select CentOS 7 - x86_64 in the Software Stack dropdown menu. Choose 2D - Medium (8 vCPUs - 32GB ram) in the Session Type dropdown menu. Click Launch my Session #1 After you click Launch my session , the SOCA solution will create a new EC2 instance with 8 vCPUs and 32GB of memory and install all desktop required packages including Gnome. You will see an message asking you to wait up to 10 minutes before being able to access your remote desktop. Warning Please wait till the desktop instance is ready before moving on to the next step. In this lab you learned how to use SOCA web portal to create a desktop cloud visualization instance so you can access the compute cluster. Click Next once the status of Linux Session #1 changes and you see green button labeled Open Session directly on a browser .","title":"Lab 2: Login to SOCA Web UI and Launch Remote Desktop Session"},{"location":"workshops/Synopsys-Verification/Section-2/02-web-login/#lab-2-login-to-soca-web-ui-and-launch-remote-desktop-session","text":"The goal of this module is to login to SOCA web interface and start a remote desktop session from which you will run applications and submit jobs into the cluster. You will use the cluster's management portal to start and monitor the session.","title":"Lab 2: Login to SOCA Web UI and Launch Remote Desktop Session"},{"location":"workshops/Synopsys-Verification/Section-2/02-web-login/#step-1-login-to-soca-web-ui","text":"Click one of the links below depending on the session you're attending to login to corresponding SOCA web interface Workshop sessions are not active at this time!! Note Your web browser will warn you about a certificate problem with the site. To open the webpage, you need to authorize the browser to trust the self-signed security certificate. In a production deployment, you would upload a Server Certificate to the Elastic Load Balancer endpoint. Log in to the web UI using the following credentials: username: provided during tutorial session password: provided during tutorial session","title":"Step 1: Login to SOCA Web UI"},{"location":"workshops/Synopsys-Verification/Section-2/02-web-login/#step-2-launch-remote-desktop-server","text":"Follow these instructions to start a full remote desktop experience in your new cluster: Click Linux Desktop on the left sidebar. Under Linux Session #1 group: Select CentOS 7 - x86_64 in the Software Stack dropdown menu. Choose 2D - Medium (8 vCPUs - 32GB ram) in the Session Type dropdown menu. Click Launch my Session #1 After you click Launch my session , the SOCA solution will create a new EC2 instance with 8 vCPUs and 32GB of memory and install all desktop required packages including Gnome. You will see an message asking you to wait up to 10 minutes before being able to access your remote desktop. Warning Please wait till the desktop instance is ready before moving on to the next step. In this lab you learned how to use SOCA web portal to create a desktop cloud visualization instance so you can access the compute cluster. Click Next once the status of Linux Session #1 changes and you see green button labeled Open Session directly on a browser .","title":"Step 2: Launch remote desktop server"},{"location":"workshops/Synopsys-Verification/Section-2/03-login-compile/","text":"Lab 3: Login to Remote Desktop and Compile Design \u00b6 The goal with this lab is to evaluate the remote visualization experience using a graphical EDA tool. Step 1: Log into your session \u00b6 By now your remote desktop session should be ready and you should see the following under Your Session #1 : Click Open Session directly on a browser to log into the remote desktop session in the cluster. Note You can also access the session with the NICE DCV native clients, which are available for Mac, Linux, and Windows from https://download.nice-dcv.com Start a new terminal session by going to Applications \u2192 System Tools \u2192 Terminal in the desktop manager. Step 2: Copy test case \u00b6 Make a working directory for your user under /fsx/ by typing mkdir /fsx/`whoami` at the command prompt and hit enter Copy the test case to your working directory by typing cp -r /data/NVDLA_export /fsx/`whoami` at the command prompt and hit enter Note /data is a mount point for Amazon Elastic File System which provides a simple, scalable elastic NFS file system. /fsx is a mount point for Amazon FSx for Lustre which provides a Lustre file system suitable for high performance computing (HPC) workloads such as EDA Change directory to test case cd /fsx/`whoami`/NVDLA_export and hit enter Source environment settings by typing source setup.sh and hit enter Step 3: Compile the test case \u00b6 Change directory to verif/sim cd verif/sim and hit enter Compile the test case by typing make clean comp comp_verdi then hit enter. You should see messages indicating that the test case is getting compiled. The compilation process takes about 90 seconds and at the end you should see a compilation performance summary with a header such as: In this lab you learned how to login to desktop cloud visualiztion instance, copied the test case we'll use to submit VCS batch job, and compiled the test case. You've completed this lab. Click Next .","title":"Lab 3: Login to Remote Desktop and Compile Design"},{"location":"workshops/Synopsys-Verification/Section-2/03-login-compile/#lab-3-login-to-remote-desktop-and-compile-design","text":"The goal with this lab is to evaluate the remote visualization experience using a graphical EDA tool.","title":"Lab 3: Login to Remote Desktop and Compile Design"},{"location":"workshops/Synopsys-Verification/Section-2/03-login-compile/#step-1-log-into-your-session","text":"By now your remote desktop session should be ready and you should see the following under Your Session #1 : Click Open Session directly on a browser to log into the remote desktop session in the cluster. Note You can also access the session with the NICE DCV native clients, which are available for Mac, Linux, and Windows from https://download.nice-dcv.com Start a new terminal session by going to Applications \u2192 System Tools \u2192 Terminal in the desktop manager.","title":"Step 1: Log into your session"},{"location":"workshops/Synopsys-Verification/Section-2/03-login-compile/#step-2-copy-test-case","text":"Make a working directory for your user under /fsx/ by typing mkdir /fsx/`whoami` at the command prompt and hit enter Copy the test case to your working directory by typing cp -r /data/NVDLA_export /fsx/`whoami` at the command prompt and hit enter Note /data is a mount point for Amazon Elastic File System which provides a simple, scalable elastic NFS file system. /fsx is a mount point for Amazon FSx for Lustre which provides a Lustre file system suitable for high performance computing (HPC) workloads such as EDA Change directory to test case cd /fsx/`whoami`/NVDLA_export and hit enter Source environment settings by typing source setup.sh and hit enter","title":"Step 2: Copy test case"},{"location":"workshops/Synopsys-Verification/Section-2/03-login-compile/#step-3-compile-the-test-case","text":"Change directory to verif/sim cd verif/sim and hit enter Compile the test case by typing make clean comp comp_verdi then hit enter. You should see messages indicating that the test case is getting compiled. The compilation process takes about 90 seconds and at the end you should see a compilation performance summary with a header such as: In this lab you learned how to login to desktop cloud visualiztion instance, copied the test case we'll use to submit VCS batch job, and compiled the test case. You've completed this lab. Click Next .","title":"Step 3: Compile the test case"},{"location":"workshops/Synopsys-Verification/Section-2/04-submit-batch/","text":"Lab 4: Submit VCS Batch Jobs \u00b6 This module provides instructions for running an example batch workload in the SOCA computing environment. The example workload is a CPU- and IO-intensive logic simulation that is found in integrated cicuit design workflows. Step 1: Submit jobs to the scheduler \u00b6 Next, you'll submit four jobs into the cluster, each job requests a specific instance type. Using multiple instance types will help provide more interesting data to look at in the analytics lab. Execute the run_tests.sh script by typing ./run_tests.sh then hit enter. The script is broken into two sections: The first section creates 5 compute instances each with 4 vCPUs then the second section submits 20 batch jobs to these instances. You'll observe that the PBS scheduler will report the corresponding job ids for each of these 20 jobs. You can examine the run_tests.sh script by typing cat run_tests.sh . Notice that we're using Spot pricing model since the duration of these jobs are short. Also, notice that we're specifying --terminate_when_idle 3 which would terminate the instances and remove them from the cluster 3 mins after all jobs running on the instances complete. Step 2: Watch job status \u00b6 Run the qstat command to view the status of the jobs. You can also view job status by clicking on My Job Queue in the left side navigation bar in SOCA portal under PROFILE section as shown in the screen shot below: The result should be similar to the screen shot below which is a table format showing job id, name, status, etc...: You can run the pbsnodes -aSjL command to see the EC2 instances that have joined the cluster. Initially, the nodes will be in state-unknown,down till they boot-up and join the queue. Note SOCA automation scripts are configured to monitor the status of the queues every minute. It typically takes 5-6 minutes to launch a new EC2 instance, boot the operating system, configure it to join the cluster, and have the assigned job to start running. Step 3: Monitor test20 job \u00b6 Monitor the status of test20 job by refreshing the My Job Queue page in SOCA portal and look for the Status column for the job with test20 under Name column. You can also monitor the status of the jobs in the terminal by typing watch -n 10 \"qstat -u `whoami`\" command which will keep monitoring the status of the jobs every 10 seconds. You'll need to wait for the S column represeting the status of the job correspoding to the one with test20 under Name column. This job usually takes around 10 minutes to complete. Once the job is in the running state ( S column changes to R in the terminal or Status column changes to RUNNING on My Job Queue page in SOCA portal), look inside test20 directory for test.log and novas.fsdb by typing ls test20/* . Wait until test20/novas.fsdb is created as you'll need to use this fsdb file in the next lab. In this lab you learned how to submit VCS batch jobs to the cluster and how to monitor the status of these jobs. Click Next to move to the next lab.","title":"Lab 4: Submit VCS Batch Jobs"},{"location":"workshops/Synopsys-Verification/Section-2/04-submit-batch/#lab-4-submit-vcs-batch-jobs","text":"This module provides instructions for running an example batch workload in the SOCA computing environment. The example workload is a CPU- and IO-intensive logic simulation that is found in integrated cicuit design workflows.","title":"Lab 4: Submit VCS Batch Jobs"},{"location":"workshops/Synopsys-Verification/Section-2/04-submit-batch/#step-1-submit-jobs-to-the-scheduler","text":"Next, you'll submit four jobs into the cluster, each job requests a specific instance type. Using multiple instance types will help provide more interesting data to look at in the analytics lab. Execute the run_tests.sh script by typing ./run_tests.sh then hit enter. The script is broken into two sections: The first section creates 5 compute instances each with 4 vCPUs then the second section submits 20 batch jobs to these instances. You'll observe that the PBS scheduler will report the corresponding job ids for each of these 20 jobs. You can examine the run_tests.sh script by typing cat run_tests.sh . Notice that we're using Spot pricing model since the duration of these jobs are short. Also, notice that we're specifying --terminate_when_idle 3 which would terminate the instances and remove them from the cluster 3 mins after all jobs running on the instances complete.","title":"Step 1: Submit jobs to the scheduler"},{"location":"workshops/Synopsys-Verification/Section-2/04-submit-batch/#step-2-watch-job-status","text":"Run the qstat command to view the status of the jobs. You can also view job status by clicking on My Job Queue in the left side navigation bar in SOCA portal under PROFILE section as shown in the screen shot below: The result should be similar to the screen shot below which is a table format showing job id, name, status, etc...: You can run the pbsnodes -aSjL command to see the EC2 instances that have joined the cluster. Initially, the nodes will be in state-unknown,down till they boot-up and join the queue. Note SOCA automation scripts are configured to monitor the status of the queues every minute. It typically takes 5-6 minutes to launch a new EC2 instance, boot the operating system, configure it to join the cluster, and have the assigned job to start running.","title":"Step 2: Watch job status"},{"location":"workshops/Synopsys-Verification/Section-2/04-submit-batch/#step-3-monitor-test20-job","text":"Monitor the status of test20 job by refreshing the My Job Queue page in SOCA portal and look for the Status column for the job with test20 under Name column. You can also monitor the status of the jobs in the terminal by typing watch -n 10 \"qstat -u `whoami`\" command which will keep monitoring the status of the jobs every 10 seconds. You'll need to wait for the S column represeting the status of the job correspoding to the one with test20 under Name column. This job usually takes around 10 minutes to complete. Once the job is in the running state ( S column changes to R in the terminal or Status column changes to RUNNING on My Job Queue page in SOCA portal), look inside test20 directory for test.log and novas.fsdb by typing ls test20/* . Wait until test20/novas.fsdb is created as you'll need to use this fsdb file in the next lab. In this lab you learned how to submit VCS batch jobs to the cluster and how to monitor the status of these jobs. Click Next to move to the next lab.","title":"Step 3: Monitor test20 job"},{"location":"workshops/Synopsys-Verification/Section-2/05-load-verdi/","text":"Lab 5: Bring up Verdi \u00b6 This module provides instructions for loading a graphical tool to debug a design which is commonly used in integrated cicuit design workflows. Step 1: Start Verdi and load test20 database \u00b6 Start Verdi and load test20 waveform database by typing the command verdi -ssf test20/novas.fsdb . When Verdi GUI comes-up, click Ok to ignore the license expiration warning. Step 2: Select Signals \u00b6 On the lower pane, click on Signal then on the pop-up click on Get Signals On the left pane, expand + top(top) then expand + nvdla_top(NV_nvdla) then click on u_parition_a(NV_NVDLA_partition_a) In the middle pane, click on accu2sc_credit_size[2:0] then hit shift and click on mac_a2accu_dst_data2[175:0] then click OK Step 3: Search for Signal Transition \u00b6 On the lower left pane, click on mac_a2accu_dst_data0[175:0] and click twice on the Search Forward button (icon with arrow pointing to the right) to find the time where the bus changes to non-zero values and observe the timestamp 419,090,000 ps Step 4: Exit Verdi \u00b6 On the upper pane, click on File menu item then click on Exit to close Verdi. Click Yes on the exit confirmation question In this lab you learned how to bring up an interactive debugging application and interact with it. Click Next to move to the next lab.","title":"Lab 5: Bring up Verdi"},{"location":"workshops/Synopsys-Verification/Section-2/05-load-verdi/#lab-5-bring-up-verdi","text":"This module provides instructions for loading a graphical tool to debug a design which is commonly used in integrated cicuit design workflows.","title":"Lab 5: Bring up Verdi"},{"location":"workshops/Synopsys-Verification/Section-2/05-load-verdi/#step-1-start-verdi-and-load-test20-database","text":"Start Verdi and load test20 waveform database by typing the command verdi -ssf test20/novas.fsdb . When Verdi GUI comes-up, click Ok to ignore the license expiration warning.","title":"Step 1: Start Verdi and load test20 database"},{"location":"workshops/Synopsys-Verification/Section-2/05-load-verdi/#step-2-select-signals","text":"On the lower pane, click on Signal then on the pop-up click on Get Signals On the left pane, expand + top(top) then expand + nvdla_top(NV_nvdla) then click on u_parition_a(NV_NVDLA_partition_a) In the middle pane, click on accu2sc_credit_size[2:0] then hit shift and click on mac_a2accu_dst_data2[175:0] then click OK","title":"Step 2: Select Signals"},{"location":"workshops/Synopsys-Verification/Section-2/05-load-verdi/#step-3-search-for-signal-transition","text":"On the lower left pane, click on mac_a2accu_dst_data0[175:0] and click twice on the Search Forward button (icon with arrow pointing to the right) to find the time where the bus changes to non-zero values and observe the timestamp 419,090,000 ps","title":"Step 3: Search for Signal Transition"},{"location":"workshops/Synopsys-Verification/Section-2/05-load-verdi/#step-4-exit-verdi","text":"On the upper pane, click on File menu item then click on Exit to close Verdi. Click Yes on the exit confirmation question In this lab you learned how to bring up an interactive debugging application and interact with it. Click Next to move to the next lab.","title":"Step 4: Exit Verdi"},{"location":"workshops/Synopsys-Verification/Section-2/06-analytics/","text":"Lab 6: Explore Analytics Dashboard \u00b6 Step 1: Open My Activity \u00b6 Return to the SOCA web UI and click on the My Activity section on the left sidebar and wait a minute for OpenSearch (formerly Elasticsearch) Kibana dashboard to load. The OpenSearch (formerly Elasticsearch) dashboard attempts to filter the jobs view for the logged-in user. However, data is refereshed every hour from the SOCA cluster to OpenSearch (formerly Elasticsearch). If you see No results match your search criteria , delete the user filter from the search field. You should see information about recent jobs submitted to the cluster for all users. You might need to update the duration dropdown just above the chart to see older or more recent job data. If you click on the small arrow to the left of the date/time, you'll see all metadata information saved into the analytics dashboard for this specific job. The metadata information is available in Table or JSON format. Here is an example of how it looks in JSON format. Notice that it includes an estimated price calculated based on job duration and instance type. Step 2: Explore Cluster Dashboard \u00b6 Above the chart click on the three-horizontal bars (a.k.a hamburger button) next to OpenSearch (formerly Elasticsearch) Discover label. Then under the Kibana section, click on Dashboard Then click on Cluster dashboard . The cluster administrator can setup various visualizations in the dashboard. Note The results of what you'll see in the cluster might be different than the example images below. Move your cursor above the visualizations to see pop-ups with more relevant information. You can also zoom in or out on the charts with date/time on the x-axis to see more details. Here are a few examples: Jobs per user split by instance type \u00b6 Instance types split by user \u00b6 Number of instances in the cluster \u00b6 In this lab you learned how to explore the analytics dashboard in SOCA cluster. Click Next to move to the next lab.","title":"Lab 6: Explore Analytics Dashboard"},{"location":"workshops/Synopsys-Verification/Section-2/06-analytics/#lab-6-explore-analytics-dashboard","text":"","title":"Lab 6: Explore Analytics Dashboard"},{"location":"workshops/Synopsys-Verification/Section-2/06-analytics/#step-1-open-my-activity","text":"Return to the SOCA web UI and click on the My Activity section on the left sidebar and wait a minute for OpenSearch (formerly Elasticsearch) Kibana dashboard to load. The OpenSearch (formerly Elasticsearch) dashboard attempts to filter the jobs view for the logged-in user. However, data is refereshed every hour from the SOCA cluster to OpenSearch (formerly Elasticsearch). If you see No results match your search criteria , delete the user filter from the search field. You should see information about recent jobs submitted to the cluster for all users. You might need to update the duration dropdown just above the chart to see older or more recent job data. If you click on the small arrow to the left of the date/time, you'll see all metadata information saved into the analytics dashboard for this specific job. The metadata information is available in Table or JSON format. Here is an example of how it looks in JSON format. Notice that it includes an estimated price calculated based on job duration and instance type.","title":"Step 1: Open My Activity"},{"location":"workshops/Synopsys-Verification/Section-2/06-analytics/#step-2-explore-cluster-dashboard","text":"Above the chart click on the three-horizontal bars (a.k.a hamburger button) next to OpenSearch (formerly Elasticsearch) Discover label. Then under the Kibana section, click on Dashboard Then click on Cluster dashboard . The cluster administrator can setup various visualizations in the dashboard. Note The results of what you'll see in the cluster might be different than the example images below. Move your cursor above the visualizations to see pop-ups with more relevant information. You can also zoom in or out on the charts with date/time on the x-axis to see more details. Here are a few examples:","title":"Step 2: Explore Cluster Dashboard"},{"location":"workshops/Synopsys-Verification/Section-2/06-analytics/#jobs-per-user-split-by-instance-type","text":"","title":"Jobs per user split by instance type"},{"location":"workshops/Synopsys-Verification/Section-2/06-analytics/#instance-types-split-by-user","text":"","title":"Instance types split by user"},{"location":"workshops/Synopsys-Verification/Section-2/06-analytics/#number-of-instances-in-the-cluster","text":"In this lab you learned how to explore the analytics dashboard in SOCA cluster. Click Next to move to the next lab.","title":"Number of instances in the cluster"},{"location":"workshops/Synopsys-Verification/Section-2/09-wrap-up/","text":"Wrap-up \u00b6 Congratulations! You've completed this workshop. Next steps \u00b6 Please complete the session survey. Clean up \u00b6 No cleanup required! This responsibility falls to AWS.","title":"Wrap-up"},{"location":"workshops/Synopsys-Verification/Section-2/09-wrap-up/#wrap-up","text":"Congratulations! You've completed this workshop.","title":"Wrap-up"},{"location":"workshops/Synopsys-Verification/Section-2/09-wrap-up/#next-steps","text":"Please complete the session survey.","title":"Next steps"},{"location":"workshops/Synopsys-Verification/Section-2/09-wrap-up/#clean-up","text":"No cleanup required! This responsibility falls to AWS.","title":"Clean up"},{"location":"workshops/TKO-Scale-Out-Computing/","text":"Scaling EDA with Vivado Workshop Overview \u00b6 Launch a turnkey scale-out compute environment in minutes on AWS \u00b6 The elasticity of the cloud puts virtually unlimited compute capacity at your fingertips, available within minutes. This can enable companies to quickly scale up in ways they couldn't before, which helps them get results faster. In this workshop, you will deploy the Scale-Out Computing on AWS reference implementation , a solution vetted by AWS Solutions Architects that provides a full-stack, dynamic computing environment that includes a web UI, a workload manager, remote desktops, directory services, analytics dashboards, and budget management. Note This tutorial assumes familiarity with the Linux command line. Lab environment at a glance \u00b6 At its core, this solution implements a scheduler Amazon Elastic Compute Cloud (Amazon EC2) instance, which leverages AWS CloudFormation and Amazon EC2 Auto Scaling to automatically provision the resources necessary to execute cluster user tasks such as scale-out compute jobs and remote visualization sessions. The solution also deploys Amazon Elastic File System (Amazon EFS) for persistent storage; AWS Lambda functions to verify the required prerequisites and create a default signed certificate for an Application Load Balancer (ALB) to manage access to Desktop Cloud Visualization (DCV) workstation sessions; an Amazon Elasticsearch Service (Amazon ES) cluster to store job and host metrics; and AWS Secrets Manager to store the solution configuration files. The solution also leverages AWS Identity and Access Management (IAM) roles to enforce least privileged access. Let's get started. Click the Next link in the bottom right corner to move on to the next module.","title":"Scaling EDA with Vivado Workshop Overview"},{"location":"workshops/TKO-Scale-Out-Computing/#scaling-eda-with-vivado-workshop-overview","text":"","title":"Scaling EDA with Vivado Workshop Overview"},{"location":"workshops/TKO-Scale-Out-Computing/#launch-a-turnkey-scale-out-compute-environment-in-minutes-on-aws","text":"The elasticity of the cloud puts virtually unlimited compute capacity at your fingertips, available within minutes. This can enable companies to quickly scale up in ways they couldn't before, which helps them get results faster. In this workshop, you will deploy the Scale-Out Computing on AWS reference implementation , a solution vetted by AWS Solutions Architects that provides a full-stack, dynamic computing environment that includes a web UI, a workload manager, remote desktops, directory services, analytics dashboards, and budget management. Note This tutorial assumes familiarity with the Linux command line.","title":"Launch a turnkey scale-out compute environment in minutes on AWS"},{"location":"workshops/TKO-Scale-Out-Computing/#lab-environment-at-a-glance","text":"At its core, this solution implements a scheduler Amazon Elastic Compute Cloud (Amazon EC2) instance, which leverages AWS CloudFormation and Amazon EC2 Auto Scaling to automatically provision the resources necessary to execute cluster user tasks such as scale-out compute jobs and remote visualization sessions. The solution also deploys Amazon Elastic File System (Amazon EFS) for persistent storage; AWS Lambda functions to verify the required prerequisites and create a default signed certificate for an Application Load Balancer (ALB) to manage access to Desktop Cloud Visualization (DCV) workstation sessions; an Amazon Elasticsearch Service (Amazon ES) cluster to store job and host metrics; and AWS Secrets Manager to store the solution configuration files. The solution also leverages AWS Identity and Access Management (IAM) roles to enforce least privileged access. Let's get started. Click the Next link in the bottom right corner to move on to the next module.","title":"Lab environment at a glance"},{"location":"workshops/TKO-Scale-Out-Computing/getting-started/","text":"Getting Started \u00b6 Account \u00b6 You will use your own AWS account for this workshop. You should log into an account with full admin privileges. Region \u00b6 You will deploy the CloudFormation stack into the US West (Oregon) . Other regions are not supported in this workshop at this time. Key Pair \u00b6 This solution requires SSH login, so be sure you have a Key Pair for the US West (Oregon) region.","title":"Getting Started"},{"location":"workshops/TKO-Scale-Out-Computing/getting-started/#getting-started","text":"","title":"Getting Started"},{"location":"workshops/TKO-Scale-Out-Computing/getting-started/#account","text":"You will use your own AWS account for this workshop. You should log into an account with full admin privileges.","title":"Account"},{"location":"workshops/TKO-Scale-Out-Computing/getting-started/#region","text":"You will deploy the CloudFormation stack into the US West (Oregon) . Other regions are not supported in this workshop at this time.","title":"Region"},{"location":"workshops/TKO-Scale-Out-Computing/getting-started/#key-pair","text":"This solution requires SSH login, so be sure you have a Key Pair for the US West (Oregon) region.","title":"Key Pair"},{"location":"workshops/TKO-Scale-Out-Computing/modules/02-deploy-env/","text":"Lab 1: Deploy Environment \u00b6 Step 1: Launch stack \u00b6 This automated AWS CloudFormation template deploys a scale-out computing environment in the AWS Cloud. Verify that you have a key pair in the US West (Oregon) region. If not, create a new key pair. Sign in to the AWS Management Console and click the link below to launch the scale-out-computing-on-aws AWS CloudFormation template. Launch Stack in US West (Oregon) On the Create stack page, you should see the template URL in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign the name \"tko\" to the stack. Warning The stack name must be less than 20 characters and must be lower-case only. Under Parameters , modify the the last four parameters, which are marked with REQUIRED . Leave all other fields with their default values. These are variables passed the CloudFormation automation that deploys the environment. Parameter Default Description Install Location Installer S3 Bucket solutions-reference The default AWS bucket name. Do not change this parameter unless you are using a custom installer. Installer Folder scale-out-computing-on-aws/latest The default AWS folder name. Do not change this parameter unless you are using a custom installer. Linux Distribution Linux Distribution AmazonLinux2 The preferred Linux distribution for the scheduler and compute instances. Do not change this parameter. Custom AMI If using a customized Amazon Machine Image, enter the ID. Leave this field blank. Network and Security EC2 Instance Type for Scheduler node m5.large The instance type for the scheduler. Do not change this parameter. VPC Cluster CIDR 10.0.0.0/16 Choose the CIDR (/16) block for the VPC. Do not change this parameter. IP Address 0.0.0.0/0 REQUIRED The public-facing IP address that is permitted to log into the environment. You can leave it at default, but we recommend you change it to your public-facing IP address. Add the /32 suffix to the IP number. Key Pair Name REQUIRED Select your key pair. Default LDAP User User Name REQUIRED Set a username for the default cluster user. This will be the administrator user for LDAP which would allow you to add other users to the cluster. Password REQUIRED Set a password for the default cluster user. (5 characters minimum, uppercase/lowercase/digit only) Choose Next . On the Configure Stack Options page, choose Next . On the Review page, review the settings and check the two boxes acknowledging that the template will create AWS Identity and Access Management (IAM) resources and might require the CAPABILITY_AUTO_EXPAND capability. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should see a status of CREATE_COMPLETE in approximately 35 minutes. Please wait for instructions from the workshop staff before moving on to the next lab.","title":"Lab 1: Deploy Environment"},{"location":"workshops/TKO-Scale-Out-Computing/modules/02-deploy-env/#lab-1-deploy-environment","text":"","title":"Lab 1: Deploy Environment"},{"location":"workshops/TKO-Scale-Out-Computing/modules/02-deploy-env/#step-1-launch-stack","text":"This automated AWS CloudFormation template deploys a scale-out computing environment in the AWS Cloud. Verify that you have a key pair in the US West (Oregon) region. If not, create a new key pair. Sign in to the AWS Management Console and click the link below to launch the scale-out-computing-on-aws AWS CloudFormation template. Launch Stack in US West (Oregon) On the Create stack page, you should see the template URL in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign the name \"tko\" to the stack. Warning The stack name must be less than 20 characters and must be lower-case only. Under Parameters , modify the the last four parameters, which are marked with REQUIRED . Leave all other fields with their default values. These are variables passed the CloudFormation automation that deploys the environment. Parameter Default Description Install Location Installer S3 Bucket solutions-reference The default AWS bucket name. Do not change this parameter unless you are using a custom installer. Installer Folder scale-out-computing-on-aws/latest The default AWS folder name. Do not change this parameter unless you are using a custom installer. Linux Distribution Linux Distribution AmazonLinux2 The preferred Linux distribution for the scheduler and compute instances. Do not change this parameter. Custom AMI If using a customized Amazon Machine Image, enter the ID. Leave this field blank. Network and Security EC2 Instance Type for Scheduler node m5.large The instance type for the scheduler. Do not change this parameter. VPC Cluster CIDR 10.0.0.0/16 Choose the CIDR (/16) block for the VPC. Do not change this parameter. IP Address 0.0.0.0/0 REQUIRED The public-facing IP address that is permitted to log into the environment. You can leave it at default, but we recommend you change it to your public-facing IP address. Add the /32 suffix to the IP number. Key Pair Name REQUIRED Select your key pair. Default LDAP User User Name REQUIRED Set a username for the default cluster user. This will be the administrator user for LDAP which would allow you to add other users to the cluster. Password REQUIRED Set a password for the default cluster user. (5 characters minimum, uppercase/lowercase/digit only) Choose Next . On the Configure Stack Options page, choose Next . On the Review page, review the settings and check the two boxes acknowledging that the template will create AWS Identity and Access Management (IAM) resources and might require the CAPABILITY_AUTO_EXPAND capability. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should see a status of CREATE_COMPLETE in approximately 35 minutes. Please wait for instructions from the workshop staff before moving on to the next lab.","title":"Step 1: Launch stack"},{"location":"workshops/TKO-Scale-Out-Computing/modules/03-configure-desktop/","text":"Lab 2: Configure Remote Desktop \u00b6 Once the solution has been deployed, we will configure the environment to use a specific Amazon Machine Image (AMI) for booting the remote desktop server. As you saw in the architecture diagram , the DCV remote desktop will be your portal into the computing environment. This AMI provides the CentOS 7.5 Linux operating system and the applications you'll use later in the workshop. Obtain IP address of scheduler server In the AWS console, navigate to the CloudFormation page. Select the root stack named \"soca-xxxxxxxxxxxx\", where 'x' is a randomized alpha-numeric string, and click Outputs . The Outputs tab provides various bits of information about the provisioned environment. Copy the value to the left of ConnectionString . We'll use this command to SSH into the scheduler instance. Connect to the instance over SSH For macOS, paste the SSH command into a terminal on the Mac. Be sure to use the full path to the private key you downloaded earlier. See the steps here in the AWS Connecting to Your Linux Instance using SSH page if you need assistance. For Windows, follow the instructions in the AWS Connect to your instance using PuTTY docs. Once logged in, type sudo su to become root on the scheduler instance, then open the file /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml using your favorite text editor. Example: vi /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml Change the the highlighted values in the file to match the example below. Note Indentation matters in this file. Therefore, we advise against copying and pasting this entire block into the file, as this can result in malformed formatting. Instead, please edit the value of each highlighted line individually. queue_type : compute : queues : [ \"high\" , \"normal\" , \"low\" ] instance_ami : \"ami-03f32123533983fdb\" instance_type : \"c5.large\" ht_support : \"false\" root_size : \"100\" base_os : \"centos7\" #scratch_size: \"100\" #scratch_iops: \"3600\" #efa_support: \"false\" # .. Refer to the doc for more supported parameters Save the changes to the file. The steps above configured the AMI ID to be used when we submit jobs to the compute queues specified above. Keep this SSH session open; you will come back to it later. Click the Next to move on to the next module.","title":"Lab 2: Configure Remote Desktop"},{"location":"workshops/TKO-Scale-Out-Computing/modules/03-configure-desktop/#lab-2-configure-remote-desktop","text":"Once the solution has been deployed, we will configure the environment to use a specific Amazon Machine Image (AMI) for booting the remote desktop server. As you saw in the architecture diagram , the DCV remote desktop will be your portal into the computing environment. This AMI provides the CentOS 7.5 Linux operating system and the applications you'll use later in the workshop. Obtain IP address of scheduler server In the AWS console, navigate to the CloudFormation page. Select the root stack named \"soca-xxxxxxxxxxxx\", where 'x' is a randomized alpha-numeric string, and click Outputs . The Outputs tab provides various bits of information about the provisioned environment. Copy the value to the left of ConnectionString . We'll use this command to SSH into the scheduler instance. Connect to the instance over SSH For macOS, paste the SSH command into a terminal on the Mac. Be sure to use the full path to the private key you downloaded earlier. See the steps here in the AWS Connecting to Your Linux Instance using SSH page if you need assistance. For Windows, follow the instructions in the AWS Connect to your instance using PuTTY docs. Once logged in, type sudo su to become root on the scheduler instance, then open the file /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml using your favorite text editor. Example: vi /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml Change the the highlighted values in the file to match the example below. Note Indentation matters in this file. Therefore, we advise against copying and pasting this entire block into the file, as this can result in malformed formatting. Instead, please edit the value of each highlighted line individually. queue_type : compute : queues : [ \"high\" , \"normal\" , \"low\" ] instance_ami : \"ami-03f32123533983fdb\" instance_type : \"c5.large\" ht_support : \"false\" root_size : \"100\" base_os : \"centos7\" #scratch_size: \"100\" #scratch_iops: \"3600\" #efa_support: \"false\" # .. Refer to the doc for more supported parameters Save the changes to the file. The steps above configured the AMI ID to be used when we submit jobs to the compute queues specified above. Keep this SSH session open; you will come back to it later. Click the Next to move on to the next module.","title":"Lab 2: Configure Remote Desktop"},{"location":"workshops/TKO-Scale-Out-Computing/modules/04-web-login/","text":"Lab 3: Launch Remote Desktop Session \u00b6 The goal of this module is to start a remote desktop session from which you will run applications and submit jobs into the cluster. You will use the cluster's management console to start and monitor the session. Step 1: Subscribe to AWS FPGA Developer AMI \u00b6 This workshop requires a subscription to the AWS FPGA Developer AMI in AWS Marketplace . This is the AMI you added to the /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml file in the previous lab. It will be used to boot the remote desktop instance in AWS and contains software required later in the workshop. Return to the AWS console for your temporary AWS account. Click here to open the page for the AWS FPGA Developer AMI in AWS Marketplace, and then choose Continue to Subscribe . Review the terms and conditions for software usage, and then choose Accept Terms . When the subscription process is complete, exit out of AWS Marketplace without further action. Do not click Continue to Configure ; the workshop CloudFormation templates will deploy the AMI for you. Click here to verify the subscription within the Marketplace dashboard. You should see the FPGA Developer AMI in the list of subscriptions. Step 2: Log into web UI \u00b6 Select the root stack named \"mod-xxxxxxxx\" again, and click Outputs . The Outputs tab provides various bits of information about the provisioned environment. Click on the link to the right of WebUserInterface to log into the Web UI Note Your web browser will warn you about a certificate problem with the site. To open the webpage, you must authorize the browser to trust the self-signed security certificate. Log in using the web UI using credentials you provided in the CloudFormation template username and password parameters. Step 3: Register the FPGA Dev AMI for DCV sessions \u00b6 Click AMI Management on the left sidebar under the ADMIN section. In the AMI ID field, paste ami-03f32123533983fdb , select Linux - Centos7 under Operating System , type 100 in the Root Disk Size (in GB) field, and finally type FPGA Dev AMI in the AMI Label field. Finally click on Register AMI in SOCA Step 4: Launch remote desktop server \u00b6 Follow these instructions to start a full remote desktop experience in your new cluster: Click Linux Desktop on the left sidebar. Type 100 in the Storage Size field. Select FPGA Dev AMI from the Software Stack dropdown list. Choose 2D - Medium (8 vCPUs - 32GB ram) from the Session Type dropdown list. Click Launch my Session #1 After you click Launch my session , a new job is submitted into the queue that will instruct AWS to provision a server with 8 vCPUs and 32GB of memory and install all desktop required packages including Gnome. You will see an message asking you to wait up to 20 minutes before being able to access your remote desktop, but it should take around 10 minutes to deploy the remote desktop server. Note You can monitor the deployment of the remote desktop server by observing the status of the CloudFormation stack with a name that ends in -Desktop1-<username> . If after 5 minutes the status of the stack is not CREATE_COMPLETE , please raise your hand for assistance. Let's move on to the next step while we wait for the desktop instance to launch. Click Next .","title":"Lab 3: Launch Remote Desktop Session"},{"location":"workshops/TKO-Scale-Out-Computing/modules/04-web-login/#lab-3-launch-remote-desktop-session","text":"The goal of this module is to start a remote desktop session from which you will run applications and submit jobs into the cluster. You will use the cluster's management console to start and monitor the session.","title":"Lab 3: Launch Remote Desktop Session"},{"location":"workshops/TKO-Scale-Out-Computing/modules/04-web-login/#step-1-subscribe-to-aws-fpga-developer-ami","text":"This workshop requires a subscription to the AWS FPGA Developer AMI in AWS Marketplace . This is the AMI you added to the /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/queue_mapping.yml file in the previous lab. It will be used to boot the remote desktop instance in AWS and contains software required later in the workshop. Return to the AWS console for your temporary AWS account. Click here to open the page for the AWS FPGA Developer AMI in AWS Marketplace, and then choose Continue to Subscribe . Review the terms and conditions for software usage, and then choose Accept Terms . When the subscription process is complete, exit out of AWS Marketplace without further action. Do not click Continue to Configure ; the workshop CloudFormation templates will deploy the AMI for you. Click here to verify the subscription within the Marketplace dashboard. You should see the FPGA Developer AMI in the list of subscriptions.","title":"Step 1: Subscribe to AWS FPGA Developer AMI"},{"location":"workshops/TKO-Scale-Out-Computing/modules/04-web-login/#step-2-log-into-web-ui","text":"Select the root stack named \"mod-xxxxxxxx\" again, and click Outputs . The Outputs tab provides various bits of information about the provisioned environment. Click on the link to the right of WebUserInterface to log into the Web UI Note Your web browser will warn you about a certificate problem with the site. To open the webpage, you must authorize the browser to trust the self-signed security certificate. Log in using the web UI using credentials you provided in the CloudFormation template username and password parameters.","title":"Step 2: Log into web UI"},{"location":"workshops/TKO-Scale-Out-Computing/modules/04-web-login/#step-3-register-the-fpga-dev-ami-for-dcv-sessions","text":"Click AMI Management on the left sidebar under the ADMIN section. In the AMI ID field, paste ami-03f32123533983fdb , select Linux - Centos7 under Operating System , type 100 in the Root Disk Size (in GB) field, and finally type FPGA Dev AMI in the AMI Label field. Finally click on Register AMI in SOCA","title":"Step 3: Register the FPGA Dev AMI for DCV sessions"},{"location":"workshops/TKO-Scale-Out-Computing/modules/04-web-login/#step-4-launch-remote-desktop-server","text":"Follow these instructions to start a full remote desktop experience in your new cluster: Click Linux Desktop on the left sidebar. Type 100 in the Storage Size field. Select FPGA Dev AMI from the Software Stack dropdown list. Choose 2D - Medium (8 vCPUs - 32GB ram) from the Session Type dropdown list. Click Launch my Session #1 After you click Launch my session , a new job is submitted into the queue that will instruct AWS to provision a server with 8 vCPUs and 32GB of memory and install all desktop required packages including Gnome. You will see an message asking you to wait up to 20 minutes before being able to access your remote desktop, but it should take around 10 minutes to deploy the remote desktop server. Note You can monitor the deployment of the remote desktop server by observing the status of the CloudFormation stack with a name that ends in -Desktop1-<username> . If after 5 minutes the status of the stack is not CREATE_COMPLETE , please raise your hand for assistance. Let's move on to the next step while we wait for the desktop instance to launch. Click Next .","title":"Step 4: Launch remote desktop server"},{"location":"workshops/TKO-Scale-Out-Computing/modules/06-launch-vivado/","text":"Lab 4: Launch Vivado GUI \u00b6 Launch the Vivado CAD Tool \u00b6 Now that your remote desktop is set up, you can launch the Vivado Design Suite (included in the AWS FPGA Developer AMI). The goal with this lab is to evaluate the remote visualization experience using a graphically intensive Computer-Aided Design (CAD) tool. Step 1: Log into your session \u00b6 By now your remote desktop session should be ready and you should see the following under Your Session #1 : Click Open Session directly on a browser to log into the remote desktop session in your new cluster using the username and password you created in the steps above. Note You can also access the session with the NICE DCV native clients, which are available for Mac, Linux, and Windows from https://download.nice-dcv.com To launch Vivado, start a new terminal session by going to Applications \u2192 Favorites \u2192 Terminal in the desktop manager. Type vivado at the command prompt and hit enter The Vivado GUI starts and shows the following screen: Step 2: Create a new project project \u00b6 Next, load a sample workload using one of the included example projects: Go to the Quick Start section and select Create Project . Wait for the wizard to initialize, then on the \"Create a New Vivado Project\" screen, click Next > On the \"Project Name\" screen, change the project location to /scratch/<username> , then click Next > On the \"Project Type\" screen, select Example Project , then click Next > On the \"Select Project Template\" screen, click Next > On the \"Default board or part\" screen, click Next > On the \"Select Design Preset\" screen, select Microcontroller , then click Next > On the \"New Project Summary\" screen, click Finish It should run for 15-20 minutes to create the project and include all required design files. You don't have to wait until this step completes and can move to the next lab. Double-click on Open Block Diagram under IP INTEGRATOR in the left-side navigation panel After the design opens you should see an image similar to this: You can now click around the GUI and scroll and pan through the schematics to get a sense of the remote desktop experience. For extra credit, double-click on Open Synthesized Design in the navigation panel to see a more complext layout of the design. You've completed this lab. Click Next .","title":"Lab 4: Launch Vivado GUI"},{"location":"workshops/TKO-Scale-Out-Computing/modules/06-launch-vivado/#lab-4-launch-vivado-gui","text":"","title":"Lab 4: Launch Vivado GUI"},{"location":"workshops/TKO-Scale-Out-Computing/modules/06-launch-vivado/#launch-the-vivado-cad-tool","text":"Now that your remote desktop is set up, you can launch the Vivado Design Suite (included in the AWS FPGA Developer AMI). The goal with this lab is to evaluate the remote visualization experience using a graphically intensive Computer-Aided Design (CAD) tool.","title":"Launch the Vivado CAD Tool"},{"location":"workshops/TKO-Scale-Out-Computing/modules/06-launch-vivado/#step-1-log-into-your-session","text":"By now your remote desktop session should be ready and you should see the following under Your Session #1 : Click Open Session directly on a browser to log into the remote desktop session in your new cluster using the username and password you created in the steps above. Note You can also access the session with the NICE DCV native clients, which are available for Mac, Linux, and Windows from https://download.nice-dcv.com To launch Vivado, start a new terminal session by going to Applications \u2192 Favorites \u2192 Terminal in the desktop manager. Type vivado at the command prompt and hit enter The Vivado GUI starts and shows the following screen:","title":"Step 1: Log into your session"},{"location":"workshops/TKO-Scale-Out-Computing/modules/06-launch-vivado/#step-2-create-a-new-project-project","text":"Next, load a sample workload using one of the included example projects: Go to the Quick Start section and select Create Project . Wait for the wizard to initialize, then on the \"Create a New Vivado Project\" screen, click Next > On the \"Project Name\" screen, change the project location to /scratch/<username> , then click Next > On the \"Project Type\" screen, select Example Project , then click Next > On the \"Select Project Template\" screen, click Next > On the \"Default board or part\" screen, click Next > On the \"Select Design Preset\" screen, select Microcontroller , then click Next > On the \"New Project Summary\" screen, click Finish It should run for 15-20 minutes to create the project and include all required design files. You don't have to wait until this step completes and can move to the next lab. Double-click on Open Block Diagram under IP INTEGRATOR in the left-side navigation panel After the design opens you should see an image similar to this: You can now click around the GUI and scroll and pan through the schematics to get a sense of the remote desktop experience. For extra credit, double-click on Open Synthesized Design in the navigation panel to see a more complext layout of the design. You've completed this lab. Click Next .","title":"Step 2: Create a new project project"},{"location":"workshops/TKO-Scale-Out-Computing/modules/07-submit-batch/","text":"Lab 5: Submit Batch Workloads \u00b6 This module provides instructions for running an example batch workload in the computing envronment created the Deploy environment module. The example workload is a CPU- and IO-intensive logic simulation that is found in integrated cicuit design workflows. The workload uses data contained in the public AWS F1 FPGA Development Kit and the Xilinx Vivado EDA software suite provided by the AWS FPGA Developer AMI that you subscribed to in the first tutorial. Although you'll be using data and tools from AWS FPGA developer resources, you will not be running on the F1 FPGA instance or executing any type of FPGA workload; we're simply running software simulations on EC2 compute instances using the design data, IP, and software that these kits provide for no additional charge. Step 1: Clone workload repo \u00b6 End your SSH sessions and log back into the DCV remote desktop session that you established Launch Remote Desktop Session lab. Minimize the Vivado GUI and open a new terminal window. Clone the example workload from the aws-fpga-sa-demo Github repo into your user's home directory on the NFS file system. cd $HOME git clone https://github.com/morrmt/aws-fpga-sa-demo.git Change into the repo's workshop directory. cd $HOME/aws-fpga-sa-demo/eda-workshop Step 2: Submit jobs into the queue \u00b6 Next, you'll submit four jobs into the cluster, each job requesting a specific instance type. Using multiple instance types will help provide more interesting data to look at in the analytics lab. Submit jobs . Submit each of the jobs below: qsub -l instance_type=c5.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=c4.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=m5.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=m4.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME Watch job status . As soon as jobs are sent to the queue, the dispatcher script will start up a new compute instance to execute each job. Run the qstat command to view the status of the jobs. You can also view job status in the web UI by clicking on My Job Queue in the left side navigation bar. You can also run the pbsnodes -a command to see the EC2 instances that have joined the cluster. Click Next to move to the next section.","title":"Lab 5: Submit Batch Workloads"},{"location":"workshops/TKO-Scale-Out-Computing/modules/07-submit-batch/#lab-5-submit-batch-workloads","text":"This module provides instructions for running an example batch workload in the computing envronment created the Deploy environment module. The example workload is a CPU- and IO-intensive logic simulation that is found in integrated cicuit design workflows. The workload uses data contained in the public AWS F1 FPGA Development Kit and the Xilinx Vivado EDA software suite provided by the AWS FPGA Developer AMI that you subscribed to in the first tutorial. Although you'll be using data and tools from AWS FPGA developer resources, you will not be running on the F1 FPGA instance or executing any type of FPGA workload; we're simply running software simulations on EC2 compute instances using the design data, IP, and software that these kits provide for no additional charge.","title":"Lab 5: Submit Batch Workloads"},{"location":"workshops/TKO-Scale-Out-Computing/modules/07-submit-batch/#step-1-clone-workload-repo","text":"End your SSH sessions and log back into the DCV remote desktop session that you established Launch Remote Desktop Session lab. Minimize the Vivado GUI and open a new terminal window. Clone the example workload from the aws-fpga-sa-demo Github repo into your user's home directory on the NFS file system. cd $HOME git clone https://github.com/morrmt/aws-fpga-sa-demo.git Change into the repo's workshop directory. cd $HOME/aws-fpga-sa-demo/eda-workshop","title":"Step 1: Clone workload repo"},{"location":"workshops/TKO-Scale-Out-Computing/modules/07-submit-batch/#step-2-submit-jobs-into-the-queue","text":"Next, you'll submit four jobs into the cluster, each job requesting a specific instance type. Using multiple instance types will help provide more interesting data to look at in the analytics lab. Submit jobs . Submit each of the jobs below: qsub -l instance_type=c5.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=c4.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=m5.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=m4.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME Watch job status . As soon as jobs are sent to the queue, the dispatcher script will start up a new compute instance to execute each job. Run the qstat command to view the status of the jobs. You can also view job status in the web UI by clicking on My Job Queue in the left side navigation bar. You can also run the pbsnodes -a command to see the EC2 instances that have joined the cluster. Click Next to move to the next section.","title":"Step 2: Submit jobs into the queue"},{"location":"workshops/TKO-Scale-Out-Computing/modules/071-budgets/","text":"Lab 6: Configure Budgets \u00b6 This environment supports AWS Budgets and lets you create custom budgets assigned to users, teams, projects, or queues. To prevent over-spending, you can integrate the scheduler with AWS Budgets to take action when customer-defined budget thesholds have been reached. In this example, we will reject job submissions from users who are not assigned to a project that is associated with an AWS Budget . Step 1. Create an AWS Budget \u00b6 Go to the AWS Billing Dashboard , and click Budgets on the left sidebar. Click Create budget . Select Cost budget and click Set your budget . Name your budget \"Project 1\" and set Budgeted amount to $100 . Leave all other fields at their default values. Click Configure alerts . Set Alert threshold to 80 and add an email address to Email contacts . Click Confirm budget to review the configuration, then click Create . Step 2. Enable budget enforcement \u00b6 Next, you will enable the workload scheduler to be aware of the new budget you just created so that it will reject jobs from users who are not members of a project and associated budget. To enable this feature, you will first need to verify the project assigned to each job during submission time. Find the account ID for your temporary AWS account. Click here to go to the Account page of the AWS console. Copy the twelve digit Account Id number located underneath the Account Settings section. Log back into the scheduler instance via SSH and edit the /apps/soca/$SOCA_CONFIGURATION/cluster_hooks/queuejob/check_project_budget.py script and paste the AWS account ID as the value for the aws_account_id variable. Save the file when done. # User Variables aws_account_id = '<ENTER_YOUR_AWS_ACCOUNT_ID>' budget_config_file = '/apps/soca/%SOCA_CONFIGURATION/cluster_manager/settings/project_cost_manager.txt' Enable the integration with the scheduler by running the following commands on the scheduler host: sudo -i source /etc/environment qmgr -c \"create hook check_project_budget event=queuejob\" qmgr -c \"import hook check_project_budget application/x-python default /apps/soca/ $SOCA_CONFIGURATION /cluster_hooks/queuejob/check_project_budget.py\" Step 3. Test budget enforcement \u00b6 Submit a job without budget assignment \u00b6 Switch to the LDAP admin cluster user created in Lab 2. For example, if the username is admin , then the command would be: sudo su - admin Submit a job. qsub -- /bin/echo Hello This job will be rejected and you will see the following messages: qsub: Error. You tried to submit job without project. Specify project using -P parameter OK, let's add a project tag to comply with the policy. qsub -P \"Project 1\" -- /bin/echo Hello This job will also be rejected: qsub: User admin is not assigned to any project. See /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/project_cost_manager.txt Next, we'll associate the user with \"Project 1\" by adding the username to the project_cost_manager.txt mapping file. As sudo, open the /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/project_cost_manager.txt file and add \"Project 1\" budget and the \"admin\" user. # This file is used to prevent job submission when budget allocated to a project exceed your threshold # This file is not used by default and must be configured manually using /apps/soca/$SOCA_CONFIGURATION/cluster_hooks/queuejob/check_project_budget.py # Help & Documentation: https://soca.dev/tutorials/set-up-budget-project/ # # # Syntax: # [project 1] # user1 # user2 # [project 2] # user1 # user3 # [project blabla] # user4 # user5 [Project 1] admin Important The config section (\"Project 1\") must match the name of the budget your created in AWS Budgets (it's case sensitive) Save this file and try to submit a job. This time the job will be accepted. The script queries the AWS Budget in real-time. So, if your users are blocked because of a budget restriction, you can at any time edit the value on AWS Budget and unblock them. If a user tries to launch a job associated to a project which does not exist on AWS Budget or with an invalid name, you will see the following error: qsub -P \"Project 2\" -- /bin/echo Hello qsub: Error. Unable to query AWS Budget API. ERROR: An error occurred ( NotFoundException ) when calling the DescribeBudget operation: [ Exception = NotFoundException ] Failed to call DescribeBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Failed to call GetBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Unable to get budget: Project 2 - the budget doesn ' t exist. Done! On to the next lab. Click Next .","title":"Lab 6: Configure Budgets"},{"location":"workshops/TKO-Scale-Out-Computing/modules/071-budgets/#lab-6-configure-budgets","text":"This environment supports AWS Budgets and lets you create custom budgets assigned to users, teams, projects, or queues. To prevent over-spending, you can integrate the scheduler with AWS Budgets to take action when customer-defined budget thesholds have been reached. In this example, we will reject job submissions from users who are not assigned to a project that is associated with an AWS Budget .","title":"Lab 6: Configure Budgets"},{"location":"workshops/TKO-Scale-Out-Computing/modules/071-budgets/#step-1-create-an-aws-budget","text":"Go to the AWS Billing Dashboard , and click Budgets on the left sidebar. Click Create budget . Select Cost budget and click Set your budget . Name your budget \"Project 1\" and set Budgeted amount to $100 . Leave all other fields at their default values. Click Configure alerts . Set Alert threshold to 80 and add an email address to Email contacts . Click Confirm budget to review the configuration, then click Create .","title":"Step 1. Create an AWS Budget"},{"location":"workshops/TKO-Scale-Out-Computing/modules/071-budgets/#step-2-enable-budget-enforcement","text":"Next, you will enable the workload scheduler to be aware of the new budget you just created so that it will reject jobs from users who are not members of a project and associated budget. To enable this feature, you will first need to verify the project assigned to each job during submission time. Find the account ID for your temporary AWS account. Click here to go to the Account page of the AWS console. Copy the twelve digit Account Id number located underneath the Account Settings section. Log back into the scheduler instance via SSH and edit the /apps/soca/$SOCA_CONFIGURATION/cluster_hooks/queuejob/check_project_budget.py script and paste the AWS account ID as the value for the aws_account_id variable. Save the file when done. # User Variables aws_account_id = '<ENTER_YOUR_AWS_ACCOUNT_ID>' budget_config_file = '/apps/soca/%SOCA_CONFIGURATION/cluster_manager/settings/project_cost_manager.txt' Enable the integration with the scheduler by running the following commands on the scheduler host: sudo -i source /etc/environment qmgr -c \"create hook check_project_budget event=queuejob\" qmgr -c \"import hook check_project_budget application/x-python default /apps/soca/ $SOCA_CONFIGURATION /cluster_hooks/queuejob/check_project_budget.py\"","title":"Step 2. Enable budget enforcement"},{"location":"workshops/TKO-Scale-Out-Computing/modules/071-budgets/#step-3-test-budget-enforcement","text":"","title":"Step 3. Test budget enforcement"},{"location":"workshops/TKO-Scale-Out-Computing/modules/071-budgets/#submit-a-job-without-budget-assignment","text":"Switch to the LDAP admin cluster user created in Lab 2. For example, if the username is admin , then the command would be: sudo su - admin Submit a job. qsub -- /bin/echo Hello This job will be rejected and you will see the following messages: qsub: Error. You tried to submit job without project. Specify project using -P parameter OK, let's add a project tag to comply with the policy. qsub -P \"Project 1\" -- /bin/echo Hello This job will also be rejected: qsub: User admin is not assigned to any project. See /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/project_cost_manager.txt Next, we'll associate the user with \"Project 1\" by adding the username to the project_cost_manager.txt mapping file. As sudo, open the /apps/soca/$SOCA_CONFIGURATION/cluster_manager/settings/project_cost_manager.txt file and add \"Project 1\" budget and the \"admin\" user. # This file is used to prevent job submission when budget allocated to a project exceed your threshold # This file is not used by default and must be configured manually using /apps/soca/$SOCA_CONFIGURATION/cluster_hooks/queuejob/check_project_budget.py # Help & Documentation: https://soca.dev/tutorials/set-up-budget-project/ # # # Syntax: # [project 1] # user1 # user2 # [project 2] # user1 # user3 # [project blabla] # user4 # user5 [Project 1] admin Important The config section (\"Project 1\") must match the name of the budget your created in AWS Budgets (it's case sensitive) Save this file and try to submit a job. This time the job will be accepted. The script queries the AWS Budget in real-time. So, if your users are blocked because of a budget restriction, you can at any time edit the value on AWS Budget and unblock them. If a user tries to launch a job associated to a project which does not exist on AWS Budget or with an invalid name, you will see the following error: qsub -P \"Project 2\" -- /bin/echo Hello qsub: Error. Unable to query AWS Budget API. ERROR: An error occurred ( NotFoundException ) when calling the DescribeBudget operation: [ Exception = NotFoundException ] Failed to call DescribeBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Failed to call GetBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Unable to get budget: Project 2 - the budget doesn ' t exist. Done! On to the next lab. Click Next .","title":"Submit a job without budget assignment"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/","text":"Lab 7: Explore Analytics Dashboard \u00b6 Step 1: Open Cluster Dashboard \u00b6 Return to the your cluster web UI and click on the Analytics section on the left sidebar. Step 2: Add Data to your Cluster \u00b6 By default, job information is ingested by the analytics system on an hourly basis. Log back into the scheduler host via SSH as ec2-user and run the follow command to force immediate ingestion into OpenSearch (formerly Elasticsearch): source /etc/environment ; /apps/soca/ $SOCA_CONFIGURATION /python/latest/bin/python3 /apps/soca/ $SOCA_CONFIGURATION /cluster_analytics/job_tracking.py Step 3: Create Indexes \u00b6 Since it's the first time you access this endpoint, you will need to configure your indexes. First, access Kibana URL and click \"Explore on my Own\" Go under Management and Click Index Patterns Create your first index by typing pbsnodes* . Click next, and then specify the Time Filter key ( timestamp ). Once done, click Create Index Pattern. Repeat the same operation for jobs* index This time, select start_iso as time filter key. Once your indexes are configured, go to Kibana, select \"Discover\" tab to start visualizing the data Index Information \u00b6 Cluster Node Information Job Information Kibana Index Name pbsnodes jobs Data ingestion /apps/soca/cluster_analytics/cluster_nodes_tracking.py /apps/soca/cluster_analytics/job_tracking.py Recurrence 1 minute 1 hour (note: job must be terminated to be shown on OpenSearch (formerly Elasticsearch)) Data uploaded Host Info (status of provisioned host, lifecycle, memory, cpu etc ..) Job Info (allocated hardware, licenses, simulation cost, job owner, instance type ...) Timestamp Key Use \"timestamp\" when you create the index for the first time use \"start_iso\" when you create the index for the first time Examples \u00b6 Cluster Node \u00b6 Job Metadata \u00b6 Generate Graph \u00b6 Money spent by instance type \u00b6 Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: Sum Field: estimate_price_ondemand X Axis (Buckets): Aggregation: Terms Field: instance_type_used.keyword Order By: metric: Sum of estimated_price_on_demand Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Sum of price_on_demand Jobs per user split by instance type \u00b6 Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Count Most active projects \u00b6 Configuration Select \"Pie\" and \"jobs\" index Slice Size (Metrics): Aggregation: Count Split Slices (Buckets): Aggregation: Terms Field: project.keyword Order By: metric: Count Instance type launched by user \u00b6 Configuration Select \"Heat Map\" and \"jobs\" index Value (Metrics): Aggregation: Count Y Axis (Buckets): Aggregation: Term Field: instance_type_used Order By: metric: Count X Axis (Buckets): Aggregation: Terms Field: user Order By: metric: Count Number of nodes in the cluster \u00b6 Configuration Select \"Lines\" and \"pbsnodes\" index Y Axis (Metrics): Aggregation: Unique Count Field: Mom.keyword X Axis (Buckets): Aggregation: Date Histogram, Field: timestamp Interval: Minute Find the price for a given simulation \u00b6 Each job comes with estimated_price_ondemand and estimated_price_reserved attributes which are calculated based on: number of nodes * ( simulation_hours * instance_hourly_rate )","title":"Lab 7: Explore Analytics Dashboard"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#lab-7-explore-analytics-dashboard","text":"","title":"Lab 7: Explore Analytics Dashboard"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#step-1-open-cluster-dashboard","text":"Return to the your cluster web UI and click on the Analytics section on the left sidebar.","title":"Step 1: Open Cluster Dashboard"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#step-2-add-data-to-your-cluster","text":"By default, job information is ingested by the analytics system on an hourly basis. Log back into the scheduler host via SSH as ec2-user and run the follow command to force immediate ingestion into OpenSearch (formerly Elasticsearch): source /etc/environment ; /apps/soca/ $SOCA_CONFIGURATION /python/latest/bin/python3 /apps/soca/ $SOCA_CONFIGURATION /cluster_analytics/job_tracking.py","title":"Step 2: Add Data to your Cluster"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#step-3-create-indexes","text":"Since it's the first time you access this endpoint, you will need to configure your indexes. First, access Kibana URL and click \"Explore on my Own\" Go under Management and Click Index Patterns Create your first index by typing pbsnodes* . Click next, and then specify the Time Filter key ( timestamp ). Once done, click Create Index Pattern. Repeat the same operation for jobs* index This time, select start_iso as time filter key. Once your indexes are configured, go to Kibana, select \"Discover\" tab to start visualizing the data","title":"Step 3: Create Indexes"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#index-information","text":"Cluster Node Information Job Information Kibana Index Name pbsnodes jobs Data ingestion /apps/soca/cluster_analytics/cluster_nodes_tracking.py /apps/soca/cluster_analytics/job_tracking.py Recurrence 1 minute 1 hour (note: job must be terminated to be shown on OpenSearch (formerly Elasticsearch)) Data uploaded Host Info (status of provisioned host, lifecycle, memory, cpu etc ..) Job Info (allocated hardware, licenses, simulation cost, job owner, instance type ...) Timestamp Key Use \"timestamp\" when you create the index for the first time use \"start_iso\" when you create the index for the first time","title":"Index Information"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#examples","text":"","title":"Examples"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#cluster-node","text":"","title":"Cluster Node"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#job-metadata","text":"","title":"Job Metadata"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#generate-graph","text":"","title":"Generate Graph"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#money-spent-by-instance-type","text":"Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: Sum Field: estimate_price_ondemand X Axis (Buckets): Aggregation: Terms Field: instance_type_used.keyword Order By: metric: Sum of estimated_price_on_demand Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Sum of price_on_demand","title":"Money spent by instance type"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#jobs-per-user-split-by-instance-type","text":"Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Count","title":"Jobs per user split by instance type"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#most-active-projects","text":"Configuration Select \"Pie\" and \"jobs\" index Slice Size (Metrics): Aggregation: Count Split Slices (Buckets): Aggregation: Terms Field: project.keyword Order By: metric: Count","title":"Most active projects"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#instance-type-launched-by-user","text":"Configuration Select \"Heat Map\" and \"jobs\" index Value (Metrics): Aggregation: Count Y Axis (Buckets): Aggregation: Term Field: instance_type_used Order By: metric: Count X Axis (Buckets): Aggregation: Terms Field: user Order By: metric: Count","title":"Instance type launched by user"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#number-of-nodes-in-the-cluster","text":"Configuration Select \"Lines\" and \"pbsnodes\" index Y Axis (Metrics): Aggregation: Unique Count Field: Mom.keyword X Axis (Buckets): Aggregation: Date Histogram, Field: timestamp Interval: Minute","title":"Number of nodes in the cluster"},{"location":"workshops/TKO-Scale-Out-Computing/modules/08-analytics/#find-the-price-for-a-given-simulation","text":"Each job comes with estimated_price_ondemand and estimated_price_reserved attributes which are calculated based on: number of nodes * ( simulation_hours * instance_hourly_rate )","title":"Find the price for a given simulation"},{"location":"workshops/TKO-Scale-Out-Computing/modules/09-wrap-up/","text":"Wrap-up \u00b6 Congratulations! You've completed this workshop. Next steps \u00b6 Try this at home! The code for this workshop is open source and publically available. Click on the link to the GitHub repo in the upper-right corner to view the code. Please complete the session survey in the mobile app. Clean up \u00b6 Delete the root stack to remove the resources from your account.","title":"Wrap-up"},{"location":"workshops/TKO-Scale-Out-Computing/modules/09-wrap-up/#wrap-up","text":"Congratulations! You've completed this workshop.","title":"Wrap-up"},{"location":"workshops/TKO-Scale-Out-Computing/modules/09-wrap-up/#next-steps","text":"Try this at home! The code for this workshop is open source and publically available. Click on the link to the GitHub repo in the upper-right corner to view the code. Please complete the session survey in the mobile app.","title":"Next steps"},{"location":"workshops/TKO-Scale-Out-Computing/modules/09-wrap-up/#clean-up","text":"Delete the root stack to remove the resources from your account.","title":"Clean up"},{"location":"workshops/reinvent19-MFG405/","text":"MFG405 Workshop Overview \u00b6 Launch a turnkey scale-out compute environment in minutes on AWS \u00b6 The elasticity of the cloud puts virtually unlimited compute capacity at your fingertips, available within minutes. This can enable companies to quickly scale up in ways they couldn't before, which helps them get results faster. In this workshop, you will deploy the Scale-Out Computing on AWS reference implementation , a solution vetted by AWS Solutions Architects that provides a full-stack, dynamic computing environment that includes a web UI, a workload manager, remote desktops, directory services, analytics dashboards, and budget management. Note This tutorial assumes familiarity with the Linux command line. Lab environment at a glance \u00b6 At its core, this solution implements a scheduler Amazon Elastic Compute Cloud (Amazon EC2) instance, which leverages AWS CloudFormation and Amazon EC2 Auto Scaling to automatically provision the resources necessary to execute cluster user tasks such as scale-out compute jobs and remote visualization sessions. The solution also deploys Amazon Elastic File System (Amazon EFS) for persistent storage; AWS Lambda functions to verify the required prerequisites and create a default signed certificate for an Application Load Balancer (ALB) to manage access to Desktop Cloud Visualization (DCV) workstation sessions; an Amazon Elasticsearch Service (Amazon ES) cluster to store job and host metrics; and AWS Secrets Manager to store the solution configuration files. The solution also leverages AWS Identity and Access Management (IAM) roles to enforce least privileged access. Let's get started. Click the Next link in the bottom right corner to move on to the next module.","title":"MFG405 Workshop Overview"},{"location":"workshops/reinvent19-MFG405/#mfg405-workshop-overview","text":"","title":"MFG405 Workshop Overview"},{"location":"workshops/reinvent19-MFG405/#launch-a-turnkey-scale-out-compute-environment-in-minutes-on-aws","text":"The elasticity of the cloud puts virtually unlimited compute capacity at your fingertips, available within minutes. This can enable companies to quickly scale up in ways they couldn't before, which helps them get results faster. In this workshop, you will deploy the Scale-Out Computing on AWS reference implementation , a solution vetted by AWS Solutions Architects that provides a full-stack, dynamic computing environment that includes a web UI, a workload manager, remote desktops, directory services, analytics dashboards, and budget management. Note This tutorial assumes familiarity with the Linux command line.","title":"Launch a turnkey scale-out compute environment in minutes on AWS"},{"location":"workshops/reinvent19-MFG405/#lab-environment-at-a-glance","text":"At its core, this solution implements a scheduler Amazon Elastic Compute Cloud (Amazon EC2) instance, which leverages AWS CloudFormation and Amazon EC2 Auto Scaling to automatically provision the resources necessary to execute cluster user tasks such as scale-out compute jobs and remote visualization sessions. The solution also deploys Amazon Elastic File System (Amazon EFS) for persistent storage; AWS Lambda functions to verify the required prerequisites and create a default signed certificate for an Application Load Balancer (ALB) to manage access to Desktop Cloud Visualization (DCV) workstation sessions; an Amazon Elasticsearch Service (Amazon ES) cluster to store job and host metrics; and AWS Secrets Manager to store the solution configuration files. The solution also leverages AWS Identity and Access Management (IAM) roles to enforce least privileged access. Let's get started. Click the Next link in the bottom right corner to move on to the next module.","title":"Lab environment at a glance"},{"location":"workshops/reinvent19-MFG405/getting-started/","text":"Getting Started \u00b6 To begin, you'll log into a temporary AWS account that will be provided to you for this workshop. Accessing your AWS account \u00b6 At the beginning of the workshop, you will be given a 12-character access code . This access code grants you access to a temporary AWS account that you'll use for this workshop. Step 1. Log in \u00b6 Go to https://dashboard.eventengine.run , and enter the access code in the Team Hash field. Click Proceed . Step 2. Get Credentials \u00b6 On the Team Dashboard , click SSH Key to download the SSH Keypair PEM file. You'll use this file later to SSH into an EC2 instance. If your using a Mac, change permissions of the PEM file that you just downloaded. This is an SSH security requirement. chmod 600 /path/to/file.pem Next, click AWS Console to begin the login process to the AWS account. Step 3. Open AWS Console \u00b6 Click Open AWS Console . For this workshop, you will not need the Credentials or CLI Snippets Awesome! Now that you are logged into your temporary AWS account, we can start the labs. Click Next .","title":"Getting Started"},{"location":"workshops/reinvent19-MFG405/getting-started/#getting-started","text":"To begin, you'll log into a temporary AWS account that will be provided to you for this workshop.","title":"Getting Started"},{"location":"workshops/reinvent19-MFG405/getting-started/#accessing-your-aws-account","text":"At the beginning of the workshop, you will be given a 12-character access code . This access code grants you access to a temporary AWS account that you'll use for this workshop.","title":"Accessing your AWS account"},{"location":"workshops/reinvent19-MFG405/getting-started/#step-1-log-in","text":"Go to https://dashboard.eventengine.run , and enter the access code in the Team Hash field. Click Proceed .","title":"Step 1. Log in"},{"location":"workshops/reinvent19-MFG405/getting-started/#step-2-get-credentials","text":"On the Team Dashboard , click SSH Key to download the SSH Keypair PEM file. You'll use this file later to SSH into an EC2 instance. If your using a Mac, change permissions of the PEM file that you just downloaded. This is an SSH security requirement. chmod 600 /path/to/file.pem Next, click AWS Console to begin the login process to the AWS account.","title":"Step 2. Get Credentials"},{"location":"workshops/reinvent19-MFG405/getting-started/#step-3-open-aws-console","text":"Click Open AWS Console . For this workshop, you will not need the Credentials or CLI Snippets Awesome! Now that you are logged into your temporary AWS account, we can start the labs. Click Next .","title":"Step 3. Open AWS Console"},{"location":"workshops/reinvent19-MFG405/modules/02-deploy-env/","text":"Lab 1: Deploy Environment \u00b6 Your temporary AWS account has been pre-provisioned with a multiuser computing environment. You'll use this pre-built cluster for the workshop labs . We also want you to have hands-on experience deploying you're own cluster, and in this module we will have you walk through the process of launching a second cluster in your temporary AWS account. This second cluster will be provisioned in the background while you work on the rest of the workshop in the pre-built cluster. Step 1: Launch stack \u00b6 This automated AWS CloudFormation template deploys a scale-out computing environment in the AWS Cloud. Sign in to the AWS Management Console and click the link below to launch the scale-out-computing-on-aws AWS CloudFormation template. Launch Stack Verify the launch region is Oregon Important The template must be launched in Oregon for this workshop. On the Create stack page, you should see the template URL in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. We recommend naming it \"mfg405\". Warning The stack name must be less than 20 characters and must be lower-case only. Under Parameters , modify the the last four parameters, which are marked with REQUIRED . Leave all other fields with their default values. These are variables passed the CloudFormation automation that deploys the environment. Parameter Default Description Install Location Installer S3 Bucket solutions-reference The default AWS bucket name. Do not change this parameter unless you are using a custom installer. Installer Folder scale-out-computing-on-aws/latest The default AWS folder name. Do not change this parameter unless you are using a custom installer. Linux Distribution Linux Distribution AmazonLinux2 The preferred Linux distribution for the scheduler and compute instances. Do not change this parameter. Custom AMI If using a customized Amazon Machine Image, enter the ID. Leave this field blank. Network and Security EC2 Instance Type for Scheduler node m5.large The instance type for the scheduler. Do not change this parameter. VPC Cluster CIDR 110.0.0.0/16 Choose the CIDR (/16) block for the VPC. Do not change this parameter. IP Address 0.0.0.0/0 REQUIRED The public-facing IP address that is permitted to log into the environment. You can leave it at default, but we recommend you change it to your public-facing IP address. You can find your public-facing IP address at http://checkip.amazonaws.com . Add the /32 suffix to the IP number. Key Pair Name ee-default-keypair REQUIRED Select the ee-default-keypair provided by the workshop. Default LDAP User User Name REQUIRED Set a username for the default cluster user. Password REQUIRED Set a password for the default cluster user. (5 characters minimum, uppercase/lowercase/digit only) Choose Next . On the Configure Stack Options page, choose Next . On the Review page, review the settings and check the two boxes acknowledging that the template will create AWS Identity and Access Management (IAM) resources and might require the CAPABILITY_AUTO_EXPAND capability. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should see a status of CREATE_COMPLETE in approximately 35 minutes. You can now move on to the next lab. Click Next .","title":"Lab 1: Deploy Environment"},{"location":"workshops/reinvent19-MFG405/modules/02-deploy-env/#lab-1-deploy-environment","text":"Your temporary AWS account has been pre-provisioned with a multiuser computing environment. You'll use this pre-built cluster for the workshop labs . We also want you to have hands-on experience deploying you're own cluster, and in this module we will have you walk through the process of launching a second cluster in your temporary AWS account. This second cluster will be provisioned in the background while you work on the rest of the workshop in the pre-built cluster.","title":"Lab 1: Deploy Environment"},{"location":"workshops/reinvent19-MFG405/modules/02-deploy-env/#step-1-launch-stack","text":"This automated AWS CloudFormation template deploys a scale-out computing environment in the AWS Cloud. Sign in to the AWS Management Console and click the link below to launch the scale-out-computing-on-aws AWS CloudFormation template. Launch Stack Verify the launch region is Oregon Important The template must be launched in Oregon for this workshop. On the Create stack page, you should see the template URL in the Amazon S3 URL text box and choose Next . On the Specify stack details page, assign a name to your solution stack. We recommend naming it \"mfg405\". Warning The stack name must be less than 20 characters and must be lower-case only. Under Parameters , modify the the last four parameters, which are marked with REQUIRED . Leave all other fields with their default values. These are variables passed the CloudFormation automation that deploys the environment. Parameter Default Description Install Location Installer S3 Bucket solutions-reference The default AWS bucket name. Do not change this parameter unless you are using a custom installer. Installer Folder scale-out-computing-on-aws/latest The default AWS folder name. Do not change this parameter unless you are using a custom installer. Linux Distribution Linux Distribution AmazonLinux2 The preferred Linux distribution for the scheduler and compute instances. Do not change this parameter. Custom AMI If using a customized Amazon Machine Image, enter the ID. Leave this field blank. Network and Security EC2 Instance Type for Scheduler node m5.large The instance type for the scheduler. Do not change this parameter. VPC Cluster CIDR 110.0.0.0/16 Choose the CIDR (/16) block for the VPC. Do not change this parameter. IP Address 0.0.0.0/0 REQUIRED The public-facing IP address that is permitted to log into the environment. You can leave it at default, but we recommend you change it to your public-facing IP address. You can find your public-facing IP address at http://checkip.amazonaws.com . Add the /32 suffix to the IP number. Key Pair Name ee-default-keypair REQUIRED Select the ee-default-keypair provided by the workshop. Default LDAP User User Name REQUIRED Set a username for the default cluster user. Password REQUIRED Set a password for the default cluster user. (5 characters minimum, uppercase/lowercase/digit only) Choose Next . On the Configure Stack Options page, choose Next . On the Review page, review the settings and check the two boxes acknowledging that the template will create AWS Identity and Access Management (IAM) resources and might require the CAPABILITY_AUTO_EXPAND capability. Choose Create stack to deploy the stack. You can view the status of the stack in the AWS CloudFormation console in the Status column. You should see a status of CREATE_COMPLETE in approximately 35 minutes. You can now move on to the next lab. Click Next .","title":"Step 1: Launch stack"},{"location":"workshops/reinvent19-MFG405/modules/03-configure-desktop/","text":"Lab 2: Configure Remote Desktop \u00b6 While the cluster you deployed in the previous lab is running in the background, we'll configure your pre-built cluster to use a specific Amazon Machine Image (AMI) for booting the remote desktop server. As you saw in the architecture diagram , the DCV remote desktop will be your portal into the computing environment. This AMI provides the CentOS 7.5 Linux operating system and the applications you'll use later in the workshop. Obtain IP address of scheduler server In the AWS console, navigate to the CloudFormation page. Select the root stack named \"mod-xxxxxxxxxxxx\", where 'x' is a randomized alpha-numeric string, and click Outputs . The Outputs tab provides various bits of information about the provisioned environment. Copy the value to the left of ConnectionString . We'll use this command to SSH into the scheduler instance. Connect to the instance over SSH For macOS, paste the SSH command into a terminal on the Mac. Be sure to use the full path to the private key you downloaded earlier. See the steps here in the AWS Connecting to Your Linux Instance using SSH page if you need assistance. For Windows, follow the instructions in the AWS Connect to your instance using PuTTY docs. Once logged in, as sudo, open the file /apps/soca/cluster_manager/settings/queue_mapping.yml using your favorite text editor. Example: sudo vi /apps/soca/cluster_manager/settings/queue_mapping.yml Change the the highlighted values in the file to match the example below. Note We strongly recommend that you do not copy and paste this entire block into the file. Instead, please edit the value of each highlighted line individually. queue_type : compute : queues : [ \"high\" , \"normal\" , \"low\" ] instance_ami : \"ami-05d709335d603daac\" instance_type : \"c5.large\" ht_support : \"false\" root_size : \"100\" base_os : \"centos7\" #scratch_size: \"100\" #scratch_iops: \"3600\" #efa_support: \"false\" # .. Refer to the doc for more supported parameters desktop : queues : [ \"desktop\" ] instance_ami : \"ami-05d709335d603daac\" instance_type : \"c5.2xlarge\" ht_support : \"false\" root_size : \"100\" base_os : \"centos7\" Save the changes to the file. Keep this SSH session open; you will come back to it later. Click the Next to move on to the next module.","title":"Lab 2: Configure Remote Desktop"},{"location":"workshops/reinvent19-MFG405/modules/03-configure-desktop/#lab-2-configure-remote-desktop","text":"While the cluster you deployed in the previous lab is running in the background, we'll configure your pre-built cluster to use a specific Amazon Machine Image (AMI) for booting the remote desktop server. As you saw in the architecture diagram , the DCV remote desktop will be your portal into the computing environment. This AMI provides the CentOS 7.5 Linux operating system and the applications you'll use later in the workshop. Obtain IP address of scheduler server In the AWS console, navigate to the CloudFormation page. Select the root stack named \"mod-xxxxxxxxxxxx\", where 'x' is a randomized alpha-numeric string, and click Outputs . The Outputs tab provides various bits of information about the provisioned environment. Copy the value to the left of ConnectionString . We'll use this command to SSH into the scheduler instance. Connect to the instance over SSH For macOS, paste the SSH command into a terminal on the Mac. Be sure to use the full path to the private key you downloaded earlier. See the steps here in the AWS Connecting to Your Linux Instance using SSH page if you need assistance. For Windows, follow the instructions in the AWS Connect to your instance using PuTTY docs. Once logged in, as sudo, open the file /apps/soca/cluster_manager/settings/queue_mapping.yml using your favorite text editor. Example: sudo vi /apps/soca/cluster_manager/settings/queue_mapping.yml Change the the highlighted values in the file to match the example below. Note We strongly recommend that you do not copy and paste this entire block into the file. Instead, please edit the value of each highlighted line individually. queue_type : compute : queues : [ \"high\" , \"normal\" , \"low\" ] instance_ami : \"ami-05d709335d603daac\" instance_type : \"c5.large\" ht_support : \"false\" root_size : \"100\" base_os : \"centos7\" #scratch_size: \"100\" #scratch_iops: \"3600\" #efa_support: \"false\" # .. Refer to the doc for more supported parameters desktop : queues : [ \"desktop\" ] instance_ami : \"ami-05d709335d603daac\" instance_type : \"c5.2xlarge\" ht_support : \"false\" root_size : \"100\" base_os : \"centos7\" Save the changes to the file. Keep this SSH session open; you will come back to it later. Click the Next to move on to the next module.","title":"Lab 2: Configure Remote Desktop"},{"location":"workshops/reinvent19-MFG405/modules/04-web-login/","text":"Lab 3: Launch Remote Desktop Session \u00b6 The goal of this module is to start a remote desktop session from which you will run applications and submit jobs into the cluster. You will use the cluster's management console to start and monitor the session. Step 1: Subscribe to AWS FPGA Developer AMI \u00b6 This workshop requires a subscription to the AWS FPGA Developer AMI in AWS Marketplace . This is the AMI you added to the /apps/soca/cluster_manager/settings/queue_mapping.yml file in the previous lab. It will be used to boot the remote desktop instance in AWS and contains software required later in the workshop. Return to the AWS console for your temporary AWS account. Click here to open the page for the AWS FPGA Developer AMI in AWS Marketplace, and then choose Continue to Subscribe . Review the terms and conditions for software usage, and then choose Accept Terms . When the subscription process is complete, exit out of AWS Marketplace without further action. Do not click Continue to Configure ; the workshop CloudFormation templates will deploy the AMI for you. Click here to verify the subscription within the Marketplace dashboard. You should see the FPGA Developer AMI in the list of subscriptions. Step 2: Log into web UI \u00b6 Select the root stack named \"mod-xxxxxxxx\" again, and click Outputs . The Outputs tab provides various bits of information about the provisioned environment. Click on the link to the right of WebUserInterface to log into the Web UI Note Your web browser will warn you about a certificate problem with the site. To open the webpage, you must authorize the browser to trust the self-signed security certificate. In a production deployment, you would upload a Server Certificate to the Elastic Load Balancer endpoint. Log in using the web UI using the following credentials: username: admin password: passw0rd (use a zero instead of 'o') Step 3: Launch remote desktop server \u00b6 Follow these instructions to start a full remote desktop experience in your new cluster: Click Graphical Access on the left sidebar. Select 1 day in the Session Validity popup menu. Choose 2D - Medium (8 vCPUs - 32GB ram) in the Session Type popup menu. Click Launch my Session #1 After you click Launch my session , a new job is submitted into the queue that will instruct AWS to provision a server with 8 vCPUs and 32GB of memory and install all desktop required packages including Gnome. You will see an message asking you to wait up to 20 minutes before being able to access your remote desktop, but it should take around 10 minutes to deploy the remote desktop server. Note You can monitor the deployment of the remote desktop server by observing the status of the CloudFormation stack with a name ending in job-0 . If after 5 minutes the status of the stack is not CREATE_COMPLETE , please raise your hand for assistance. Let's move on to the next step while we wait for the desktop instance to launch. Click Next .","title":"Lab 3: Launch Remote Desktop Session"},{"location":"workshops/reinvent19-MFG405/modules/04-web-login/#lab-3-launch-remote-desktop-session","text":"The goal of this module is to start a remote desktop session from which you will run applications and submit jobs into the cluster. You will use the cluster's management console to start and monitor the session.","title":"Lab 3: Launch Remote Desktop Session"},{"location":"workshops/reinvent19-MFG405/modules/04-web-login/#step-1-subscribe-to-aws-fpga-developer-ami","text":"This workshop requires a subscription to the AWS FPGA Developer AMI in AWS Marketplace . This is the AMI you added to the /apps/soca/cluster_manager/settings/queue_mapping.yml file in the previous lab. It will be used to boot the remote desktop instance in AWS and contains software required later in the workshop. Return to the AWS console for your temporary AWS account. Click here to open the page for the AWS FPGA Developer AMI in AWS Marketplace, and then choose Continue to Subscribe . Review the terms and conditions for software usage, and then choose Accept Terms . When the subscription process is complete, exit out of AWS Marketplace without further action. Do not click Continue to Configure ; the workshop CloudFormation templates will deploy the AMI for you. Click here to verify the subscription within the Marketplace dashboard. You should see the FPGA Developer AMI in the list of subscriptions.","title":"Step 1: Subscribe to AWS FPGA Developer AMI"},{"location":"workshops/reinvent19-MFG405/modules/04-web-login/#step-2-log-into-web-ui","text":"Select the root stack named \"mod-xxxxxxxx\" again, and click Outputs . The Outputs tab provides various bits of information about the provisioned environment. Click on the link to the right of WebUserInterface to log into the Web UI Note Your web browser will warn you about a certificate problem with the site. To open the webpage, you must authorize the browser to trust the self-signed security certificate. In a production deployment, you would upload a Server Certificate to the Elastic Load Balancer endpoint. Log in using the web UI using the following credentials: username: admin password: passw0rd (use a zero instead of 'o')","title":"Step 2: Log into web UI"},{"location":"workshops/reinvent19-MFG405/modules/04-web-login/#step-3-launch-remote-desktop-server","text":"Follow these instructions to start a full remote desktop experience in your new cluster: Click Graphical Access on the left sidebar. Select 1 day in the Session Validity popup menu. Choose 2D - Medium (8 vCPUs - 32GB ram) in the Session Type popup menu. Click Launch my Session #1 After you click Launch my session , a new job is submitted into the queue that will instruct AWS to provision a server with 8 vCPUs and 32GB of memory and install all desktop required packages including Gnome. You will see an message asking you to wait up to 20 minutes before being able to access your remote desktop, but it should take around 10 minutes to deploy the remote desktop server. Note You can monitor the deployment of the remote desktop server by observing the status of the CloudFormation stack with a name ending in job-0 . If after 5 minutes the status of the stack is not CREATE_COMPLETE , please raise your hand for assistance. Let's move on to the next step while we wait for the desktop instance to launch. Click Next .","title":"Step 3: Launch remote desktop server"},{"location":"workshops/reinvent19-MFG405/modules/06-launch-vivado/","text":"Lab 4: Launch Vivado GUI \u00b6 Launch the Vivado CAD Tool \u00b6 Now that your remote desktop is set up, you can launch the Vivado Design Suite (included in the AWS FPGA Developer AMI). The goal with this lab is to evaluate the remote visualization experience using a graphically intensive Computer-Aided Design (CAD) tool. Step 1: Log into your session \u00b6 By now your remote desktop session should be ready and you should see the following under Your Session #1 : Click Open Session directly on a browser to log into the remote desktop session in your new cluster using the username and password you created in the steps above. Note You can also access the session with the NICE DCV native clients, which are available for Mac, Linux, and Windows from https://download.nice-dcv.com To launch Vivado, start a new terminal session by going to Applications \u2192 Favorites \u2192 Terminal in the desktop manager. Type vivado at the command prompt and hit enter The Vivado GUI start and show the following screen: Step 2: Load sample project \u00b6 Next, load a sample workload using one of the included example projects: Go to the Quick Start section and select Open Project . Navigate to /projects/mfg405/ , select the project_1.xpr file, and click OK . Double-click on Open Block Diagram under IP INTEGRATOR in the left-side navigation panel. After the design opens you should see this: You can now click around the GUI and scroll and pan through the schematics to get a sense of the remote desktop experience. For extra credit, double-click on Open Synthesized Design in the navigation panel to see a more complext layout of the design. You've completed this lab. Click Next .","title":"Lab 4: Launch Vivado GUI"},{"location":"workshops/reinvent19-MFG405/modules/06-launch-vivado/#lab-4-launch-vivado-gui","text":"","title":"Lab 4: Launch Vivado GUI"},{"location":"workshops/reinvent19-MFG405/modules/06-launch-vivado/#launch-the-vivado-cad-tool","text":"Now that your remote desktop is set up, you can launch the Vivado Design Suite (included in the AWS FPGA Developer AMI). The goal with this lab is to evaluate the remote visualization experience using a graphically intensive Computer-Aided Design (CAD) tool.","title":"Launch the Vivado CAD Tool"},{"location":"workshops/reinvent19-MFG405/modules/06-launch-vivado/#step-1-log-into-your-session","text":"By now your remote desktop session should be ready and you should see the following under Your Session #1 : Click Open Session directly on a browser to log into the remote desktop session in your new cluster using the username and password you created in the steps above. Note You can also access the session with the NICE DCV native clients, which are available for Mac, Linux, and Windows from https://download.nice-dcv.com To launch Vivado, start a new terminal session by going to Applications \u2192 Favorites \u2192 Terminal in the desktop manager. Type vivado at the command prompt and hit enter The Vivado GUI start and show the following screen:","title":"Step 1: Log into your session"},{"location":"workshops/reinvent19-MFG405/modules/06-launch-vivado/#step-2-load-sample-project","text":"Next, load a sample workload using one of the included example projects: Go to the Quick Start section and select Open Project . Navigate to /projects/mfg405/ , select the project_1.xpr file, and click OK . Double-click on Open Block Diagram under IP INTEGRATOR in the left-side navigation panel. After the design opens you should see this: You can now click around the GUI and scroll and pan through the schematics to get a sense of the remote desktop experience. For extra credit, double-click on Open Synthesized Design in the navigation panel to see a more complext layout of the design. You've completed this lab. Click Next .","title":"Step 2: Load sample project"},{"location":"workshops/reinvent19-MFG405/modules/07-submit-batch/","text":"Lab 5: Submit Batch Workloads \u00b6 This module provides instructions for running an example batch workload in the computing envronment created the Deploy environment module. The example workload is a CPU- and IO-intensive logic simulation that is found in integrated cicuit design workflows. The workload uses data contained in the public AWS F1 FPGA Development Kit and the Xilinx Vivado EDA software suite provided by the AWS FPGA Developer AMI that you subscribed to in the first tutorial. Although you'll be using data and tools from AWS FPGA developer resources, you will not be running on the F1 FPGA instance or executing any type of FPGA workload; we're simply running software simulations on EC2 compute instances using the design data, IP, and software that these kits provide for no additional charge. Step 1: Clone workload repo \u00b6 End your SSH sessions and log back into the DCV remote desktop session that you established Launch Remote Desktop Session lab. Minimize the Vivado GUI and open a new terminal window. Clone the example workload from the aws-fpga-sa-demo Github repo into your user's home directory on the NFS file system. cd $HOME git clone https://github.com/morrmt/aws-fpga-sa-demo.git Change into the repo's workshop directory. cd $HOME/aws-fpga-sa-demo/eda-workshop Step 2: Submit jobs into the queue \u00b6 Next, you'll submit four jobs into the cluster, each job requesting a specific instance type. Using multiple instance types will help provide more interesting data to look at in the analytics lab. Submit jobs . Submit each of the jobs below: qsub -l instance_type=c5.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=c4.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=m5.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=m4.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME Watch job status . As soon as jobs are sent to the queue, the dispatcher script will start up a new compute instance to execute each job. Run the qstat command to view the status of the jobs. You can also view job status in the web UI by clicking on My Job Queue in the left side navigation bar. You can also run the pbsnodes -a command to see the EC2 instances that have joined the cluster. Click Next to move to the next section.","title":"Lab 5: Submit Batch Workloads"},{"location":"workshops/reinvent19-MFG405/modules/07-submit-batch/#lab-5-submit-batch-workloads","text":"This module provides instructions for running an example batch workload in the computing envronment created the Deploy environment module. The example workload is a CPU- and IO-intensive logic simulation that is found in integrated cicuit design workflows. The workload uses data contained in the public AWS F1 FPGA Development Kit and the Xilinx Vivado EDA software suite provided by the AWS FPGA Developer AMI that you subscribed to in the first tutorial. Although you'll be using data and tools from AWS FPGA developer resources, you will not be running on the F1 FPGA instance or executing any type of FPGA workload; we're simply running software simulations on EC2 compute instances using the design data, IP, and software that these kits provide for no additional charge.","title":"Lab 5: Submit Batch Workloads"},{"location":"workshops/reinvent19-MFG405/modules/07-submit-batch/#step-1-clone-workload-repo","text":"End your SSH sessions and log back into the DCV remote desktop session that you established Launch Remote Desktop Session lab. Minimize the Vivado GUI and open a new terminal window. Clone the example workload from the aws-fpga-sa-demo Github repo into your user's home directory on the NFS file system. cd $HOME git clone https://github.com/morrmt/aws-fpga-sa-demo.git Change into the repo's workshop directory. cd $HOME/aws-fpga-sa-demo/eda-workshop","title":"Step 1: Clone workload repo"},{"location":"workshops/reinvent19-MFG405/modules/07-submit-batch/#step-2-submit-jobs-into-the-queue","text":"Next, you'll submit four jobs into the cluster, each job requesting a specific instance type. Using multiple instance types will help provide more interesting data to look at in the analytics lab. Submit jobs . Submit each of the jobs below: qsub -l instance_type=c5.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=c4.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=m5.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME qsub -l instance_type=m4.xlarge -- $HOME/aws-fpga-sa-demo/eda-workshop/run-sim.sh --scratch-dir $HOME Watch job status . As soon as jobs are sent to the queue, the dispatcher script will start up a new compute instance to execute each job. Run the qstat command to view the status of the jobs. You can also view job status in the web UI by clicking on My Job Queue in the left side navigation bar. You can also run the pbsnodes -a command to see the EC2 instances that have joined the cluster. Click Next to move to the next section.","title":"Step 2: Submit jobs into the queue"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/","text":"Lab 6: Explore Analytics Dashboard \u00b6 Step 1: Open Cluster Dashboard \u00b6 Return to the your cluster web UI and click on the Analytics section on the left sidebar. Step 2: Add Data to your Cluster \u00b6 By default, job information is ingested by the analytics system on an hourly basis. Log back into the scheduler host via SSH as ec2-user and run the follow command to force immediate ingestion into OpenSearch (formerly Elasticsearch): source /etc/environment ; /apps/soca/ $SOCA_CONFIGURATION /python/latest/bin/python3 /apps/soca/ $SOCA_CONFIGURATION /cluster_analytics/job_tracking.py Step 3: Create Indexes \u00b6 Since it's the first time you access this endpoint, you will need to configure your indexes. First, access Kibana URL and click \"Explore on my Own\" Go under Management and Click Index Patterns Create your first index by typing pbsnodes* . Click next, and then specify the Time Filter key ( timestamp ). Once done, click Create Index Pattern. Repeat the same operation for jobs* index This time, select start_iso as time filter key. Once your indexes are configured, go to Kibana, select \"Discover\" tab to start visualizing the data Index Information \u00b6 Cluster Node Information Job Information Kibana Index Name pbsnodes jobs Data ingestion /apps/soca/cluster_analytics/cluster_nodes_tracking.py /apps/soca/cluster_analytics/job_tracking.py Recurrence 1 minute 1 hour (note: job must be terminated to be shown on OpenSearch (formerly Elasticsearch)) Data uploaded Host Info (status of provisioned host, lifecycle, memory, cpu etc ..) Job Info (allocated hardware, licenses, simulation cost, job owner, instance type ...) Timestamp Key Use \"timestamp\" when you create the index for the first time use \"start_iso\" when you create the index for the first time Examples \u00b6 Cluster Node \u00b6 Job Metadata \u00b6 Generate Graph \u00b6 Money spent by instance type \u00b6 Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: Sum Field: estimate_price_ondemand X Axis (Buckets): Aggregation: Terms Field: instance_type_used.keyword Order By: metric: Sum of estimated_price_on_demand Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Sum of price_on_demand Jobs per user split by instance type \u00b6 Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Count Most active projects \u00b6 Configuration Select \"Pie\" and \"jobs\" index Slice Size (Metrics): Aggregation: Count Split Slices (Buckets): Aggregation: Terms Field: project.keyword Order By: metric: Count Instance type launched by user \u00b6 Configuration Select \"Heat Map\" and \"jobs\" index Value (Metrics): Aggregation: Count Y Axis (Buckets): Aggregation: Term Field: instance_type_used Order By: metric: Count X Axis (Buckets): Aggregation: Terms Field: user Order By: metric: Count Number of nodes in the cluster \u00b6 Configuration Select \"Lines\" and \"pbsnodes\" index Y Axis (Metrics): Aggregation: Unique Count Field: Mom.keyword X Axis (Buckets): Aggregation: Date Histogram, Field: timestamp Interval: Minute Find the price for a given simulation \u00b6 Each job comes with estimated_price_ondemand and estimated_price_reserved attributes which are calculated based on: number of nodes * ( simulation_hours * instance_hourly_rate )","title":"Lab 6: Explore Analytics Dashboard"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#lab-6-explore-analytics-dashboard","text":"","title":"Lab 6: Explore Analytics Dashboard"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#step-1-open-cluster-dashboard","text":"Return to the your cluster web UI and click on the Analytics section on the left sidebar.","title":"Step 1: Open Cluster Dashboard"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#step-2-add-data-to-your-cluster","text":"By default, job information is ingested by the analytics system on an hourly basis. Log back into the scheduler host via SSH as ec2-user and run the follow command to force immediate ingestion into OpenSearch (formerly Elasticsearch): source /etc/environment ; /apps/soca/ $SOCA_CONFIGURATION /python/latest/bin/python3 /apps/soca/ $SOCA_CONFIGURATION /cluster_analytics/job_tracking.py","title":"Step 2: Add Data to your Cluster"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#step-3-create-indexes","text":"Since it's the first time you access this endpoint, you will need to configure your indexes. First, access Kibana URL and click \"Explore on my Own\" Go under Management and Click Index Patterns Create your first index by typing pbsnodes* . Click next, and then specify the Time Filter key ( timestamp ). Once done, click Create Index Pattern. Repeat the same operation for jobs* index This time, select start_iso as time filter key. Once your indexes are configured, go to Kibana, select \"Discover\" tab to start visualizing the data","title":"Step 3: Create Indexes"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#index-information","text":"Cluster Node Information Job Information Kibana Index Name pbsnodes jobs Data ingestion /apps/soca/cluster_analytics/cluster_nodes_tracking.py /apps/soca/cluster_analytics/job_tracking.py Recurrence 1 minute 1 hour (note: job must be terminated to be shown on OpenSearch (formerly Elasticsearch)) Data uploaded Host Info (status of provisioned host, lifecycle, memory, cpu etc ..) Job Info (allocated hardware, licenses, simulation cost, job owner, instance type ...) Timestamp Key Use \"timestamp\" when you create the index for the first time use \"start_iso\" when you create the index for the first time","title":"Index Information"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#examples","text":"","title":"Examples"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#cluster-node","text":"","title":"Cluster Node"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#job-metadata","text":"","title":"Job Metadata"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#generate-graph","text":"","title":"Generate Graph"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#money-spent-by-instance-type","text":"Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: Sum Field: estimate_price_ondemand X Axis (Buckets): Aggregation: Terms Field: instance_type_used.keyword Order By: metric: Sum of estimated_price_on_demand Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Sum of price_on_demand","title":"Money spent by instance type"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#jobs-per-user-split-by-instance-type","text":"Configuration Select \"Vertical Bars\" and \"jobs\" index Y Axis (Metrics): Aggregation: count X Axis (Buckets): Aggregation: Terms Field: user.keyword Order By: metric: Count Split Series (Buckets): Sub Aggregation: Terms Field: instance_type_used Order By: metric: Count","title":"Jobs per user split by instance type"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#most-active-projects","text":"Configuration Select \"Pie\" and \"jobs\" index Slice Size (Metrics): Aggregation: Count Split Slices (Buckets): Aggregation: Terms Field: project.keyword Order By: metric: Count","title":"Most active projects"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#instance-type-launched-by-user","text":"Configuration Select \"Heat Map\" and \"jobs\" index Value (Metrics): Aggregation: Count Y Axis (Buckets): Aggregation: Term Field: instance_type_used Order By: metric: Count X Axis (Buckets): Aggregation: Terms Field: user Order By: metric: Count","title":"Instance type launched by user"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#number-of-nodes-in-the-cluster","text":"Configuration Select \"Lines\" and \"pbsnodes\" index Y Axis (Metrics): Aggregation: Unique Count Field: Mom.keyword X Axis (Buckets): Aggregation: Date Histogram, Field: timestamp Interval: Minute","title":"Number of nodes in the cluster"},{"location":"workshops/reinvent19-MFG405/modules/08-analytics/#find-the-price-for-a-given-simulation","text":"Each job comes with estimated_price_ondemand and estimated_price_reserved attributes which are calculated based on: number of nodes * ( simulation_hours * instance_hourly_rate )","title":"Find the price for a given simulation"},{"location":"workshops/reinvent19-MFG405/modules/09-wrap-up/","text":"Wrap-up \u00b6 Congratulations! You've completed this workshop. Next steps \u00b6 Try this at home! The code for this workshop is open source and publically available. Please complete the session survey in the mobile app. Clean up \u00b6 No cleanup required! This responsibility falls to AWS.","title":"Wrap-up"},{"location":"workshops/reinvent19-MFG405/modules/09-wrap-up/#wrap-up","text":"Congratulations! You've completed this workshop.","title":"Wrap-up"},{"location":"workshops/reinvent19-MFG405/modules/09-wrap-up/#next-steps","text":"Try this at home! The code for this workshop is open source and publically available. Please complete the session survey in the mobile app.","title":"Next steps"},{"location":"workshops/reinvent19-MFG405/modules/09-wrap-up/#clean-up","text":"No cleanup required! This responsibility falls to AWS.","title":"Clean up"},{"location":"workshops/reinvent19-MFG405/modules/z-budgets/","text":". \u00b6 Lab 4 Configure Budgets \u00b6 This environment supports AWS Budgets and lets you create custom budgets assigned to users, teams, projects, or queues. To prevent over-spending, you can integrate the scheduler with AWS Budgets to take action when customer-defined budget thesholds have been reached. In this example, we will reject job submissions from users who are not assigned to a project that is associated with an AWS Budget . Step 1. Create an AWS Budget \u00b6 Go to the AWS Billing Dashboard , and click Budgets on the left sidebar. Click Create budget . Select Cost budget and click Set your budget . Name your budget \"Project 1\" and set Budgeted amount to $100 . Leave all other fields at their default values. Click Configure alerts . Set Alert threshold to 80 and add an email address to Email contacts . Click Confirm budget to review the configuration, then click Create . Step 2. Enable budget enforcement \u00b6 Next, you will enable the workload scheduler to be aware of the new budget you just created so that it will reject jobs from users who are not members of a project and associated budget. To enable this feature, you will first need to verify the project assigned to each job during submission time. Find the account ID for your temporary AWS account. Click here to go to the Account page of the AWS console. Copy the twelve digit Account Id number located underneath the Account Settings section. Log back into the scheduler instance via SSH and edit the /apps/soca/cluster_hooks/queuejob/check_project_budget.py script and paste the AWS account ID as the value for the aws_account_id variable. Save the file when done. # User Variables aws_account_id = '<ENTER_YOUR_AWS_ACCOUNT_ID>' budget_config_file = '/apps/soca/cluster_manager/settings/project_cost_manager.txt' Enable the integration with the scheduler by running the following commands on the scheduler host: sudo -i source /etc/environment qmgr -c \"create hook check_project_budget event=queuejob\" qmgr -c \"import hook check_project_budget application/x-python default /apps/soca/cluster_hooks/queuejob/check_project_budget.py\" Step 3. Test budget enforcement \u00b6 Submit a job without budget assignment \u00b6 Switch to the admin cluster user. sudo su - admin Submit a job. qsub -- /bin/echo Hello This job will be rejected and you will see the following messages: qsub: Error. You tried to submit job without project. Specify project using -P parameter OK, let's add a project tag to comply with the policy. qsub -P \"Project 1\" -- /bin/echo Hello This job will also be rejected: qsub: User morrmt is not assigned to any project. See /apps/soca/cluster_manager/settings/project_cost_manager.txt Next, we'll associated the user with \"Project 1\" by adding the username to the project_cost_manager.txt mapping file. As sudo, open the /apps/soca/cluster_manager/settings/project_cost_manager.txt file and add \"Project 1\" budget and the \"admin\" user. # This file is used to prevent job submission when budget allocated to a project exceed your threshold # This file is not used by default and must be configured manually using /apps/soca/cluster_hooks/queuejob/check_project_budget.py # Help & Documentation: https://soca.dev/tutorials/set-up-budget-project/ # # # Syntax: # [project 1] # user1 # user2 # [project 2] # user1 # user3 # [project blabla] # user4 # user5 [Project 1] admin Important The config section (\"Project 1\") must match the name of the budget your created in AWS Budgets (it's case sensitive) Save this file and try to submit a job. This time the job will be accepted. The script queries the AWS Budget in real-time. So, if your users are blocked because of a budget restriction, you can at any time edit the value on AWS Budget and unblock them. If a user tries to launch a job associated to a project which does not exist on AWS Budget or with an invalid name, you will see the following error: bash-4.2$ qsub -P \"Project 2\" -- /bin/echo Hello qsub: Error. Unable to query AWS Budget API. ERROR: An error occurred ( NotFoundException ) when calling the DescribeBudget operation: [ Exception = NotFoundException ] Failed to call DescribeBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Failed to call GetBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Unable to get budget: Project 2 - the budget doesn ' t exist. Done! On to the next lab. Click Next .","title":"."},{"location":"workshops/reinvent19-MFG405/modules/z-budgets/#_1","text":"","title":"."},{"location":"workshops/reinvent19-MFG405/modules/z-budgets/#lab-4-configure-budgets","text":"This environment supports AWS Budgets and lets you create custom budgets assigned to users, teams, projects, or queues. To prevent over-spending, you can integrate the scheduler with AWS Budgets to take action when customer-defined budget thesholds have been reached. In this example, we will reject job submissions from users who are not assigned to a project that is associated with an AWS Budget .","title":"Lab 4 Configure Budgets"},{"location":"workshops/reinvent19-MFG405/modules/z-budgets/#step-1-create-an-aws-budget","text":"Go to the AWS Billing Dashboard , and click Budgets on the left sidebar. Click Create budget . Select Cost budget and click Set your budget . Name your budget \"Project 1\" and set Budgeted amount to $100 . Leave all other fields at their default values. Click Configure alerts . Set Alert threshold to 80 and add an email address to Email contacts . Click Confirm budget to review the configuration, then click Create .","title":"Step 1. Create an AWS Budget"},{"location":"workshops/reinvent19-MFG405/modules/z-budgets/#step-2-enable-budget-enforcement","text":"Next, you will enable the workload scheduler to be aware of the new budget you just created so that it will reject jobs from users who are not members of a project and associated budget. To enable this feature, you will first need to verify the project assigned to each job during submission time. Find the account ID for your temporary AWS account. Click here to go to the Account page of the AWS console. Copy the twelve digit Account Id number located underneath the Account Settings section. Log back into the scheduler instance via SSH and edit the /apps/soca/cluster_hooks/queuejob/check_project_budget.py script and paste the AWS account ID as the value for the aws_account_id variable. Save the file when done. # User Variables aws_account_id = '<ENTER_YOUR_AWS_ACCOUNT_ID>' budget_config_file = '/apps/soca/cluster_manager/settings/project_cost_manager.txt' Enable the integration with the scheduler by running the following commands on the scheduler host: sudo -i source /etc/environment qmgr -c \"create hook check_project_budget event=queuejob\" qmgr -c \"import hook check_project_budget application/x-python default /apps/soca/cluster_hooks/queuejob/check_project_budget.py\"","title":"Step 2. Enable budget enforcement"},{"location":"workshops/reinvent19-MFG405/modules/z-budgets/#step-3-test-budget-enforcement","text":"","title":"Step 3. Test budget enforcement"},{"location":"workshops/reinvent19-MFG405/modules/z-budgets/#submit-a-job-without-budget-assignment","text":"Switch to the admin cluster user. sudo su - admin Submit a job. qsub -- /bin/echo Hello This job will be rejected and you will see the following messages: qsub: Error. You tried to submit job without project. Specify project using -P parameter OK, let's add a project tag to comply with the policy. qsub -P \"Project 1\" -- /bin/echo Hello This job will also be rejected: qsub: User morrmt is not assigned to any project. See /apps/soca/cluster_manager/settings/project_cost_manager.txt Next, we'll associated the user with \"Project 1\" by adding the username to the project_cost_manager.txt mapping file. As sudo, open the /apps/soca/cluster_manager/settings/project_cost_manager.txt file and add \"Project 1\" budget and the \"admin\" user. # This file is used to prevent job submission when budget allocated to a project exceed your threshold # This file is not used by default and must be configured manually using /apps/soca/cluster_hooks/queuejob/check_project_budget.py # Help & Documentation: https://soca.dev/tutorials/set-up-budget-project/ # # # Syntax: # [project 1] # user1 # user2 # [project 2] # user1 # user3 # [project blabla] # user4 # user5 [Project 1] admin Important The config section (\"Project 1\") must match the name of the budget your created in AWS Budgets (it's case sensitive) Save this file and try to submit a job. This time the job will be accepted. The script queries the AWS Budget in real-time. So, if your users are blocked because of a budget restriction, you can at any time edit the value on AWS Budget and unblock them. If a user tries to launch a job associated to a project which does not exist on AWS Budget or with an invalid name, you will see the following error: bash-4.2$ qsub -P \"Project 2\" -- /bin/echo Hello qsub: Error. Unable to query AWS Budget API. ERROR: An error occurred ( NotFoundException ) when calling the DescribeBudget operation: [ Exception = NotFoundException ] Failed to call DescribeBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Failed to call GetBudget for [ AccountId: <REDACTED_ACCOUNT_ID> ] - Unable to get budget: Project 2 - the budget doesn ' t exist. Done! On to the next lab. Click Next .","title":"Submit a job without budget assignment"}]}